@url{matplotlib,
	Author = {John D. Hunter},
	Date-Added = {2012-01-24 11:53:27 -0200},
	Date-Modified = {2012-01-24 11:54:27 -0200},
	Lastchecked = {2012-01-10},
	Title = {matplotlib},
	Url = {http://matplotlib.sourceforge.net/}}

@url{python,
	Author = {Guido van Rossum},
	Date-Added = {2012-01-24 11:50:50 -0200},
	Date-Modified = {2012-01-24 11:52:34 -0200},
	Lastchecked = {2012-01-10},
	Title = {Python Programming Language},
	Url = {http://python.org/}}

@article{hofstadter1995seeing,
	  title={{On seeing A's and seeing As}},
	  author={Hofstadter, Douglas R},
	  journal={Stanford Humanities Review},
	  volume={4},
	  number={2},
	  pages={109--121},
	  year={1995},
	  publisher={Stanford Humanities Review},
	  Url = {https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html}
}


@article{hong_character_1991,
	title = {Character recognition in a sparse distributed memory},
	volume = {21},
	issn = {00189472},
	url = {http://ieeexplore.ieee.org/document/97459/},
	doi = {10.1109/21.97459},
	abstract = {Kanerva’s sparse distributed memory model is applied to character recognition. The results of recognizing corrupted characters, using a sparse distributed memory with fewer than 10000 locations, is described; 100 corrupted test patterns were generated to recognize 18 template patterns and performance evaluation of the model is obtained. Insights about the behavior of sparse distributed memories as well as the model’s applicability to character recognition, are provided.},
	language = {en},
	number = {3},
	urldate = {2018-04-07},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Hong, Y.-S. and Chen, S.-S.},
	month = jun,
	year = {1991},
	pages = {674--678},
	file = {Hong and Chen - 1991 - Character recognition in a sparse distributed memo.pdf:/Users/AL/Zotero/storage/DPHDGZK4/Hong and Chen - 1991 - Character recognition in a sparse distributed memo.pdf:application/pdf}
}


@techreport{marshall_technical_1989,
	address = {Bloomington, Indiana},
	title = {Technical {Report}: {Learning} with a {Sparse} {Distributed} {Memory}},
	abstract = {In this paper we describe a software implementation of a sparse distributed memory in the *Lisp language on a Connection Machine. We use several di erent learning tasks to evaluate the performance of the memory model, including generalization, prediction, and retraining.},
	language = {en},
	institution = {Indiana University, Bloomington},
	author = {Marshall, Jim and Meeden, Lisa},
	month = may,
	year = {1989},
	pages = {14},
	file = {Marshall and Meeden - Technical Report Learning with a Sparse Distribut.pdf:/Users/AL/Zotero/storage/MCFRE5QV/Marshall and Meeden - Technical Report Learning with a Sparse Distribut.pdf:application/pdf;Marshall and Meeden - Technical Report Learning with a Sparse Distribut.pdf:/Users/AL/Zotero/storage/QTARH4UB/Marshall and Meeden - Technical Report Learning with a Sparse Distribut.pdf:application/pdf}
}


@article{denning_sparse_1989,
	title = {Sparse {Distributed} {Memory}},
	volume = {77},
	issn = {0003-0996},
	url = {http://adsabs.harvard.edu/abs/1989AmSci..77..333D},
	abstract = {Not Available},
	urldate = {2018-04-07},
	journal = {American Scientist},
	author = {Denning, Peter J.},
	month = jul,
	year = {1989},
	pages = {333--335},
	file = {Denning Sparse Distributed Memory:/Users/AL/Zotero/storage/XXL9W2QD/Denning Sparse Distributed Memory.pdf:application/pdf}
}

@misc{noauthor_sparse_2018,
	title = {Sparse distributed memory},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Sparse_distributed_memory&oldid=822493454},
	abstract = {Sparse distributed memory (SDM) is a mathematical model of human long-term memory introduced by Pentti Kanerva in 1988 while he was at NASA Ames Research Center. It is a generalized random-access memory (RAM) for long (e.g., 1,000 bit) binary words. These words serve as both addresses to and data for the memory. The main attribute of the memory is sensitivity to similarity, meaning that a word can be read back not only by giving the original write address but also by giving one close to it, as measured by the number of mismatched bits (i.e., the Hamming distance between memory addresses).
SDM implements transformation from logical space to physical space using distributed data representation and storage, similarly to encoding processes in human memory. A value corresponding to a logical address is stored into many physical addresses. This way of storing is robust and not deterministic. A memory cell is not addressed directly. If input data (logical addresses) are partially damaged at all, we can still get correct output data.
The theory of the memory is mathematically complete and has been verified by computer simulation. It arose from the observation that the distances between points of a high-dimensional space resemble the proximity relations between concepts in human memory. The theory is also practical in that memories based on it can be implemented with conventional RAM-memory elements.},
	language = {en},
	urldate = {2018-04-08},
	journal = {Wikipedia},
	month = jan,
	year = {2018},
	note = {Page Version ID: 822493454},
	file = {Snapshot:/Users/AL/Zotero/storage/2JHTHYG7/index.html:text/html}
}

@article{Anwar2003,
	title = {Sparse distributed memory for ‘conscious’ software agents},
	volume = {4},
	issn = {13890417},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1389041703000159},
	doi = {10.1016/S1389-0417(03)00015-9},
	abstract = {In this work we are reporting a case study on the use of SDM as the associative memory for a software agent, CMattie, whose architecture is modeled on human cognition. Sparse distributed memory (SDM) is a content-addressable memory technique that relies on close memory items tending to be clustered together. In this work, we used an enhanced version of SDM augmented with the use of genetic algorithms as an associative memory in our ‘conscious’ software agent, CMattie, who is responsible for emailing seminar announcements in an academic department. Interacting with seminar organizers via email in natural language, CMattie can replace the secretary who normally handles such announcements. SDM is a key ingredient in a complex agent architecture that implements global workspace theory, a psychological theory of consciousness and cognition. In this architecture, SDM, as the primary memory for the agent, provides associations with incoming percepts. These include disambiguation of the percept by removing noise, correcting misspellings, and adding missing pieces of information. It also retrieves behaviors and emotions associated with the percept. These associations are based on previous similar percepts, and their consequences, that have been recorded earlier. SDM also possesses several key psychological features. Some enhancements to SDM including multiple writes of important items, use of error detection and correction, and the use of hashing to map the original information into ﬁxed size keys were used. Test results indicate that SDM can be used successfully as an associative memory in such complex agent architectures. The results show that SDM is capable of recovering a percept based on a part of that percept, and ﬁnding defaults for empty perception registers. The evaluation of suggested actions and emotional states is satisfactory. We think that this work opens the door to more scientiﬁc and empirical uses for SDM.},
	language = {en},
	number = {4},
	urldate = {2018-04-08},
	journal = {Cognitive Systems Research},
	author = {Anwar, Ashraf and Franklin, Stan},
	month = dec,
	year = {2003},
	pages = {339--354},
	file = {Anwar and Franklin - 2003 - Sparse distributed memory for ‘conscious’ software.pdf:/Users/AL/Zotero/storage/MK7JS478/Anwar and Franklin - 2003 - Sparse distributed memory for ‘conscious’ software.pdf:application/pdf}
}

@article{snaider_integer_2013,
	title = {Integer sparse distributed memory: {Analysis} and results},
	volume = {46},
	issn = {08936080},
	shorttitle = {Integer sparse distributed memory},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608013001354},
	doi = {10.1016/j.neunet.2013.05.005},
	abstract = {Sparse distributed memory is an auto-associative memory system that stores high dimensional Boolean vectors. Here we present an extension of the original SDM, the Integer SDM that uses modular arithmetic integer vectors rather than binary vectors. This extension preserves many of the desirable properties of the original SDM: auto-associativity, content addressability, distributed storage, and robustness over noisy inputs. In addition, it improves the representation capabilities of the memory and is more robust over normalization. It can also be extended to support forgetting and reliable sequence storage. We performed several simulations that test the noise robustness property and capacity of the memory. Theoretical analyses of the memory’s fidelity and capacity are also presented.},
	language = {en},
	urldate = {2018-04-08},
	journal = {Neural Networks},
	author = {Snaider, Javier and Franklin, Stan and Strain, Steve and George, E. Olusegun},
	month = oct,
	year = {2013},
	pages = {144--153},
	file = {Snaider et al. - 2013 - Integer sparse distributed memory Analysis and re.pdf:/Users/AL/Zotero/storage/CMKICS3Z/Snaider et al. - 2013 - Integer sparse distributed memory Analysis and re.pdf:application/pdf;Snaider et al. - 2013 - Integer sparse distributed memory Analysis and re.pdf:/Users/AL/Zotero/storage/C5KRHNI6/Snaider et al. - 2013 - Integer sparse distributed memory Analysis and re.pdf:application/pdf}
}

@article{franklin_lida:_2014,
	title = {{LIDA}: {A} {Systems}-level {Architecture} for {Cognition}, {Emotion}, and {Learning}},
	volume = {6},
	issn = {1943-0604, 1943-0612},
	shorttitle = {{LIDA}},
	url = {http://ieeexplore.ieee.org/document/6587077/},
	doi = {10.1109/TAMD.2013.2277589},
	abstract = {We describe a cognitive architecture (LIDA) that affords attention, action selection and human-like learning intended for use in controlling cognitive agents that replicate human experiments as well as performing real-world tasks. LIDA combines sophisticated action selection, motivation via emotions, a centrally important attention mechanism, and multimodal instructionalist and selectionist learning. Empirically grounded in cognitive science and cognitive neuroscience, the LIDA architecture employs a variety of modules and processes, each with its own effective representations and algorithms. LIDA has much to say about motivation, emotion, attention, and autonomous learning in cognitive agents. In this paper we summarize the LIDA model together with its resulting agent architecture, describe its computational implementation, and discuss results of simulations that replicate known experimental data. We also discuss some of LIDA’s conceptual modules, propose non-linear dynamics as a bridge between LIDA’s modules and processes and the underlying neuroscience, and point out some of the differences between LIDA and other cognitive architectures. Finally, we discuss how LIDA addresses some of the open issues in cognitive architecture research.},
	language = {en},
	number = {1},
	urldate = {2018-04-08},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Franklin, Stan and Madl, Tamas and D'Mello, Sidney and Snaider, Javier},
	month = mar,
	year = {2014},
	pages = {19--41},
	file = {Franklin et al. - 2014 - LIDA A Systems-level Architecture for Cognition, .pdf:/Users/AL/Zotero/storage/ANQVDB4C/Franklin et al. - 2014 - LIDA A Systems-level Architecture for Cognition, .pdf:application/pdf}
}

@article{snaider_modular_2014,
	title = {Modular {Composite} {Representation}},
	volume = {6},
	issn = {1866-9956, 1866-9964},
	url = {http://link.springer.com/10.1007/s12559-013-9243-y},
	doi = {10.1007/s12559-013-9243-y},
	language = {en},
	number = {3},
	urldate = {2018-04-08},
	journal = {Cognitive Computation},
	author = {Snaider, Javier and Franklin, Stan},
	month = sep,
	year = {2014},
	pages = {510--527},
	file = {Snaider and Franklin - 2014 - Modular Composite Representation.pdf:/Users/AL/Zotero/storage/3KWR6FGY/Snaider and Franklin - 2014 - Modular Composite Representation.pdf:application/pdf;Snaider and Franklin - 2014 - Modular Composite Representation.pdf:/Users/AL/Zotero/storage/Z4HRMN33/Snaider and Franklin - 2014 - Modular Composite Representation.pdf:application/pdf}
}

@article{silva_reconfigurable_2004,
	title = {Reconfigurable co-processor for {Kanerva}'s sparse distributed memory},
	volume = {28},
	issn = {01419331},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0141933104000043},
	doi = {10.1016/j.micpro.2004.01.003},
	abstract = {The implementation on hardware of the ﬁrst layer of Kanerva’s sparse distributed memory (SDM) is presented in this work. The hardware consist on a co-processor board for connection on ISA standard bus of an IBM –PCcompatible computer. The board, named reconﬁgurable co-processor for SDM(RC-SDM), comprises on Xilinx FPGAs, local random access memory and bus interface circuits. Based on in-system reconﬁguration capacity of FPGAs, RC-SDM easily allows change of the characteristics of SDM topology implemented. First results show a speed-up of four times of RC-SDM in relation to a software implementation of the algorithm.},
	language = {en},
	number = {3},
	urldate = {2018-04-07},
	journal = {Microprocessors and Microsystems},
	author = {Silva, Marcus Tadeu Pinheiro and Braga, Antônio Pádua and Lacerda, Wilian Soares},
	month = apr,
	year = {2004},
	pages = {127--134},
	file = {Silva et al. - 2004 - Reconfigurable co-processor for Kanerva's sparse d.pdf:/Users/AL/Zotero/storage/IQZX3IX6/Silva et al. - 2004 - Reconfigurable co-processor for Kanerva's sparse d.pdf:application/pdf}
}

@book{waidyasooriya2018design,
  title={Design of FPGA-Based Computing Systems with OpenCL},
  author={Waidyasooriya, Hasitha Muthumala and Hariyama, Masanori and Uchiyama, Kunio},
  year={2018},
  publisher={Springer}
}


@inproceedings{czajkowski_opencl_2012,
	title = {From {O}pen{CL} to high-performance hardware on {FPGAS}},
	isbn = {978-1-4673-2256-0},
	url = {http://ieeexplore.ieee.org/document/6339272/},
	doi = {10.1109/FPL.2012.6339272},
	abstract = {We present an OpenCL compilation framework to generate high-performance hardware for FPGAs. For an OpenCL application comprising a host program and a set of kernels, it compiles the host program, generates Verilog HDL for each kernel, compiles the circuit using Altera Complete Design Suite 12.0, and downloads the compiled design onto an FPGA. We can then run the application by executing the host program on a Windows(tm)-based machine, which communicates with kernels on an FPGA using a PCIe interface.},
	language = {en},
	urldate = {2018-04-08},
	publisher = {IEEE},
	booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	author = {Czajkowski, Tomasz S. and Aydonat, Utku and Denisenko, Dmitry and Freeman, John and Kinsner, Michael and Neto, David and Wong, Jason and Yiannacouras, Peter and Singh, Deshanand P.},
	month = aug,
	year = {2012},
	pages = {531--534},
	file = {Czajkowski et al. - 2012 - From opencl to high-performance hardware on FPGAS.pdf:/Users/AL/Zotero/storage/LBJ69QPC/Czajkowski et al. - 2012 - From opencl to high-performance hardware on FPGAS.pdf:application/pdf}
}

@article{van2014top,
  title={The top 100 papers},
  author={Van Noorden, Richard and Maher, Brendan and Nuzzo, Regina},
  journal={Nature},
  volume={514},
  number={7524},
  pages={550},
  year={2014}
}


@url{only-one-life,
	title = {You only get one life},
	url = {https://www.slideshare.net/linhares/you-only-get-one-life},
	urldate = {2018-04-09},
	Lastchecked = {2018-03-08},
	journal = {Slideshare},
	author = {Linhares, Alexandre},
	month = dec,
	year = {2007},
	file = {You only get one life:/Users/AL/Zotero/storage/AWMBRR4P/you-only-get-one-life.html:text/html}
}
@book{IEEE1996,
	Date-Added = {2012-01-24 11:37:03 -0200},
	Date-Modified = {2012-01-24 11:43:32 -0200},
	Institution = {IEEE/ISO/IEC},
	Publisher = {IEEE/ISO/IEC},
	Series = {ISBN 1-55937-573-0},
	Title = {ISO/IEC 9948-1:1990 IEEE Std. 1003.1-1996 Information Technology Portable Operating System Interface (POSIX) Part 1 System Application Program Interface (API) [C Language]},
	Year = {1996}}

@article{morris1991,
	Author = {Max. D. Morris},
	Date-Added = {2012-01-23 15:20:58 -0200},
	Date-Modified = {2012-01-23 15:21:47 -0200},
	Journal = {Technometrics},
	Pages = {161-174},
	Title = {Factorial Sampling Plans for Preliminary Computational Experiments},
	Volume = {33},
	Year = {1991}}

@inproceedings{rao1995natural,
  title={Natural basis functions and topographic memory for face recognition},
  author={Rao, Rajesh PN and Ballard, Dana H},
  booktitle = {Proceedings of the 14th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	pages={10--19},
  year={1995}
}
@article{norman2003modeling,
  title={Modeling hippocampal and neocortical contributions to recognition memory: a complementary-learning-systems approach.},
  author={Norman, Kenneth A and O'reilly, Randall C},
  journal={{Psychological Review}},
  volume={110},
  number={4},
  pages={611},
  year={2003},
  publisher={American Psychological Association}
}
@article{harary1988survey,
      title={A survey of the theory of hypercube graphs},
      author={Harary, Frank and Hayes, John P and Wu, Horng-Jyh},
      journal={Computers \& Mathematics with Applications},
      volume={15},
      number={4},
      pages={277--289},
      year={1988},
      publisher={Elsevier}
}
@article{hely1997new,
  title={A new approach to {Kanerva's} sparse distributed memory},
  author={Hely, Tim A and Willshaw, David J and Hayes, Gillian M},
  journal={IEEE Transactions on Neural Networks},
  volume={8},
  number={3},
  pages={791--794},
  year={1997},
  publisher={IEEE}
}

@article{Meng2009,
	Author = {Meng and others},
	Date-Added = {2011-12-30 02:03:20 +0000},
	Date-Modified = {2011-12-30 02:04:17 +0000},
	Journal = {Proceedings of International Joint Conference on Neural Networks},
	Month = {June},
	Title = {A Modified Sparse Distributed Memory Model for Extracting Clean Patterns from Noisy Inputs},
	Year = {2009}}

@inproceedings{mendes2008robot,
  title={Robot navigation using a sparse distributed memory},
  author={Mendes, Mateus and Cris{\'o}stomo, Manuel and Coimbra, A Paulo},
  booktitle={Robotics and automation, 2008. ICRA 2008. IEEE international conference on},
  pages={53--58},
  year={2008},
  organization={IEEE}
}

@article{Rajesh1998,
	Author = {Rajesh Rao and Olac Fuentes},
	Date-Added = {2011-12-30 02:00:39 +0000},
	Date-Modified = {2011-12-30 02:01:46 +0000},
	Journal = {Autonomous Robots},
	Pages = {297-316},
	Title = {Hierarchical Learning of Navigational Behaviors in an Autonomous Robot using a Predictive Sparse Distributed Memory},
	Year = {1998}}


@unpublished{Snaider2011,
	Author = {Javier Snaider and Stan Franklin},
	Date-Added = {2011-12-30 01:56:26 +0000},
	Date-Modified = {2011-12-30 01:58:48 +0000},
	Note = {Paper presented at the Biological Inspired Cognitive Architectures 2011, Washington D.C. USA.},
	Title = {Extended Sparse Distributed Memory}}

@url{GPL,
	Author = {Richard Stallman},
	Date-Added = {2011-12-29 19:19:43 +0000},
	Date-Modified = {2011-12-29 19:23:07 +0000},
	Lastchecked = {2011-12-28},
	Title = {GNU General Public License},
	Url = {http://www.gnu.org/copyleft/gpl.html},
	Bdsk-Url-1 = {http://www.gnu.org/copyleft/gpl.html}}

@book{Anderson1981,
	Author = {J. A. Anderson and G. E. Hinton},
	Date-Added = {2011-12-29 18:23:51 +0000},
	Date-Modified = {2011-12-29 18:27:16 +0000},
	Publisher = {Lawrence Erlbaum Associates},
	Title = {Models of information processing in the brain},
	Year = {1981}}

@article{Hopfield1982,
	Author = {J. J. Hopfield},
	Date-Added = {2011-12-29 18:22:12 +0000},
	Date-Modified = {2011-12-29 18:23:34 +0000},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {79},
	Pages = {2554-2558},
	Title = {Neural networks and physical systems with emergent collective computational abiities},
	Volume = {8},
	Year = {1982}}

@techreport{Hinton1984,
	Author = {G. E. Hinton and others},
	Date-Added = {2011-12-29 18:15:11 +0000},
	Date-Modified = {2011-12-29 18:21:24 +0000},
	Institution = {Department of Computer Science, Carnegie-Mellon University},
	Number = {CMU-CS-84-119},
	Title = {Boltzman Machines: Constraint Satisfaction Networks that Learn},
	Year = {1984}}

@article{Anderson1983,
	Author = {J. Anderson},
	Date-Added = {2011-12-29 18:07:16 +0000},
	Date-Modified = {2011-12-29 18:09:11 +0000},
	Journal = {IEEE Transactions on Systems, Man and Cybernetics},
	Number = {13},
	Pages = {799-815},
	Title = {Cognitive and psychological computation with neural models},
	Volume = {5},
	Year = {1983}}

@article{Anderson1970,
	Author = {J. Anderson},
	Date-Added = {2011-12-29 18:05:55 +0000},
	Date-Modified = {2011-12-29 18:07:02 +0000},
	Journal = {Mathematical Biosciences},
	Number = {8},
	Pages = {137-160},
	Title = {Two models for memory organization using interacting traces},
	Year = {1970}}

@article{Anderson1968,
	Author = {J. Anderson},
	Date-Added = {2011-12-29 18:04:32 +0000},
	Date-Modified = {2011-12-29 18:05:51 +0000},
	Journal = {Kybernetic},
	Number = {5},
	Pages = {113-119},
	Title = {A memory storage module utilizing spatial correlation functions},
	Volume = {3},
	Year = {1968}}

@article{Aleksander1970,
	Author = {I. Aleksander},
	Date-Added = {2011-12-29 17:59:55 +0000},
	Date-Modified = {2011-12-29 18:02:42 +0000},
	Journal = {International Journal of Man-Machine Studies},
	Number = {2},
	Pages = {189-212},
	Title = {Some psychological properties of digital learning nets},
	Year = {1970}}

@book{Hawkins2004,
	Author = {Jeff Hawkings and Sandra Blakeslee},
	Date-Added = {2011-12-29 17:05:28 +0000},
	Date-Modified = {2011-12-29 17:08:01 +0000},
	Edition = {1},
	Publisher = {Times Books},
	Title = {On Intelligence},
	Year = {2004}}

@article{Abe2009,
	Author = {M. Abe},
	Journal = {Market Science},
	Owner = {CYG},
	Pages = {541-553},
	Timestamp = {2011.05.10},
	Title = {{"C}ounting Your Customers{"} One by One: A Hierarchical Bayes Extension to the Pareto/NBD Model},
	Volume = {28},
	Year = {2009}}

@article{Ahna1997,
	Author = {J.{-}H. Ahna and K.J. Ezawa},
	Journal = {Decision Support Systems},
	Owner = {CYG},
	Pages = {17-27},
	Timestamp = {2011.05.10},
	Title = {Decision support for real{-}time telemarketing operations through Bayesian network learning},
	Volume = {21},
	Year = {1997}}

@article{Alvesson1990,
	Author = {M. Alvesson},
	Journal = {Organization Studies},
	Owner = {CYG},
	Pages = {373-394},
	Timestamp = {2011.05.10},
	Title = {Organization: From substance to image?},
	Volume = {11},
	Year = {1990}}

@book{Ariely2010,
	Address = {New York},
	Author = {D. Ariely},
	Owner = {CYG},
	Publisher = {Harper},
	Timestamp = {2011.05.13},
	Title = {The Upside of Irrationality: The Unexpected Benefits of Defying Logic at Work and at Home},
	Year = {2010}}

@book{Ariely2008,
	Address = {New York},
	Author = {D. Ariely},
	Owner = {CYG},
	Publisher = {Harper},
	Timestamp = {2011.05.13},
	Title = {Predictably Irrational: The Hidden Forces That Shape Our Decisions},
	Year = {2008}}

@article{Assisi2007,
	Author = {C. Assisi and M. Stopfer and G. Laurent and M. Bazhenov},
	Journal = {Nature Neuroscience},
	Owner = {CYG},
	Pages = {1176-1184},
	Timestamp = {2011.05.10},
	Title = {Adaptive regulation of sparseness by feedforward inhibition},
	Volume = {10},
	Year = {2007}}

@article{Basel2004,
	Author = {B. Basel},
	Journal = {Commentarii Mathematici Helvetici},
	Owner = {CYG},
	Pages = {317-340},
	Timestamp = {2011.05.10},
	Title = {Families of strong KT structures in six dimensions},
	Volume = {79},
	Year = {2004}}

@article{Boland1989,
	Author = {P. J. Boland},
	Journal = {The Statistician},
	Owner = {CYG},
	Pages = {181-189},
	Timestamp = {2011.05.10},
	Title = {Majority systems and the {C}ondorcet {J}ury {T}heorem},
	Volume = {38},
	Year = {1989}}

@article{Borisyuk2000,
	Author = {R. Borisyuk and M. Denham and F. Hoppensteadt and Y. Kazanovich and O. Vinogradova},
	Journal = {Biosystems},
	Owner = {CYG},
	Pages = {265-272},
	Timestamp = {2011.05.10},
	Title = {An oscillatory neural network model of sparse distributed memory and novelty detection},
	Volume = {58},
	Year = {2000}}

@mastersthesis{Brogliato2011,
	Author = {M. S. Brogliato},
	Owner = {cyg},
	School = {Escola Brasileira de Administra\c{c}\~ao P\'ublica e de Empresas - EBAPE, Funda\c{c}\~ao Getulio Vargas},
	Timestamp = {2011.05.20},
	Title = {Understanding the Critical Distance in Sparse Distributed Memory},
	Year = {2011}}

@article{brogliato2014sparse,
	title={Sparse distributed memory: understanding the speed and robustness of expert memory},
	author={Brogliato, Marcelo S and Chada, Daniel M and Linhares, Alexandre},
	journal={Frontiers in Human Neuroscience},
	volume={8},
	pages={222},
	year={2014},
	publisher={Frontiers}
}

@article{Cancho2001,
	Author = {R.F. Cancho and R. Sol\'e},
	Journal = {Proceedings of the Royal Society B},
	Owner = {CYG},
	Pages = {2261-2265},
	Timestamp = {2011.05.10},
	Title = {The Small World of Human Language},
	Volume = {268},
	Year = {2001}}

@article{Corley2000,
	Author = {K.G. Corley and D.A. Gioia},
	Journal = {Corporate Reputation Review},
	Owner = {CYG},
	Pages = {319-333},
	Timestamp = {2011.05.10},
	Title = {The Rankings Game: Managing Business School Reputation},
	Volume = {3},
	Year = {2000}}

@article{Cornelissen2002,
	Author = {J. Cornelissen and R. Thorpe},
	Journal = {European Management Journal},
	Owner = {CYG},
	Pages = {172-178},
	Timestamp = {2011.05.10},
	Title = {Measuring a business school{'}s reputation: perspectives, problems, and prospects},
	Volume = {20},
	Year = {2002}}

@article{Cowan2001,
	Author = {N. Cowan},
	Journal = {Behavioral and Brain Sciences},
	Owner = {CYG},
	Pages = {87-185},
	Timestamp = {2011.05.10},
	Title = {The magical number 4 in short-term memory: {A} reconsideration of mental storage capacity},
	Volume = {24},
	Year = {2000}}

@article{Damoulas2009,
	Author = {T. Damoulas and M.A. Girolami},
	Journal = {Pattern Recognition},
	Owner = {CYG},
	Pages = {2671-2683},
	Timestamp = {2011.05.10},
	Title = {Combining feature spaces for classification},
	Volume = {42},
	Year = {2009}}

@article{Dichev1999,
	Author = {I.D. Dichev},
	Journal = {Journal of Business},
	Month = {April},
	Number = {2},
	Owner = {CYG},
	Pages = {201-213},
	Timestamp = {2011.05.10},
	Title = {How Good Are Business School Rankings?},
	Volume = {72},
	Year = {1999}}

@article{Ehrenberg2001,
	Author = {R.G. Ehrenberg and J.G. Cheslock and J. Epifantseva and { }},
	Journal = {Review of Higher Education},
	Owner = {CYG},
	Pages = {15-37},
	Timestamp = {2011.05.10},
	Title = {Paying Our Presidents: What Do Trustees Value?},
	Volume = {25},
	Year = {2001}}

@article{Elsbach1996,
	Author = {K.D. Elsbach and R.M. Kramer},
	Journal = {Administrative Science Quarterly},
	Owner = {CYG},
	Pages = {442-476},
	Timestamp = {2011.05.10},
	Title = {Members{'} Responses to Organizational Identity Threats: Encountering and Countering the Business Week Rankings},
	Volume = {41},
	Year = {1996}}

@article{Fee2005,
	Author = {C.E. Fee and C.J. Hadlock and J.R. Pierce and { }},
	Journal = {Financial Management},
	Owner = {CYG},
	Pages = {143-166},
	Timestamp = {2011.05.10},
	Title = {Business School Rankings and Business School Deans: A Study of Nonprofit Governance},
	Volume = {34},
	Year = {2005}}

@article{Florian2007,
	Author = {R.V. Florian},
	Journal = {Scientometrics},
	Owner = {CYG},
	Pages = {25-32},
	Timestamp = {2011.05.10},
	Title = {Irreproducibility of the results of the Shanghai academic ranking of world universities},
	Volume = {72},
	Year = {2007}}

@inproceedings{French1997,
	Author = {R. M. French},
	Booktitle = {Proceedings of the 1997 International Conference on New Trends in Cognitive Science},
	Editor = {A. Riegler and M. Peschl},
	Organization = {Austrian Society for Cognitive Science},
	Owner = {CYG},
	Pages = {158-163},
	Timestamp = {2011.05.10},
	Title = {When Coffee Cups Are Like Old Elephants, or Why Representation Modules Dont Make Sense},
	Year = {1997}}

@article{Gavetti2005,
	Author = {G. Gavetti and D.A. Levinthal and J.W. Rivkin and { }},
	Journal = {Strategic Management Journal},
	Month = {August},
	Number = {8},
	Owner = {CYG},
	Pages = {691-712},
	Timestamp = {2011.05.09},
	Title = {Strategy making in novel and complex worlds: The power of analogy},
	Volume = {26},
	Year = {2005}}

@unpublished{Gavetti2007,
	Author = {G. Gavetti and M. Warglien},
	Month = {Oct.},
	Note = {Revise and resubmit, Administrative Science Quarterly},
	Owner = {CYG},
	Timestamp = {2011.05.13},
	Title = {Recognizing the New: A Multi-Agent Model of Analogy in Strategic Decision-Making},
	Year = {2007}}

@inproceedings{Gayler2003,
	Address = {Sydney, Australia},
	Author = {R.W. Gayler},
	Booktitle = {ICCS ASCS International Conference on Cognitive Science},
	Editor = {Peter Slezak},
	Organization = {University of New South Wales},
	Owner = {CYG},
	Pages = {133-138},
	Timestamp = {2011.05.09},
	Title = {Vector Symbolic Architectures answer Jackendoff{'}s challenges for cognitive neuroscience},
	Year = {2003}}

@article{Gioia2002,
	Author = {D.A. Gioia and K.G. Corley},
	Journal = {Academy of Management Learning {\&} Education},
	Month = {september},
	Number = {1},
	Owner = {CYG},
	Pages = {107-120},
	Timestamp = {2011.05.10},
	Title = {Being Good Versus Looking Good: Business School Rankings and the Circean Transformation from Substance to Image},
	Volume = {1},
	Year = {2002}}

@article{Gioia2000,
	Author = {D.A. Gioia and M. Schultz and K.G. Corley and { }},
	Journal = {Academy of Management Review},
	Owner = {CYG},
	Pages = {63-81},
	Timestamp = {2011.05.10},
	Title = {Organizational identity, image and adaptive instability},
	Volume = {25},
	Year = {2000}}

@electronic{busweek2008,
	Author = {Geoff Gloeckler and Fred Jespersen and Louis Lavelle and { }},
	Note = {acessed 23-05-2011},
	Owner = {cyg},
	Timestamp = {2011.05.23},
	Title = {The Best U.S. B-Schools Of 2008},
	Url = {www.businessweek.com/interactive\_reports /mba\_domestic\_2008.html},
	Year = {2008},
	Bdsk-Url-1 = {www.businessweek.com/interactive%5C_reports%20/mba%5C_domestic%5C_2008.html}}

@article{Gobet2001,
	Author = {F. Gobet and P.C.R. Lane and S. Croker and P.C.H. Cheng and G. Jones and I. Oliver and J.M. Pine},
	Journal = {Trends in Cognitive Science},
	Owner = {CYG},
	Pages = {236-243},
	Timestamp = {2011.05.09},
	Title = {Chunking mechanisms in human learning},
	Volume = {5},
	Year = {2001}}

@article{Gobet2000,
	Author = {F. Gobet and H.A. Simon},
	Journal = {Cognitive Science},
	Owner = {CYG},
	Pages = {651-682},
	Timestamp = {2011.05.09},
	Title = {Five seconds or sixty? Presentation time in expert memory},
	Volume = {24},
	Year = {2000}}

@book{Hofstadter1985,
	Author = {D.R. Hofstadter},
	Owner = {CYG},
	Publisher = {Basic Books},
	Timestamp = {2011.05.09},
	Title = {Metamagical Themas: Questing for the Essence of Mind and Pattern},
	Year = {1985}}

@book{Hofstadter1995,
	Address = {New York, NY},
	Author = {D. Hofstadter and FARG},
	Owner = {CYG},
	Publisher = {Basic Books},
	Timestamp = {2011.05.13},
	Title = {Fluid Concept and Creative Analogies: Computer Models of The Fundamental Mechanisms of Thought},
	Year = {1995}}

@article{Ioannidis2007,
	Author = {J.P.A. Ioannidis and N.A. Patsopoulos and F.K. Kavvoura and A. Tatsioni and E. Evangelou and I. Kouri and D.G. Contopoulos-Ioannidis and G. Liberopoulos},
	Journal = {BMC Medicine},
	Owner = {CYG},
	Pages = {30},
	Timestamp = {2011.05.10},
	Title = {International ranking systems for universities and institutions: a critical appraisal},
	Volume = {5},
	Year = {2007}}

@INPROCEEDINGS{Karlsson95afast,
	    author = {Roland Karlsson},
	    title = {A Fast Activation Mechanism for the Kanerva SDM Memory},
	    booktitle = {SICS Research Report R95:10, Swedish Institute of Computer Science},
	    year = {1995},
	    pages = {69--70},
		url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.44.5112&rep=rep1&type=pdf}
}

@article{aleksander1982computer,
  title={Computer vision systems for industry},
  author={Aleksander, Igor and Stonham, Thomas John and Wilkie, BA},
  journal={Digital Systems for Industrial Automation},
  volume={1},
  number={4},
  pages={305--320},
  year={1982}
}

@inproceedings{Kanerva1994,
	Address = {London},
	Author = {P. Kanerva},
	Booktitle = {ICANN {'}94, Proceedings of International Conference on Artificial Neural Networks},
	Editor = {M. Marinaro and P.G. Morasso},
	Owner = {CYG},
	Pages = {226-229},
	Publisher = {Springer-Verlag},
	Timestamp = {2011.05.09},
	Title = {The Spatter Code for encoding concepts at many levels},
	Volume = {1},
	Year = {1994}}

@article{Kanerva2009,
	Author = {P. Kanerva},
	Journal = {Cognitive Computation},
	Owner = {CYG},
	Pages = {139-159},
	Timestamp = {2011.05.09},
	Title = {Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors},
	Volume = {1},
	Year = {2009}}

@inproceedings{Kanerva1996,
	Author = {P. Kanerva},
	Booktitle = {In ICANN96, Artificial Neural Networks},
	Pages = {869-873},
	Publisher = {Springer},
	Title = {Binary Spatter-Coding of Ordered K-Tuples},
	Year = {1996}}

@conference{Kanerva1993,
	Address = {Denver Colorado},
	Author = {P. Kanerva},
	Owner = {CYG},
	Timestamp = {2011.05.09},
	Title = {Large patterns make great symbols: An example of learning from example},
	Year = {1993}}

@book{Kanerva1988,
	Author = {P. Kanerva},
	Owner = {CYG},
	Publisher = {MIT Press},
	Timestamp = {2011.05.10},
	Title = {Sparse Distributed Memory},
	Year = {1988}}

@phdthesis{Kemp2007,
	Author = {C. Kemp},
	Owner = {CYG},
	School = {MIT},
	Timestamp = {2011.05.10},
	Title = {The acquisition of inductive constraints},
	Year = {2007}}

@article{Kemp2008,
	Author = {C. Kemp and J.B. Tenenbaum},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {31},
	Owner = {CYG},
	Pages = {10687-10692},
	Timestamp = {2011.05.10},
	Title = {The Discovery of Structural Form},
	Volume = {105},
	Year = {2008}}

@article{fan1997genetic,
  title={A genetic sparse distributed memory approach to the application of handwritten character recognition},
  author={Fan, Kuo-Chin and Wang, Yuan-Kai},
  journal={Pattern Recognition},
  volume={30},
  number={12},
  pages={2015--2022},
  year={1997},
  publisher={Elsevier}
}

@article{Kim2005,
	Author = {Y. Kim and W. Nick Street and W. and G.J. Russell and F. Menczer},
	Journal = {Management Science},
	Number = {2},
	Owner = {CYG},
	Pages = {264-276},
	Timestamp = {2011.05.13},
	Title = {Customer Targeting: A Neural Network Approach Guided by Genetic Algorithms},
	Volume = {51},
	Year = {2005}}

@article{Loennstedt2005,
	Author = {I. L\"onnstedt and A. Britton},
	Journal = {Biostatistics},
	Owner = {CYG},
	Pages = {279-291},
	Timestamp = {2011.05.10},
	Title = {Hierarchical Bayes models for cDNA microarray gene expression},
	Volume = {6},
	Year = {2005}}

@article{Linhares2000,
	Author = {Alexandre Linhares},
	Journal = {Artificial Intelligence},
	Owner = {CYG},
	Pages = {251-270},
	Timestamp = {2011.05.10},
	Title = {A glimpse at the metaphysics of Bongard problems},
	Url = {https://www.sciencedirect.com/science/article/pii/S0004370200000424},
	Volume = {121},
	Year = {2000}}

@article{Linhares2007,
	Author = {Alexandre Linhares and Paulo R.S. Brum},
	Journal = {Cognitive Science},
	Owner = {CYG},
	Pages = {989-1007},
	Timestamp = {2011.05.09},
	Title = {Understanding our understanding of strategic scenarios: What role do chunks play?},
	Volume = {31},
	Year = {2007}}

@article{Linhares2011,
	Author = {Alexandre Linhares and Daniel M. Chada and Christian N. Aranha},
	Doi = {10.1371/journal.pone.0015592},
	Journal = {PLOS One},
	Month = {Jan},
	Number = {1},
	Owner = {CYG},
	Pages = {e15592},
	Timestamp = {2011.05.19},
	Title = {The Emergence of {Miller's Magic Number on a Sparse Distributed Memory}},
	Volume = {6},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1371/journal.pone.0015592}}

@article{Linhares2010,
	Author = {Alexandre Linhares and Anna Elizabeth T.A. Freitas},
	Journal = {New Ideas in Psychology},
	Owner = {CYG},
	Pages = {64-78},
	Timestamp = {2011.05.09},
	Title = {Questioning Chase and Simon{'}s (1973) {"}Perception in chess{"}: The {"}experience recognition{"} hypothesis},
	Volume = {28},
	Year = {2010}}

@article{Liu2005,
	Author = {N. Cai Liu and Y. Cheng},
	Journal = {Higher Education in Europe},
	Owner = {CYG},
	Pages = {127-136},
	Timestamp = {2011.05.10},
	Title = {The academic ranking of world universities},
	Volume = {30},
	Year = {2005}}

@article{Lloyd2002,
	Author = {S. Lloyd},
	Journal = {Physics Review Letters},
	Owner = {CYG},
	Pages = {237901-1:4},
	Timestamp = {2011.05.10},
	Title = {Computational capacity of the Universe},
	Volume = {88},
	Year = {2002}}

@article{Lock2010,
	Author = {K. Lock and A. Gelman},
	Journal = {Political Analysis},
	Number = {3},
	Owner = {CYG},
	Pages = {337-348},
	Timestamp = {2011.05.10},
	Title = {Bayesian Combination of State Polls and Election Forecasts},
	Volume = {18},
	Year = {2010}}

@article{Miller1955,
	Author = {G. A. Miller},
	Journal = {{Psychological Review}},
	Owner = {CYG},
	Pages = {81-97},
	Timestamp = {2011.05.09},
	Title = {The magical number seven, plus or minus two: Some limits on our capacity for processing information},
	Volume = {63},
	Year = {1955}}

@article{Natter2001,
	Author = {M. Natter and A. Mild and M. Feurstein and G. Dorffner and A. Taudes},
	Journal = {Management Science},
	Number = {8},
	Owner = {CYG},
	Pages = {1029-1045},
	Timestamp = {2011.05.13},
	Title = {The Effect of Incentive Schemes and Organizational Arrangements on the New Product Development Process},
	Volume = {47},
	Year = {2001}}

@inproceedings{Perfors2009,
	Address = {Austin TX},
	Author = {A. Perfors and J.B. Tenenbaum},
	Booktitle = {Proceedings of the 31st Annual Conference of the Cognitive Science Society},
	Editor = {N. Taatgen and H. van Rijn and L. Schomaker and J. Nerbonne},
	Organization = {Cognitive Science Society},
	Owner = {CYG},
	Pages = {136-141},
	Publisher = {Cognitive Science Society},
	Timestamp = {2011.05.10},
	Title = {Learning to Learn Categories},
	Year = {2009}}

@article{Perfors2011,
	Author = {A. Perfors and J.B. Tenenbaum and T. Regier and { }},
	Journal = {Cognition},
	Number = {3},
	Owner = {cyg},
	Pages = {306-338},
	Timestamp = {2011.05.19},
	Title = {The Learnability of Abstract Syntactic Principles},
	Volume = {118},
	Year = {2011}}

@article{Pfeffer2004,
	Author = {J. Pfeffer and C.T. Fong},
	Journal = {Journal of Management Studies},
	Number = {8},
	Owner = {CYG},
	Pages = {1501-1520},
	Timestamp = {2011.05.10},
	Title = {The Business School {"}Business{"}: Some Lessons From the {U.S.} Experience},
	Volume = {41},
	Year = {2004}}

@article{Piramuthu1998,
	Author = {S. Piramuthu and H. Ragavan and M.J. Shaw and { }},
	Journal = {Management Science},
	Number = {3},
	Owner = {CYG},
	Pages = {416-430},
	Timestamp = {2011.05.13},
	Title = {Using Feature Construction to Improve the Performance of Neural Networks},
	Volume = {44},
	Year = {1998}}

@book{Plate2003,
	Address = {Stanford CA},
	Author = {T.A. Plate},
	Owner = {CYG},
	Publisher = {CSLI Publications},
	Timestamp = {2011.05.09},
	Title = {Holographic reduced representations: Distributed representations for cognitive structures},
	Year = {2003}}

@book{Ries1993,
	Address = {New York},
	Author = {A. Ries and J. Trout},
	Journal = {Management Science},
	Owner = {CYG},
	Publisher = {HarperCollins},
	Timestamp = {2011.05.13},
	Title = {The 22 immutable laws of marketing},
	Year = {1993}}

@article{Siemens2005,
	Author = {J.C. Siemens and S. Burton and T. Jensen and N.A. Mendoza},
	Journal = {Journal of Business Research},
	Month = {April},
	Number = {4},
	Owner = {CYG},
	Pages = {467-476},
	Timestamp = {2011.05.10},
	Title = {An examination of the relationship between research productivity in prestigious business journals and popular press business school rankings},
	Volume = {58},
	Year = {2005}}

@article{Smith2004,
	Author = {J.E. Smith and D. von Winterfeldt},
	Journal = {Management Science},
	Owner = {CYG},
	Pages = {561-574},
	Timestamp = {2011.05.13},
	Title = {Decision Analysis in Management Science},
	Volume = {50},
	Year = {2004}}

@inbook{Stewart2009,
	Author = {T.C. Stewart and M. Eliasmith},
	Chapter = {Compositionality and Biologically Plausible Models},
	Editor = {Markus Werning and Wolfram Hinzen and Edouard Machery},
	Owner = {CYG},
	Publisher = {Oxford},
	Timestamp = {2011.05.09},
	Title = {Oxford Handbook of Compositionality},
	Year = {2009}}

@article{Tracy1997,
	Author = {J. Tracy and J. Waldfogel},
	Journal = {Journal of Business},
	Owner = {CYG},
	Pages = {1-31},
	Timestamp = {2011.05.10},
	Title = {The best business schools: a market{-}based approach},
	Volume = {70},
	Year = {1997}}

@article{Trieschmann2000,
	Author = {J.S. Trieschmann and A.R. Dennis and G.B. Northcraft and A.W. Niemi Jr.},
	Journal = {Academy of Management Journal},
	Owner = {CYG},
	Pages = {1130-1141},
	Timestamp = {2011.05.10},
	Title = {Serving Multiple Constituencies in Business Schools: {M.B.A.} Program versus Research Performance},
	Volume = {43},
	Year = {2000}}

@article{Zell2001,
	Author = {D. Zell},
	Journal = {Journal of Management Inquiry},
	Owner = {CYG},
	Pages = {324-338},
	Timestamp = {2011.05.10},
	Title = {The Market{-}Driven Business School: Has the Pendulum Swung Too Far?},
	Volume = {10},
	Year = {2001}}

@unpublished{Zhoua,
	Author = {W. Zhoua and G. Kapoor},
	Journal = {Decision Support Systems},
	Note = {accepted for publication, doi:10.1016/j.dss.2010.08.007},
	Owner = {CYG},
	Timestamp = {2011.05.10},
	Title = {Detecting evolutionary financial statement fraud}}

@unpublished{Zhouaa,
	Author = {Z.{-}J. Zhoua and C.{-}H. Hu and D.{-}L. Xu and J.{-}B. Yangc and D.{-}H. Zhou},
	Doi = {10.1016/j.eswa.2010.09.055},
	Note = {Expert Systems with Applications, accepted for publication},
	Owner = {CYG},
	Timestamp = {2011.05.10},
	Title = {Bayesian reasoning approach based recursive algorithm for online updating belief rule based expert system of pipeline leak detection},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.eswa.2010.09.055}}

@inproceedings{chan2011orthogonal,
  title={Orthogonal range searching on the RAM, revisited},
  author={Chan, Timothy M and Larsen, Kasper Green and P{\u{a}}tra{\c{s}}cu, Mihai},
  booktitle={Proceedings of the twenty-seventh annual symposium on Computational geometry},
  pages={1--10},
  year={2011},
  organization={ACM}
}

@article{chazelle1988functional,
  title={A functional approach to data structures and its use in multidimensional searching},
  author={Chazelle, Bernard},
  journal={SIAM Journal on Computing},
  volume={17},
  number={3},
  pages={427--462},
  year={1988},
  publisher={SIAM}
}

@article{norouzi2014fast,
  title={Fast exact search in hamming space with multi-index hashing},
  author={Norouzi, Mohammad and Punjani, Ali and Fleet, David J},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={36},
  number={6},
  pages={1107--1119},
  year={2014},
  publisher={IEEE}
}

@inproceedings{kluyver2016jupyter,
  title={Jupyter Notebooks-a publishing format for reproducible computational workflows.},
  author={Kluyver, Thomas and Ragan-Kelley, Benjamin and P{\'e}rez, Fernando and Granger, Brian E and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica B and Grout, Jason and Corlay, Sylvain and others},
  booktitle={ELPUB},
  pages={87--90},
  year={2016}
}

@inproceedings{pai2004linux,
  title={Linux 2.6 performance improvement through readahead optimization},
  author={Pai, Ram and Pulavarty, Badari and Cao, Mingming},
  booktitle={Proceedings of the Linux Symposium},
  volume={2},
  pages={105--116},
  year={2004}
}

@book{cover2012elements,
  title={Elements of information theory},
  author={Cover, Thomas M and Thomas, Joy A},
  year={2012},
  publisher={John Wiley \& Sons}
}

@book{warren2013hacker,
  title={Hacker's delight},
  author={Warren, Henry S},
  year={2013},
  publisher={Pearson Education}
}

@article{shen2014interactive,
  title={Interactive notebooks: Sharing the code},
  author={Shen, Helen},
  journal={Nature News},
  volume={515},
  number={7525},
  pages={151},
  year={2014}
}

@phdthesis{chada2016you,
  title={Are you experienced? Contributions towards experience recognition, cognition, and decision making},
  author={Chada, Daniel de Magalh{\~a}es},
  year={2016}
}



@url{sdmframework,
	Author = {Marcelo Brogliato},
	Lastchecked = {2018-03-05},
	Title = {{SDM Framework Documentation}},
	Url = {http://sdm-framework.readthedocs.io/}}

@inproceedings{munshi2009opencl,
  title={The {OpenCL} specification},
  author={Munshi, Aaftab},
  booktitle={Hot Chips 21 Symposium (HCS), 2009 IEEE},
  pages={1--314},
  year={2009},
  organization={IEEE}
}


@book{ferguson_computer_2002,
	title = {Computer {Wars}: {The} {Post}-{IBM} {World}},
	isbn = {978-1-58798-139-5},
	shorttitle = {Computer {Wars}},
	abstract = {The citizenship curriculum, which became statutory in 2002, aims to create informed citizens by enabling pupils to play an effective role in society. This series examines the institutions, rights and responsibilities that underpin our lives in the UK and relates them to the experience of the reader. Each book looks at a different aspect of UK society, such as the law, national and local government or the media.},
	language = {en},
	publisher = {Beard Books},
	author = {Ferguson, Charles H. and Morris, Charles R.},
	month = sep,
	year = {2002},
	note = {Google-Books-ID: hOdAStd3mR4C},
	keywords = {Business \& Economics / Infrastructure, Computers / Social Aspects / General}
}

@article{de1995geometrical,
  title={Geometrical treatment and statistical modelling of the distribution of patterns in the n-dimensional Boolean space},
  author={de P{\'a}dua Braga, Ant{\^o}nio and Aleksander, Igor},
  journal={Pattern Recognition Letters},
  volume={16},
  number={5},
  pages={507--515},
  year={1995},
  publisher={Elsevier}
}

@article{deng2012mnist,
  title={The MNIST database of handwritten digit images for machine learning research [best of the web]},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{mnist,
  title={{The MNIST database of handwritten digit images for machine learning research [best of the web]}},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}




@techreport{sjodin_improving_1995,
	title = {Improving the {Capacity} of {SDM}},
	abstract = {A more e cient way of reading the SDM memory is presented. This is accomplished by using implicit information, hitherto not utilized, to nd the information-carrying units and thus removing unnecessary noise when reading the memory.},
	language = {en},
	number = {R95:12},
	institution = {Swedish Institute of Computer Science},
	author = {Sjodin, Gunnar},
	pages = {55},
	file = {Fulltext:/Users/AL/Zotero/storage/Y3J7PE3Z/Sjödin - 1995 - Improving the capacity of SDM.zip:application/x-compress;Sjodin - Improving the Capacity of SDM.pdf:/Users/AL/Zotero/storage/YTW24MBN/Sjodin - Improving the Capacity of SDM.pdf:application/pdf;Snapshot:/Users/AL/Zotero/storage/9DJ277DA/2152.html:text/html}
}


@inproceedings{chou_capacity_1988,
	title = {The {Capacity} of the {Kanerva} {Associative} {Memory} is {Exponential}},
	abstract = {The capacity of an associative memory is defined as the maximum number of vords that can be stored and retrieved reliably by an address vithin a given sphere of attraction. It is shown by sphere packing arguments that as the address length increases. the capacity of any associati ve memory is limited to an exponential grovth rate of 1 - h2( 0). vhere h2(0) is the binary entropy function in bits. and 0 is the radius of the sphere of attraction. This exponential grovth in capacity can actually be achieved by the Kanerva associative memory. if its parameters are optimally set . Formulas for these op.timal values are provided. The exponential grovth in capacity for the Kanerva associative memory contrasts sharply vith the sub-linear grovth in capacity for the Hopfield associative memory.},
	language = {en},
	author = {Chou, Philip A},
	booktitle={Neural information processing systems},
    pages={184--191},
    year={1988},
	file = {Chou - The Capacity of the Kanerva Associative Memory is .pdf:/Users/AL/Zotero/storage/SITCY73H/Chou - The Capacity of the Kanerva Associative Memory is .pdf:application/pdf}
}

@article{chou_capacity_1989,
	title = {The capacity of the {Kanerva} associative memory},
	volume = {35},
	issn = {0018-9448},
	doi = {10.1109/18.32123},
	abstract = {Asymptotic expressions for the capacity of an associative memory proposed by P. Kanerva (1984) are derived. Capacity is defined as the maximum number of random binary words that can be stored at random addresses so that the probability that a word is in error is arbitrarily small when it is retrieved by an n-bit address containing fewer than δn errors, δ⩽1/2. Sphere-packing arguments show that the capacity of any associative memory can grow exponentially in n at a rate of at most 1-h2(δ), where h2(δ) is the binary entropy function in bits. It turns out that the Kanerva associative memory achieves this upper bound when its parameters are optimally set. Thus, the capacity of the Kanerva associative memory has an exponential growth rate equal to the rate of the best information-theoretic codes, that is 1-h 2(δ). However, the Kanerva memory achieves its exponential growth in capacity at the expense of an exponential growth in hardware},
	number = {2},
	journal = {IEEE Transactions on Information Theory},
	author = {Chou, P. A.},
	month = mar,
	year = {1989},
	keywords = {Artificial neural networks, Associative memory, asymptotic expressions, binary entropy function, Brain modeling, capacity, Capacity planning, Computational modeling, content-addressable storage, Entropy, Hardware, Humans, information theory, information-theoretic codes, Kanerva associative memory, neural nets, neural networks, random addresses, random binary words, sphere packing arguments, upper bound, Upper bound, Vector quantization},
	pages = {281--298},
	file = {Chou - 1989 - The capacity of the Kanerva associative memory.pdf:/Users/AL/Zotero/storage/9U9E32SB/Chou - 1989 - The capacity of the Kanerva associative memory.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/AL/Zotero/storage/QAGU39JH/32123.html:text/html}
}



@inproceedings{turk_kanervas_1995,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'95},
	title = {Kanerva's {Sparse} {Distributed} {Memory}: {An} {Object}-oriented {Implementation} on the {Connection} {Machine}},
	isbn = {978-1-55860-363-9},
	shorttitle = {Kanerva's {Sparse} {Distributed} {Memory}},
	url = {http://dl.acm.org/citation.cfm?id=1625855.1625917},
	abstract = {This paper reports on an implementation of Kanerva's Sparse Distributed Memory for the Connection Machine. In order to accomplish a modular and adaptive software library we applied a plain object-oriented programming style to the Common Lisp extension ltsp. Some variations of the original model, the selected coordinate design, the hyperplane design, and a new general design, as well as the folded SDM due to Kanerva are realized. It has been necessary to elaborate a uniform presentation of the theoretical foundations the different designs are based on. We demonstrate the simulator's functionality with some simple applications. Runtime comparisons are given. We encourage the use of our simulation tool when outlining research topics of special interest to SDM.},
	urldate = {2018-04-12},
	booktitle = {Proceedings of the 14th {International} {Joint} {Conference} on {Artificial} {Intelligence} - {Volume} 1},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Turk, Andreas and Gorz, Gunther},
	year = {1995},
	pages = {473--479},
	file = {Turk and Gorz - 1995 - Kanerva's Sparse Distributed Memory An Object-ori.pdf:/Users/AL/Zotero/storage/9MMZYPT6/Turk and Gorz - 1995 - Kanerva's Sparse Distributed Memory An Object-ori.pdf:application/pdf}
}




@inproceedings{surkan_wsdm:_1992,
	address = {New York, NY, USA},
	series = {{APL} '92},
	title = {{WSDM}: {Weighted} {Sparse} {Distributed} {Memory} {Prototype} {Expressed} in {APL}},
	isbn = {978-0-89791-477-2},
	shorttitle = {{WSDM}},
	url = {http://doi.acm.org/10.1145/144045.144142},
	doi = {10.1145/144045.144142},
	abstract = {A functional style application of APL notation succinctly describes the architecture and principles of operation of one kind of connection-based computer. In the future it is expected that these machines will have thousands of processors and large arrays of dynamic connections. APL programs running on von Neumann computers now provide precise descriptions of connection-based machines which are convenient for exploring the potential of connection-based computation. Experience with radically new structures and different principles of operation for neural network problem solving can be obtained using virtual machines provided by software. Virtual machines are described by functions programmed on conventional computers.
Two adaptive variants of the sparse distributed memory or SDM (Kanerva [1991]) show improved efficiency. The demonstrated superiority of Kanerva's new pattern weighting idea can be obtained by improved coding of the input patterns. This coding is done by generally defined preprocessing of features of representative binary input patterns. Transformed input patterns select addresses which pack distributed memories more efficiently.
Coding is done by first computing customized weight vectors for each input pattern vector. Individual weighting of each pattern leads to more uniform utilization of the addresses and their corresponding memory connection weights. The derived pattern weights improve discrimination between pairs of similar inputs with few significant differences. This paper is to provide the APL community access to a concise symbolic description of Kanerva's weighted SDM machine. APL's rich set of computer modeling and exposition tools have the potential of markedly accelerating software and hardware development for array-base connectionist computing.},
	urldate = {2018-04-12},
	booktitle = {Proceedings of the {International} {Conference} on {APL}},
	publisher = {ACM},
	author = {Surkan, Alvin J.},
	year = {1992},
	pages = {235--242}
}



@techreport{berchtold_processing_2005,
	title = {Processing {Sensor} {Data} with the {Common} {Sense} {Toolkit} ({CSTK})},
	url = {https://www.researchgate.net/publication/241441844_Processing_Sensor_Data_with_the_Common_Sense_Toolkit_CSTK},
	abstract = {The Common Sense Toolkit (CSTK) is a middleware framework for developers to process sensor data. It consists of diﬀerent interconnected C++ classes, conﬁgurable through a XML settings language. Consisting of diﬀerent levels of processing algorithms, the CSTK can provide a bottom-up approach to resource eﬃcient gather information from raw sensor readings. To visualize the information, several graphical visualization tools are provided. Algorithms implemented belong to the ﬁelds of clustering, classiﬁcation, topological representation, memory organization and stochastic analysis.},
	language = {en},
	urldate = {2018-04-12},
	author = {Berchtold, Martin},
	month = mar,
	year = {2005},
	pages = {35},
	file = {Berchtold - Processing Sensor Data with the Common Sense Toolk.pdf:/Users/AL/Zotero/storage/YULIRU8N/Berchtold - Processing Sensor Data with the Common Sense Toolk.pdf:application/pdf}
}


@misc{noauthor_cstk:_nodate,
	author = {The {CSTK Developers}},
	title = {{CSTK}: {The} {CommonSense} {ToolKit}},
	url = {http://cstk.sourceforge.net/},
	urldate = {2018-04-12},
	file = {CSTK\: The CommonSense ToolKit:/Users/AL/Zotero/storage/67FAJ7N7/cstk.sourceforge.net.html:text/html}
}



@article{emruli_vector_2015,
	title = {Vector space architecture for emergent interoperability of systems by learning from demonstration},
	volume = {11},
	issn = {2212-683X},
	url = {http://www.sciencedirect.com/science/article/pii/S2212683X14000784},
	doi = {10.1016/j.bica.2014.11.015},
	abstract = {The rapid integration of physical systems with cyberspace infrastructure, the so-called Internet of Things, is likely to have a significant effect on how people interact with the physical environment and design information and communication systems. Internet-connected systems are expected to vastly outnumber people on the planet in the near future, leading to grand challenges in software engineering and automation in application domains involving complex and evolving systems. Several decades of artificial intelligence research suggests that conventional approaches to making such systems automatically interoperable using handcrafted “semantic” descriptions of services and information are difficult to apply. In this paper we outline a bioinspired learning approach to creating interoperable systems, which does not require handcrafted semantic descriptions and rules. Instead, the idea is that a functioning system (of systems) can emerge from an initial pseudorandom state through learning from examples, provided that each component conforms to a set of information coding rules. We combine a binary vector symbolic architecture (VSA) with an associative memory known as sparse distributed memory (SDM) to model context-dependent prediction by learning from examples. We present simulation results demonstrating that the proposed architecture can enable system interoperability by learning, for example by human demonstration.},
	urldate = {2018-04-12},
	journal = {Biologically Inspired Cognitive Architectures},
	author = {Emruli, Blerim and Sandin, Fredrik and Delsing, Jerker},
	month = jan,
	year = {2015},
	keywords = {Artificial intelligence, Communications, Interoperability, Sparse distributed memory, System of systems, Vector symbolic architecture},
	pages = {53--64},
	file = {Emruli et al. - Vector space architecture for emergent interoperab.pdf:/Users/AL/Zotero/storage/MWU75RCD/Emruli et al. - Vector space architecture for emergent interoperab.pdf:application/pdf;ScienceDirect Snapshot:/Users/AL/Zotero/storage/3XENHWHA/S2212683X14000784.html:text/html}
}


@techreport{linhares_sparse_2018,
	title = {Sparse {Distributed} {Memory} {Framework} {Documentation}},
	url = {https://media.readthedocs.org/pdf/sdm-framework/stable/sdm-framework.pdf},
	language = {en},
	urldate = {2018-04-14},
	author = {Linhares, Alexandre and Brogliato, Marcelo Salhab},
	month = feb,
	year = {2018},
	pages = {104},
	file = {Linhares and Brogliato - Sparse Distributed Memory Framework Documentation.pdf:/Users/AL/Zotero/storage/UMPRJMNV/Linhares and Brogliato - Sparse Distributed Memory Framework Documentation.pdf:application/pdf}
}


@misc{ccrg_ccrg_nodate,
	title = {{CCRG} - {Cognitive} {Computing} {Research} {Group} - {Projects}},
	url = {http://ccrg.cs.memphis.edu/projects.html},
	urldate = {2018-04-13},
	author = {CCRG},
	file = {CCRG - Cognitive Computing Research Group - Projects:/Users/AL/Zotero/storage/KW3N5RNX/projects.html:text/html}
}

@article{chandran_cubicity_2008,
	title = {The cubicity of hypercube graphs},
	volume = {308},
	issn = {0012365X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0012365X07008308},
	doi = {10.1016/j.disc.2007.10.011},
	language = {en},
	number = {23},
	urldate = {2018-04-14},
	journal = {Discrete Mathematics},
	author = {Chandran, L. Sunil and Sivadasan, Naveen},
	month = dec,
	year = {2008},
	pages = {5795--5800},
	file = {Chandran and Sivadasan - 2008 - The cubicity of hypercube graphs.pdf:/Users/AL/Zotero/storage/69R53SH2/Chandran and Sivadasan - 2008 - The cubicity of hypercube graphs.pdf:application/pdf}
}

@article{foldes_characterization_1977,
	title = {A characterization of hypercubes},
	volume = {17},
	abstract = {A combinatorial characterization of hypercubes is given. A hypercube is defined as the undirected Hasse diagram of a Boolean lattice. The method used is inductive extension of partial isomorphisms.},
	number = {1},
	journal = {Discrete Mathematics},
	author = {Foldes, S},
	year = {1977},
	pages = {155--159},
	file = {Foldes - 1977 - A characterization of hypercubes.pdf:/Users/AL/Zotero/storage/DKR5GMQJ/Foldes - 1977 - A characterization of hypercubes.pdf:application/pdf}
}

@article{wagner_embedding_1990,
	title = {Embedding {Trees} in a {Hypercube} is {NP}-{Complete}},
	volume = {19},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/abs/10.1137/0219038},
	doi = {10.1137/0219038},
	abstract = {An important family of graphs is the n-dimensional hypercube, the graph with \$2{\textasciicircum}\{n\}\$ nodes labelled \$0,1,{\textbackslash}cdots, 2{\textasciicircum}\{n\}-1\$, and an edge joining two nodes whenever their binary representation differs in a single coordinate. The problem of deciding if a given source graph is a partial subgraph of an n-dimensional cube has recently been shown to be NP-complete. In this paper the same problem on a very restricted family of source graphs, trees, is considered. It is shown that the problem of determining for a given tree T and integer k if T is a partial subgraph of a k-dimensional cube is NP-complete.},
	number = {3},
	urldate = {2018-04-14},
	journal = {SIAM Journal on Computing},
	author = {Wagner, A. and Corneil, D.},
	month = jun,
	year = {1990},
	pages = {570--590},
	file = {Snapshot:/Users/AL/Zotero/storage/63JVCBDF/0219038.html:text/html}
}

@article{laborde_another_1982,
	title = {Another characterization of hypercubes},
	volume = {39},
	issn = {0012-365X},
	url = {http://www.sciencedirect.com/science/article/pii/0012365X8290139X},
	doi = {10.1016/0012-365X(82)90139-X},
	abstract = {Résumé
Nous montrons que dans la classe des graphes connexes tels que deux arêtes incidentes quelconques appartiennent á un et un seul quadrilatére, les hypercubes finis sont les graphes de degré minimum n fini et possédant 2n sommets. The following theorem1 is proved: Let C be the class connected graphs such that each pair of distinct adjacent edges lies in exactly one 4-cycle. Then G in C is a finite hypercube ifthe minimum degree δ of G is finite and2 {\textbar}V(G){\textbar} = 2δ.},
	number = {2},
	urldate = {2018-04-14},
	journal = {Discrete Mathematics},
	author = {Laborde, Jean-Marie and Rao Hebbare, Surya Prakash},
	month = jan,
	year = {1982},
	pages = {161--166},
	file = {ScienceDirect Full Text PDF:/Users/AL/Zotero/storage/5GQI7VR8/Laborde and Rao Hebbare - 1982 - Another characterization of hypercubes.pdf:application/pdf;ScienceDirect Snapshot:/Users/AL/Zotero/storage/MJBPTCX4/0012365X8290139X.html:text/html}
}


@book{ruskey_combinatorial_2003,
	address = {University of Victoria},
	edition = {Working version (1j-CSC 425/520)},
	title = {Combinatorial {Generation}},
	url = {http://www.1stworks.com/ref/RuskeyCombGen.pdf},
	urldate = {2018-04-14},
	author = {Ruskey, Frank},
	year = {2003},
	file = {RuskeyCombGen.pdf:/Users/AL/Zotero/storage/XA6NHWC9/RuskeyCombGen.pdf:application/pdf}
}

@book{douglas1980godel,
  title={G{\"o}del, {E}scher, {B}ach: An eternal golden braid},
  author={Hofstadter, Douglas},
  publisher = {Basic Books},
  year={1980}
}

@phdthesis{foundalis_phaeaco:_2006,
	title = {{PHAEACO}: a cognitive architecture inspired by {Bongard}'s problems},
	shorttitle = {Phaeaco},
	abstract = {Phaeaco is a cognitive architecture for visual pattern recognition that starts at the ground level of receiving pixels as input, and works its way through creating abstract representations of geometric figures formed by those pixels. Phaeaco can tell how similar such figures are by using a psychologically plausible metric to compute a difference value among representations, and use that value to group figures together, if possible. Groups of figures are represented by statistical attributes (average, standard deviation, and other statistics), and serve as the basis for a formed and thereafter learned concept (e.g., triangle), stored in long-term memory. Phaeaco focuses on the Bongard problems, a set of puzzles in visual categorization, and applies its cognitive principles in its efforts to solve them, faring nearly as well as humans in the puzzles it manages to solve.},
	school = {Indiana University},
	author = {Foundalis, Harry E},
	year = {2006},
	file = {Foundalis - 2006 - PHAEACO A COGNITIVE ARCHITECTURE INSPIRED BY BONG.pdf:/Users/AL/Zotero/storage/8S645ZWK/Foundalis - 2006 - PHAEACO A COGNITIVE ARCHITECTURE INSPIRED BY BONG.pdf:application/pdf}
}

@article{lupyan_difficulties_2013,
	title = {The difficulties of executing simple algorithms: {Why} brains make mistakes computers don’t},
	volume = {129},
	issn = {0010-0277},
	shorttitle = {The difficulties of executing simple algorithms},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027713001728},
	doi = {10.1016/j.cognition.2013.08.015},
	abstract = {It is shown that educated adults routinely make errors in placing stimuli into familiar, well-defined categories such as triangle and odd number. Scalene triangles are often rejected as instances of triangles and 798 is categorized by some as an odd number. These patterns are observed both in timed and untimed tasks, hold for people who can fully express the necessary and sufficient conditions for category membership, and for individuals with varying levels of education. A sizeable minority of people believe that 400 is more even than 798 and that an equilateral triangle is the most “trianglest” of triangles. Such beliefs predict how people instantiate other categories with necessary and sufficient conditions, e.g., grandmother. I argue that the distributed and graded nature of mental representations means that human algorithms, unlike conventional computer algorithms, only approximate rule-based classification and never fully abstract from the specifics of the input. This input-sensitivity is critical to obtaining the kind of cognitive flexibility at which humans excel, but comes at the cost of generally poor abilities to perform context-free computations. If human algorithms cannot be trusted to produce unfuzzy representations of odd numbers, triangles, and grandmothers, the idea that they can be trusted to do the heavy lifting of moment-to-moment cognition that is inherent in the metaphor of mind as digital computer still common in cognitive science, needs to be seriously reconsidered.},
	number = {3},
	urldate = {2018-04-15},
	journal = {Cognition},
	author = {Lupyan, Gary},
	month = dec,
	year = {2013},
	keywords = {Categorization, Concepts, Distributed representations, Human algorithms, Inference, Prototypes},
	pages = {615--636},
	file = {ScienceDirect Snapshot:/Users/AL/Zotero/storage/7F98MB53/S0010027713001728.html:text/html}
}


@book{bongard_recognition_1968,
	title = {The recognition problem},
	url = {http://www.dtic.mil/docs/citations/AD0682462},
	abstract = {The book deals with provisional solutions of as yet mathematically unformulated problems encountered in the design of machines that make use of a recognition function. A survey of work that has been done in a variety of approaches to the problem is developed beginning with conditioned-reflex and neuron theory and Rosenblatt's 'Perceptron.' Topics include similarity search, receptor-space transformation, recognition vs. simulation, causes of poor system performance, a 'discover-the-law' game, the information-theoretical usefulness concept, and statistical criteria. An analogy is drawn with learning in a child, where instruction is based not on direct modification of the physical system itself, but by indirect stimulation of its receptors. Appendixes deal with hypotheses containing only truth, optimum hypotheses, and nonlogarithmic optimum determining algorithms, and, lastly, present a set of 100 recognition problems for a program. (Author)},
	language = {en},
	urldate = {2018-04-15},
	author = {Bongard, M. M.},
	month = jul,
	year = {1968},
	file = {Snapshot:/Users/AL/Zotero/storage/AWLPC56H/AD0682462.html:text/html}
}


@misc{misiti_awesome-machine-learning:_2018,
	title = {{Awesome-Machine-Learning}: {A} curated list of awesome {Machine} {Learning} frameworks, libraries and software},
	shorttitle = {awesome-machine-learning},
	url = {https://github.com/josephmisiti/awesome-machine-learning},
	urldate = {2018-04-15},
	author = {Misiti, Joseph},
	month = apr,
	year = {2018},
	note = {original-date: 2014-07-15T19:11:19Z},
	file = {Snapshot:/Users/AL/Zotero/storage/SHU75HPF/awesome-machine-learning.html:text/html}
}

@misc{lewis_awesome-artificial-intelligence:_2018,
	title = {{Awesome-Artificial-Intelligence}: a curated list of {Artificial} {Intelligence} ({AI}) courses, books, video lectures and papers},
	shorttitle = {awesome-artificial-intelligence},
	url = {https://github.com/owainlewis/awesome-artificial-intelligence},
	urldate = {2018-04-15},
	author = {Lewis, Owain},
	month = apr,
	year = {2018},
	note = {original-date: 2015-01-27T09:27:48Z},
	keywords = {artificial-intelligence, deep-learning, intelligent-machines, intelligent-systems, machine-intelligence, machine-learning, reinforcement-learning},
	file = {Snapshot:/Users/AL/Zotero/storage/24SJ2BUM/awesome-artificial-intelligence.html:text/html}
}


@misc{im/ufrj_programa_nodate,
	title = {Programa de {Pós}-{Graduação} em {Ensino} de {Matemática}},
	url = {http://www.pg.im.ufrj.br/pemat/dout_publico.htm},
	urldate = {2018-04-15},
	author = {IM/UFRJ},
	file = {pemat_estrutura.pdf:/Users/AL/Zotero/storage/ID3I4HFW/pemat_estrutura.pdf:application/pdf;Programa de Pós-Graduação em Ensino de Matemática:/Users/AL/Zotero/storage/BY28MICD/dout_publico.html:text/html}
}


@book{plous_psychology_1993,
	title = {The psychology of judgment and decision making.},
	publisher = {Mcgraw-Hill Book Company},
	author = {Plous, Scott},
	year = {1993},
	file = {Snapshot:/Users/AL/Zotero/storage/DQBFXTJS/1993-97429-000.html:text/html}
}





@article{meyer_tip---tongue_1992,
	title = {The tip-of-the-tongue phenomenon: {Blocking} or partial activation?},
	volume = {20},
	number = {6},
	journal = {Memory \& Cognition},
	author = {Meyer, Antje and Bock, Kathryn},
	year = {1992},
	pages = {715--726},
	file = {Meyer and Bock - 1992 - The tip-of-the-tongue phenomenon Blocking or part.pdf:/Users/AL/Zotero/storage/UP2Z2PXJ/Meyer and Bock - 1992 - The tip-of-the-tongue phenomenon Blocking or part.pdf:application/pdf}
}

@article{brown_review_1991,
	title = {A {Review} of the {Tip}-of-the-{Tongue} {Experience}},
	volume = {109},
	language = {en},
	number = {2},
	journal = {Psychological Bulletin},
	author = {Brown, Alan S},
	year = {1991},
	pages = {204--223},
	file = {Brown - A Review of the Tip-of-the-Tongue Experience.pdf:/Users/AL/Zotero/storage/5SQ57S88/Brown - A Review of the Tip-of-the-Tongue Experience.pdf:application/pdf}
}

@article{brown_tip_1966,
	title = {The {Tip} of the {Tongue} phenomenon},
	volume = {5},
	journal = {Journal of Verbal Learning and Verbal Behavior},
	author = {Brown, Roger and McNeill, David},
	year = {1966},
	pages = {325--337},
	file = {The_Tip_of_the_Tongue_phenomenon.pdf:/Users/AL/Zotero/storage/9T9E2CA9/The_Tip_of_the_Tongue_phenomenon.pdf:application/pdf}
}


@article{bowers_biological_2009,
	title = {On the biological plausibility of grandmother cells: {Implications} for neural network theories in psychology and neuroscience.},
	volume = {116},
	issn = {1939-1471, 0033-295X},
	shorttitle = {On the biological plausibility of grandmother cells},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0014462},
	doi = {10.1037/a0014462},
	abstract = {A fundamental claim associated with parallel distributed processing (PDP) theories of cognition is that knowledge is coded in a distributed manner in mind and brain. This approach rejects the claim that knowledge is coded in a localist fashion, with words, objects, and simple concepts (e.g. “dog”), that is, coded with their own dedicated representations. One of the putative advantages of this approach is that the theories are biologically plausible. Indeed, advocates of the PDP approach often highlight the close parallels between distributed representations learned in connectionist models and neural coding in brain and often dismiss localist (grandmother cell) theories as biologically implausible. The author reviews a range a data that strongly challenge this claim and shows that localist models provide a better account of single-cell recording studies. The author also contrast local and alternative distributed coding schemes (sparse and coarse coding) and argues that common rejection of grandmother cell theories in neuroscience is due to a misunderstanding about how localist models behave. The author concludes that the localist representations embedded in theories of perception and cognition are consistent with neuroscience; biology only calls into question the distributed representations often learned in PDP models.},
	language = {en},
	number = {1},
	urldate = {2018-04-19},
	journal = {Psychological Review},
	author = {Bowers, Jeffrey S.},
	year = {2009},
	pages = {220--251},
	file = {Bowers - 2009 - On the biological plausibility of grandmother cell.pdf:/Users/AL/Zotero/storage/Z9QDVHYT/Bowers - 2009 - On the biological plausibility of grandmother cell.pdf:application/pdf}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{silver2017masteringchess,
  title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@incollection{kahneman2013prospect,
  title={Prospect theory: An analysis of decision under risk},
  author={Kahneman, Daniel and Tversky, Amos},
  booktitle={Handbook of the fundamentals of financial decision making: Part I},
  pages={99--127},
  year={2013},
  publisher={World Scientific}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}


@inproceedings{kwon_atm_1998,
	title = {{ATM} call admission control using sparse distributed memory. {II}},
	volume = {3},
	booktitle = {Neural {Networks} {Proceedings}, 1998. {IEEE} {World} {Congress} on {Computational} {Intelligence}. {The} 1998 {IEEE} {International} {Joint} {Conference} on},
	publisher = {IEEE},
	author = {Kwon, Hee-Yong and Kim, Dong-Keyu and Song, Seung-Jun and CHoi, Je-U. and Lee, In-Heang and Hwang, Hee-Yeung},
	year = {1998},
	pages = {1799--1803},
	file = {Kwon et al. - 1998 - ATM call admission control using sparse distribute.pdf:/Users/AL/Zotero/storage/KHWSXQ26/Kwon et al. - 1998 - ATM call admission control using sparse distribute.pdf:application/pdf;Snapshot:/Users/AL/Zotero/storage/PAMZ7VP9/687130.html:text/html}
}

@inproceedings{hee-yong_kwon_atm_1997,
	title = {{ATM} call admission control using sparse distributed memory},
	volume = {2},
	isbn = {978-0-7803-4122-7},
	url = {http://ieeexplore.ieee.org/document/616226/},
	doi = {10.1109/ICNN.1997.616226},
	abstract = {We have proposed a neural network call admission control(CAC) method based on sparse distributed memory(SDM). CAC is a key technology of ATM network traffic control. It should be adaptable to the rapid and various changes of the ATM network environment. Conventional approaches to the ATM CAC require network analysis in detail in all cases. The optimal implementation is said to be very difficult. Therefore, neural approaches have recently been employed. However, it does not meet the adaptability requirements. We, thus, have proposed a method which based on SDM as the neural network controller. Since SDM is an RAM-like associative memory, it has the property of good adaptability. It provides CAC with good adaptability to manage changes. Experimental results are as good as those of the previous neural approaches without additional analytical data, and without relearning from initial state.},
	language = {en},
	urldate = {2018-05-09},
	booktitle = {Neural {Networks}, 1997., {International} {Conference} on},
	publisher = {IEEE},
	author = {Kwon, Hee-Yong},
	year = {1997},
	pages = {1321--1325},
	file = {Hee-Yong Kwon - 1997 - ATM call admission control using sparse distribute.pdf:/Users/AL/Zotero/storage/P3C4AZE3/Hee-Yong Kwon - 1997 - ATM call admission control using sparse distribute.pdf:application/pdf}
}

@inproceedings{jockel_sparse_2009,
	title = {Sparse distributed memory for experience-based robot manipulation},
	isbn = {978-1-4244-2678-2},
	url = {http://ieeexplore.ieee.org/document/4913187/},
	doi = {10.1109/ROBIO.2009.4913187},
	abstract = {Sparse distributed memory (SDM) is a mathematical technique based on the properties of high-dimensional space for storing and retrieving large binary patterns. This model has been proposed for cerebellar functions, and has been used in simple visual and linguistic applications to date. This paper presents an SDM for robotic applications, especially for storing and recognising mobile manipulation actions of a 6-DOF robot arm. Sequences of events are stored as subjective experiences and are later used to guide robot arm behaviour based on its memory content. Several simple manipulation tasks, such as lift and place a wastebin from and on the ﬂoor, push an object aside on a tabletop, and draw shapes in the air are analysed under different operation modes. The robot system shows good reproduction abilities of task-dependent arm trajectories based on sparse distributed memory. Moreover, the content-addressable, associative memory even predicts the residual arm trajectory of a task if the arm is placed somewhere close to a learnt trajectory.},
	language = {en},
	urldate = {2018-05-09},
	booktitle = {Proceedings of the 2008 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}},
	publisher = {IEEE},
	author = {Jockel, S. and Lindner, F. and {Jianwei Zhang}},
	month = feb,
	year = {2009},
	pages = {1298--1303},
	file = {Jockel et al. - 2009 - Sparse distributed memory for experience-based rob.pdf:/Users/AL/Zotero/storage/Z2ZCHFLQ/Jockel et al. - 2009 - Sparse distributed memory for experience-based rob.pdf:application/pdf}
}

@inproceedings{meng_modified_2009,
	title = {A modified sparse distributed memory model for extracting clean patterns from noisy inputs},
	isbn = {978-1-4244-3548-7},
	url = {http://ieeexplore.ieee.org/document/5178873/},
	doi = {10.1109/IJCNN.2009.5178873},
	abstract = {The Sparse Distributed Memory (SDM) proposed by Kanerva provides a simple model for human long-term memory, with a strong underlying mathematical theory. However, there are problematic features in the original SDM model that affect its efficiency and performance in real world applications and for hardware implementation. In this paper, we propose modifications to the SDM model that improve its efficiency and performance in pattern recall. First, the address matrix is built using training samples rather than random binary sequences. This improves the recall performance significantly. Second, the content matrix is modified using a simple tri-state logic rule. This reduces the storage requirements of the SDM and simplifies the implementation logic, making it suitable for hardware implementation. The modified model has been tested using pattern recall experiments. It is found that the modified model can recall clean patterns very well from noisy inputs.},
	language = {en},
	urldate = {2018-05-09},
	booktitle = {Proceedings of {International} {Joint} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Meng, Hongying and Appiah, Kofi and Hunter, Andrew and Yue, Shigang and Hobden, Mervyn and Priestley, Nigel and Hobden, Peter and Pettit, Cy},
	month = jun,
	year = {2009},
	pages = {2084--2089},
	file = {Meng et al. - 2009 - A modified sparse distributed memory model for ext.pdf:/Users/AL/Zotero/storage/RTVQA3UQ/Meng et al. - 2009 - A modified sparse distributed memory model for ext.pdf:application/pdf}
}



@book{uesaka_foundations_2001,
	address = {Stanford, Calif},
	series = {{CSLI} lecture notes},
	title = {Foundations of real-world intelligence},
	isbn = {978-1-57586-339-9 978-1-57586-338-2},
	language = {en},
	number = {no. 125},
	publisher = {CSLI Publications},
	editor = {Uesaka, Yoshinori and Kanerva, Pentti and Asoh, Hideki},
	year = {2001},
	keywords = {Artificial intelligence, Evolutionary programming (Computer science), Neural networks (Computer science)},
	file = {Uesaka et al. - 2001 - Foundations of real-world intelligence.pdf:/Users/AL/Zotero/storage/2AJBUJZW/Uesaka et al. - 2001 - Foundations of real-world intelligence.pdf:application/pdf}
}

@incollection{goos_binary_1996,
	address = {Berlin, Heidelberg},
	title = {Binary spatter-coding of ordered {K}-tuples},
	volume = {1112},
	isbn = {978-3-540-61510-1 978-3-540-68684-2},
	url = {http://link.springer.com/10.1007/3-540-61510-5_146},
	abstract = {Information with structure is traditionally organized into records with ﬁelds. For example, a medical record consisting of name, sex, age, and weight might look like (Joe, male, 66, 77). What 77 stands for is determined by its location in the record, so that this is an example of local representation. The brain’s wiring, and robustness under local damage, speak for the importance of distributed representations. The Holographic Reduced Representation (HRR) of Plate is a prime example based on real or complex vectors. This paper describes how spatter coding leads to binary HRRs, and how the ﬁelds of a record are encoded into a long binary word without ﬁelds and how they are extracted from such a word.},
	language = {en},
	urldate = {2018-04-07},
	booktitle = {Artificial {Neural} {Networks} — {ICANN} 96},
	publisher = {Springer Berlin Heidelberg},
	author = {Kanerva, Pentti},
	editor = {Goos, Gerhard and Hartmanis, Juris and Leeuwen, Jan and Malsburg, Christoph and Seelen, Werner and Vorbrüggen, Jan C. and Sendhoff, Bernhard},
	year = {1996},
	doi = {10.1007/3-540-61510-5_146},
	pages = {869--873},
	file = {Kanerva - 1996 - Binary spatter-coding of ordered K-tuples.pdf:/Users/AL/Zotero/storage/ASKWDTI6/Kanerva - 1996 - Binary spatter-coding of ordered K-tuples.pdf:application/pdf;Kanerva - 1996 - Binary spatter-coding of ordered K-tuples.pdf:/Users/AL/Zotero/storage/9W6KHHI9/Kanerva - 1996 - Binary spatter-coding of ordered K-tuples.pdf:application/pdf}
}

@incollection{kanerva_sparse_1993,
	title = {Sparse {Distributed} {Memory} and {Related} {Models}},
	language = {en},
	booktitle = {Associative {Neural} {Memories}: {Theory} and {Implementation}},
	publisher = {Oxford University Press},
	author = {Kanerva, Pentti},
	year = {1993},
	pages = {41},
	file = {Kanerva - Sparse Distributed Memory and Related Models.pdf:/Users/AL/Zotero/storage/VQIFKXZT/Kanerva - Sparse Distributed Memory and Related Models.pdf:application/pdf;Kanerva - Sparse Distributed Memory and Related Models.pdf:/Users/AL/Zotero/storage/CVMHAQCU/Kanerva - Sparse Distributed Memory and Related Models.pdf:application/pdf;Kanerva - Sparse Distributed Memory and Related Models.pdf:/Users/AL/Zotero/storage/8TGLC6E8/Kanerva - Sparse Distributed Memory and Related Models.pdf:application/pdf}
}

@incollection{marinaro_spatter_1994,
	address = {London},
	title = {The {Spatter} {Code} for {Encoding} {Concepts} at {Many} {Levels}},
	isbn = {978-3-540-19887-1 978-1-4471-2097-1},
	url = {http://link.springer.com/10.1007/978-1-4471-2097-1_52},
	abstract = {The Spatter Code is a high-dimensional (e.g., N = 10,000), random code that encodes “high-level concepts” in terms of their “low-level attributes” so that concepts at different levels can be mixed freely. The binary spatter code is the simplest. It has two N-bit codewords for each concept or item, a “high-level,” or dense, word with many randomly placed 1s and a “low-level,” or sparse, word with a few (that are contained in the many). The dense codewords can be used as inputs to an associative memory. The sparse codewords are used in encoding new concepts. When several items (attributes, concepts, chunks) are combined to form a new item, the two codewords for the new item are made from the sparse codewords of its constituents as follows: the new dense word is the logical OR of the constituents (i.e., their sum thresholded at 0.5), and the new sparse word has 1s where the constituent words overlap (i.e., their sum thresholded at 1.5). When the parameters for the code are chosen properly, the number of 1s in the codewords is maintained as new items are encoded from combinations of old ones.},
	language = {en},
	urldate = {2018-04-07},
	booktitle = {{ICANN} ’94},
	publisher = {Springer London},
	author = {Kanerva, Pentti},
	editor = {Marinaro, Maria and Morasso, Pietro G.},
	year = {1994},
	doi = {10.1007/978-1-4471-2097-1_52},
	pages = {226--229},
	file = {Kanerva - 1994 - The Spatter Code for Encoding Concepts at Many Lev.pdf:/Users/AL/Zotero/storage/PXD7H5V6/Kanerva - 1994 - The Spatter Code for Encoding Concepts at Many Lev.pdf:application/pdf}
}

@article{keeler_comparison_1988,
	title = {Comparison between {Kanerva}'s {SDM} and {Hopfield}-type neural networks},
	volume = {12},
	number = {3},
	journal = {Cognitive Science},
	author = {Keeler, James D.},
	year = {1988},
	pages = {299--329},
	file = {Fulltext:/Users/AL/Zotero/storage/JLLN3X38/Keeler - 1988 - Comparison between Kanerva's SDM and Hopfield-type.pdf:application/pdf;KEELER - ComparisonBetweenKanerva’sSDMand Hopfield-typeNeur.pdf:/Users/AL/Zotero/storage/PABBUQX7/KEELER - ComparisonBetweenKanerva’sSDMand Hopfield-typeNeur.pdf:application/pdf;Snapshot:/Users/AL/Zotero/storage/CM6WBQDI/0364021388900262.html:text/html}
}

@inproceedings{rwcp_fully_1997,
	title = {Fully {Distributed} {Representation}},
	abstract = {A fully distributed representation based on the binary spatter code is described. It is shown how the  information of a conventional record with fields is encoded into a long random bit string, or a holistic  record, that has no fields, and how the fields are extracted from the holistic record. It is argued that  holistic representation should be used in modeling high-level mental functions.},
	booktitle = {In {Proceedings} {RWC} {Symposium}},
	author = {Pentti Kanerva},
	year = {1997},
	pages = {358--365},
	file = {Citeseer - Full Text PDF:/Users/AL/Zotero/storage/J369RAY6/Rwcp and Kanerva - 1997 - Fully Distributed Representation.pdf:application/pdf;Citeseer - Snapshot:/Users/AL/Zotero/storage/47W7FPH8/summary.html:text/html}
}

@article{kanerva_hyperdimensional_2009,
	title = {Hyperdimensional computing: {An} introduction to computing in distributed representation with high-dimensional random vectors},
	volume = {1},
	shorttitle = {Hyperdimensional computing},
	abstract = {The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture. They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.},
	number = {2},
	journal = {Cognitive Computation},
	author = {Kanerva, Pentti},
	year = {2009},
	pages = {139--159},
	file = {Hyperdimensional Computing\: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors | SpringerLink:/Users/AL/Zotero/storage/R9SK3K4B/10.html:text/html;Kanerva - 2009 - Cognitive Computation.pdf:/Users/AL/Zotero/storage/LGUGUJAQ/Kanerva - 2009 - Cognitive Computation.pdf:application/pdf;Snapshot:/Users/AL/Zotero/storage/Z42EFCJB/425931e434f6b370cc6cdd2db58873843def7d7f.html:text/html;Snapshot:/Users/AL/Zotero/storage/R6TR7UHZ/s12559-009-9009-8.html:text/html}
}

@article{sahlgren_permutations_nodate,
	title = {Permutations as a {Means} to {Encode} {Order} in {Word} {Space}},
	abstract = {We show that sequence information can be encoded into highdimensional ﬁxed-width vectors using permutations of coordinates. Computational models of language often represent words with high-dimensional semantic vectors compiled from word-use statistics. A word’s semantic vector usually encodes the contexts in which the word appears in a large body of text but ignores word order. However, word order often signals a word’s grammatical role in a sentence and thus tells of the word’s meaning. Jones and Mewhort (2007) show that word order can be included in the semantic vectors using holographic reduced representation and convolution. We show here that the order information can be captured also by permuting of vector coordinates, thus providing a general and computationally light alternative to convolution.},
	language = {en},
	author = {Sahlgren, Magnus and Kanerva, Pentti},
	pages = {6},
	file = {Sahlgren and Kanerva - Permutations as a Means to Encode Order in Word Sp.pdf:/Users/AL/Zotero/storage/GICEWLFT/Sahlgren and Kanerva - Permutations as a Means to Encode Order in Word Sp.pdf:application/pdf;Sahlgren and Kanerva - Permutations as a Means to Encode Order in Word Sp.pdf:/Users/AL/Zotero/storage/3KYKXW87/Sahlgren and Kanerva - Permutations as a Means to Encode Order in Word Sp.pdf:application/pdf;Sahlgren and Kanerva - Permutations as a Means to Encode Order in Word Sp.pdf:/Users/AL/Zotero/storage/KV4G39BC/Sahlgren and Kanerva - Permutations as a Means to Encode Order in Word Sp.pdf:application/pdf}
}

@techreport{flynn_sparse_1989,
	title = {Sparse {Distributed} {Memory}: {Principles} and {Operation}},
	abstract = {Sparse distributed memory is a generalized random-access memory (RAM) for long (e.g., 1,000 bit) binary words. Such words can be written into and read from the memory, and they can also be used to address the memory. The main attribute of the memory is sensitivity to similarity, meaning that a word can be read back not only by giving the original write address but also by giving one close to it as measured by the Hamming distance between addresses.},
	language = {en},
	number = {CSL-TR-89-400},
	institution = {NASA Ames Research Center},
	author = {Flynn, M J and Kanerva, P and Bhadkamkar, N},
	year = {1989},
	pages = {60},
	file = {Flynn et al. - Sparse Distributed Memory Principles and Operatio.pdf:/Users/AL/Zotero/storage/YHWBKAAA/Flynn et al. - Sparse Distributed Memory Principles and Operatio.pdf:application/pdf;Flynn et al. - Sparse Distributed Memory Principles and Operatio.pdf:/Users/AL/Zotero/storage/D3J8FB9L/Flynn et al. - Sparse Distributed Memory Principles and Operatio.pdf:application/pdf}
}

@article{kanerva_parallel_1985,
	title = {Parallel {Structures} in {Human} and {Computer} {Memory}},
	volume = {85},
	url = {https://cseweb.ucsd.edu/~gary/PAPER-SUGGESTIONS/SDM_Kanerva.pdf},
	urldate = {2018-05-01},
	journal = {Cognitiva},
	author = {Kanerva, Pentti},
	month = jun,
	year = {1985},
	file = {Kanerva - 1985 - Parallel Structures in Human and Computer Memory.pdf:/Users/AL/Zotero/storage/8CWR3L7X/Kanerva - 1985 - Parallel Structures in Human and Computer Memory.pdf:application/pdf}
}




@inproceedings{kang_energy-efficient_2015,
	title = {Energy-efficient and high throughput sparse distributed memory architecture},
	isbn = {978-1-4799-8391-9},
	url = {http://ieeexplore.ieee.org/document/7169194/},
	doi = {10.1109/ISCAS.2015.7169194},
	abstract = {This paper presents an energy-efﬁcient VLSI implementation of Sparse Distributed Memory (SDM). High throughput and energy-efﬁcient Hamming distance-based address decoder (CM-DEC) is proposed by employing compute memory [1], where computation is deeply embedded into a memory (SRAM). Hierarchical binary decision (HBD) is also proposed to enhance area- and energy-efﬁciency of read operation by minimizing data transfer. The SDM is employed as an auto-associative memory with four read iterations and 16×16 binary noisy input image with input error rates of 15\%, 25\%, and 30\%. The proposed SDM achieves 39× smaller energy delay product with 14.5× and 2.7× reduced delay and energy, respectively as compared to conventional digital implementation of SDM in 45 nm SOI CMOS process with output error rate degradation less than 0.4\%.},
	language = {en},
	urldate = {2018-05-08},
	booktitle = {2015 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	publisher = {IEEE},
	author = {Kang, Mingu and Kim, Eric P. and Keel, Min-sun and Shanbhag, Naresh R.},
	month = may,
	year = {2015},
	pages = {2505--2508},
	file = {Kang et al. - 2015 - Energy-efficient and high throughput sparse distri.pdf:/Users/AL/Zotero/storage/NQ9BU6PD/Kang et al. - 2015 - Energy-efficient and high throughput sparse distri.pdf:application/pdf}
}

@article{kang_-memory_2016,
	title = {In-memory {Computing} {Architectures} for {Sparse} {Distributed} {Memory}},
	volume = {10},
	abstract = {This paper presents an energy-efﬁcient and highthroughput architecture for Sparse Distributed Memory (SDM) - a computational model of the human brain [1]. The proposed SDM architecture is based on the recently proposed inmemory computing kernel for machine learning applications called Compute Memory (CM) [2], [3]. CM achieves energy and throughput efﬁciencies by deeply embedding computation into the memory array. SDM-speciﬁc techniques such as hierarchical binary decision (HBD) are employed to reduce the delay and energy further. The CM-based SDM (CM-SDM) is a mixedsignal circuit, and hence circuit-aware behavioral, energy, and delay models in a 65 nm CMOS process are developed in order to predict system performance of SDM architectures in the autoand hetero-associative modes. The delay and energy models indicate that CM-SDM, in general, can achieve up to 25× and 12× delay and energy reduction, respectively, over conventional SDM. When classifying 16×16 binary images with high noise levels (input bad pixel ratios: 15\% {\textasciitilde} 25\%) into nine classes, all SDM architectures are able to generate output bad pixel ratios (Bo) ≤ 2\%. The CM-SDM exhibits negligible loss in accuracy, i.e., its Bo degradation is within 0.4\% as compared to that of the conventional SDM.},
	language = {en},
	journal = {IEEE Transactions on Biomedical Circuits and Systems},
	author = {Kang, Mingu},
	year = {2016},
	pages = {855--862},
	file = {Kang - In-memory Computing Architectures for Sparse Distr.pdf:/Users/AL/Zotero/storage/TR85J5BE/Kang - In-memory Computing Architectures for Sparse Distr.pdf:application/pdf}
}

@inproceedings{salahuddin_energy_nodate,
	address = {Kamakura, Japan},
	title = {Energy {Efficient} {Computing} with {Hyperdimensional} {Vector} {Space} {Models}},
	language = {en},
	booktitle = {2017 {International} {Conference} on {Simulation} of {Semiconductor} {Processes} and {Devices} ({SISPAD})},
	publisher = {IEEE},
	author = {Salahuddin, Sayeef},
	pages = {9--12},
	file = {Salahuddin - Energy Efficient Computing with Hyperdimensional V.pdf:/Users/AL/Zotero/storage/MNQL2QR9/Salahuddin - Energy Efficient Computing with Hyperdimensional V.pdf:application/pdf}
}

@article{montagna_pulp-hd:_2018,
	title = {{PULP}-{HD}: {Accelerating} {Brain}-{Inspired} {High}-{Dimensional} {Computing} on a {Parallel} {Ultra}-{Low} {Power} {Platform}},
	shorttitle = {{PULP}-{HD}},
	url = {http://arxiv.org/abs/1804.09123},
	abstract = {Computing with high-dimensional (HD) vectors, also referred to as hypervectors, is a brain-inspired alternative to computing with scalars. Key properties of HD computing include a well-defined set of arithmetic operations on hypervectors, generality, scalability, robustness, fast learning, and ubiquitous parallel operations. HD computing is about manipulating and comparing large patterns—binary hypervectors with 10,000 dimensions—making its efficient realization on minimalistic ultra-low-power platforms challenging. This paper describes HD computing’s acceleration and its optimization of memory accesses and operations on a silicon prototype of the PULPv3 4-core platform (1.5 mm2, 2 mW), surpassing the stateof-the-art classification accuracy (on average 92.4\%) with simultaneous 3.7× end-to-end speed-up and 2× energy saving compared to its single-core execution. We further explore the scalability of our accelerator by increasing the number of inputs and classification window on a new generation of the PULP architecture featuring bitmanipulation instruction extensions and larger number of 8 cores. These together enable a near ideal speed-up of 18.4× compared to the single-core PULPv3.},
	language = {en},
	urldate = {2018-05-09},
	journal = {arXiv:1804.09123 [eess]},
	author = {Montagna, Fabio and Rahimi, Abbas and Benatti, Simone and Rossi, Davide and Benini, Luca},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.09123},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
	file = {Montagna et al. - 2018 - PULP-HD Accelerating Brain-Inspired High-Dimensio.pdf:/Users/AL/Zotero/storage/2B97MGRR/Montagna et al. - 2018 - PULP-HD Accelerating Brain-Inspired High-Dimensio.pdf:application/pdf}
}

@inproceedings{li_hyperdimensional_2016,
	title = {Hyperdimensional computing with 3D {VRRAM} in-memory kernels: {Device}-architecture co-design for energy-efficient, error-resilient language recognition},
	isbn = {978-1-5090-3902-9},
	shorttitle = {Hyperdimensional computing with 3D {VRRAM} in-memory kernels},
	url = {http://ieeexplore.ieee.org/document/7838428/},
	doi = {10.1109/IEDM.2016.7838428},
	abstract = {The ability to learn from few examples, known as resistance states (LRS); ‘0’: high resistance states (HRS)) (Fig. one-shot learning, is a hallmark of human cognition. 2). XOR on \{0, 1\} binary code is equivalent to MULT on \{1, -1\} Hyperdimensional (HD) computing is a brain-inspired bipolar code. Based on the binary code, MULT is therefore computational framework capable of one-shot learning, using performed by programming and evaluating XOR logic along random binary vectors with high dimensionality. Device- vertical pillars in VRRAMs (Fig. 2(a)). ADD and PERM are architecture co-design of HD cognitive computing systems performed by current summing (Fig. 2(b)) and in-memory bit using 3D VRRAM/CMOS is presented for language transfer (Fig. 2(c)), respectively. To experimentally demonstrate recognition. Multiplication-addition-permutation (MAP), the the MAP kernels, 4-layer TiN/Ti/HfOx/TiN 3D VRRAMs central operations of HD computing, are experimentally integrated with FinFET select transistors were fabricated. demonstrated on 4-layer 3D VRRAM/FinFET as non-volatile Detailed fabrication process was reported in [17]. Typical in-memory MAP kernels. Extensive cycle-to-cycle (up to 1012 DC/endurance characteristics from bottom layer-1 (L1) to top cycles) and wafer-level device-to-device (256 RRAMs) layer-4 (L4) are shown in Fig. 3. Random ‘0’s and ‘1’s, the experiments are performed to validate reproducibility and medium for HD computing, are naturally produced within robustness. For 28-nm node, the 3D in-memory architecture VRRAM utilizing the intrinsic probabilistic switching reduces total energy consumption by 52.2\% with 412 times less behaviors (Fig. 4) [18], [19]. D2D statistical distributions of area compared with LP digital design (using registers as SET probabilities (PSET) are also measured (Fig. 5), which are memory), owing to the energy-efficient VRRAM MAP kernels then incorporated into a variation-aware RRAM compact model and dense connectivity. Meanwhile, the system trained with 21 on top of cycle-to-cycle variations [20]. PSET can be tuned by samples texts achieves 90.4\% accuracy recognizing 21 programming conditions. Shorter pulses result in tighter D2D European languages on 21,000 test sentences. Hard-error spreads around certain PSET, owing to better reproducibility of analysis shows the HD architecture is amazingly resilient to filament morphology during C2C measurements (Fig. 5). 50\% RRAM endurance failures, making the use of various types of PSET (with +/- 4\% D2D variations) is used to produce random RRAMs/CBRAMs (1k {\textasciitilde} 10M endurance) feasible.},
	language = {en},
	urldate = {2018-05-09},
	booktitle = {2016 {IEEE} {International} {Electron} {Devices} {Meeting} ({IEDM})},
	publisher = {IEEE},
	author = {Li, Haitong and Wu, Tony F. and Rahimi, Abbas and Li, Kai-Shin and Rusch, Miles and Lin, Chang-Hsien and Hsu, Juo-Luen and Sabry, Mohamed M. and Eryilmaz, S. Burc and Sohn, Joon and Chiu, Wen-Cheng and Chen, Min-Cheng and Wu, Tsung-Ta and Shieh, Jia-Min and Yeh, Wen-Kuan and Rabaey, Jan M. and Mitra, Subhasish and Wong, H.-S. Philip},
	month = dec,
	year = {2016},
	pages = {16.1.1--16.1.4},
	file = {Li et al. - 2016 - Hyperdimensional computing with 3D VRRAM in-memory.pdf:/Users/AL/Zotero/storage/AW7N8E95/Li et al. - 2016 - Hyperdimensional computing with 3D VRRAM in-memory.pdf:application/pdf;Li et al. - 2016 - Hyperdimensional computing with 3D VRRAM in-memory.pdf:/Users/AL/Zotero/storage/4ZVJHHMY/Li et al. - 2016 - Hyperdimensional computing with 3D VRRAM in-memory.pdf:application/pdf}
}

@inproceedings{aguiar_sdm-go:_2013,
	title = {{SDM}-{Go}: {An} {Agent} for {Go} with an {Improved} {Search} {Process} {Based} on {Monte}-{Carlo} {Tree} {Search} and {Sparse} {Distributed} {Memory}},
	shorttitle = {{SDM}-{Go}},
	doi = {10.1109/CSE.2013.71},
	abstract = {This paper describes the player agent for Go named SDM-Go. It uses sparse distributed memory (SDM) as an additional resource to improve the classical Monte-Carlo (MC) simulation based search used by many current top player agents. In this process, the search tree used to select the best move is constructed according to the values of the game boards. The SDM-Go is built upon the Fuego framework. The contributions here are: the use of a SDM to reduce the number of MC simulations and to increase the accuracy of the calculus of the values corresponding to the game boards (nodes) that keep a certain level of similarity with some node of the SDM, the implementation of a bit based representation for the game boards such as not to compromise the performance of the system when checking the similarities among them and, to extend the use of the results of the simulations to update the values of the nodes of the SDM. The use of the SDM represents an approach independent of domain, what reduces the supervised character of the traditional MC simulations. The results obtained in tournaments against the well known open-source agent Fuego confirm the benefits provided by this approach.},
	booktitle = {2013 {IEEE} 16th {International} {Conference} on {Computational} {Science} and {Engineering}},
	author = {Aguiar, M. A. and Julia, R. M. S.},
	month = dec,
	year = {2013},
	keywords = {Agent for Go, bit based representation, computer games, distributed memory systems, Equations, Fuego, Game, game boards, Games, Hamming distance, Mathematical model, MC simulations, Monte Carlo methods, Monte Carlo Tree Search, Monte-Carlo tree search, multi-agent systems, Open source software, open-source agent, public domain software, SDM-Go, search problems, search process improvement, Simulations, software agents, sparse distributed memory, Sparse Distributed Memory, trees (mathematics), Vectors},
	pages = {424--431},
	file = {IEEE Xplore Abstract Record:/Users/AL/Zotero/storage/6RE7BHQP/6755250.html:text/html}
}


@article{wu_kanerva_2018,
	title = {The {Kanerva} {Machine}: {A} {Generative} {Distributed} {Memory}},
	shorttitle = {The {Kanerva} {Machine}},
	url = {http://arxiv.org/abs/1804.01756},
	abstract = {We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva’s sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory signiﬁcantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is signiﬁcantly easier to train.},
	language = {en},
	urldate = {2018-05-09},
	journal = {arXiv:1804.01756 [cs, stat]},
	author = {Wu, Yan and Wayne, Greg and Graves, Alex and Lillicrap, Timothy},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.01756},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2018},
	file = {Wu et al. - 2018 - The Kanerva Machine A Generative Distributed Memo.pdf:/Users/AL/Zotero/storage/G4QU8AVT/Wu et al. - 2018 - The Kanerva Machine A Generative Distributed Memo.pdf:application/pdf}
}
