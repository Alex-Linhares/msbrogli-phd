% !TEX root = ../partial-sdm.tex

The framework implements the basic operations in a Sparse Distributed Memory which may be used to create more complex operations. It is developed in C language and the OpenCL parallel framework --- which may be loaded in many platforms and programming languages --- with a wrapper in Python. The Python module makes it easy to create and execute simulations in a Sparse Distributed Memory and works properly in Jupyter Notebook \citep{kluyver2016jupyter}. It works in both Python 2 and Python 3.

We split the SDM memory in two parts: the hard-location addresses and the hard-location counters. Thus, the addresses (bitstrings) of the hard locations are stored in one array, while their counters in another. This makes possible to create multiple SDMs using the same address space, which would save computational effort to scan a bitstring in all the SDMs --- since they share the same address space, the activated hard locations will be the same in all of them. As the slowest part of reading and writing operations is scanning the address space, the performance benefits are significant.

Each part may be stored either in the RAM memory or in a file. The RAM memory is interesting for quick experiments, automated tests, and others scenarios in which the SDM may be lost, while the file is interesting for a long-term SDM, like creating an SDM file with 10,000 random writes, which will be copied over and over to run multiple experiments. The file may also be sent to another researcher or may be published within the paper to let others run their own checks and verify the results. In summary, the framework fits many different uses and necessities.

Let a SDM memory with $N$ dimensions and $H$ hard locations. Then, in a 64-bit computer, the array of hard-location addresses will use $H \cdot 8 \cdot \lceil N/64 \rceil$ bytes of memory, and there will be $H \cdot N$ hard-location counters. For example, in a SDM memory with 1,000 dimensions and 1,000,000 hard locations, using 32-bit integers for the counters, the array of addresses will use 122MB of memory and the counters will use 3.8 GB of memory.

Basic operations were grouped in four sets: (i) for bitstrings, (ii) for addresses, (iii) for counters, and (iv) for memories (SDMs). Operations include creating new bitstrings, flipping bits, generating a bitstring with a specific distance from a given bitstring, scanning the address space using different algorithms, writing a bitstring to a counter, writing in an SDM, reading from an SDM, and iteratively reading from an SDM until convergence.


\section{Bitstring}

Bitstrings are the main structure of SDM. The addresses are represented in bitstrings, as well as the data. A bitstring is stored as an array of integers. Each integer may be 16-bit, 32-bit, or 64-bit long, depending on the configuration. By default, each integer is 64-bit long.

For instance, a 1,000-bit bitstring will have $\lceil 1000/64 \rceil = 16$ integers. These integers will have a total of $16 \cdot 64 = 1,024$ bits. The remaining 24 bits are always zero, so they do not affect the result of any operation. The memory usage efficiency is $1 - 24/1024 = 97.65\%$. Bitstrings store neither how many bits they have nor the array length. These pieces of information are only stored in the address space.


\subsection{The distance between two bitstrings}

The distance between two bitstrings is calculated by the hamming distance, which is the number of different bits between them. It is calculated counting the number of ones in the exclusive or (xor) between the bitstrings, i.e., $d(x, y) = \text{number of ones in } x \oplus y$.

There are several algorithms to calculate the number of ones \citep{warren2013hacker}, but the performance depends on the processor. So, we have implemented three different algorithms and one may be selected through compiling flags. The default algorithm is to use a built-in \_\_popcnt() instruction from the compiler.

There is also the naive algorithm, which really counts the number of ones checking bit by bit. It is available only for testing purposes and should never be used.

The other algorithm available is the lookup. It pre-calculates a table with the number of ones of all possible 16-bit integers. This table is accessed a few times to calculate the number of ones of a 64-bit integer, i.e., to calculate the distance between two bitstrings, it sums the distance of each 16-bit part of the bitstrings, i.e., $d(x[0:63], y[0:63]) = d(x[0:15], y[0:15]) + d(x[16:31], y[16:31]) + d(x[32:47], y[32:47]) + d(x[48:63], y[48:63])$ where $x[i:i+15]$ and $y[i, i+15]$ are the 16-bit integers formed by the bits between $i$ and $i+15$ of $x$ and $y$, respectivelly. Each 16-bit distance is calculated through a single table access. As each distance is calculated in O(1), this algorithm runs in O($\lceil bits/16 \rceil$). This table uses 65MB of RAM. One may change the table from 16-bit integers to 32-bit integers, which would halve the number of accesses at the expense of 4GB of RAM (instead of 65MB).


\section{Address space}

An address space is a fixed collection of bitstrings, and each bitstring represents a hard-location address. They store the number of bitstrings, as well as the number of bits, number of integers per bitstring, and the number of remaining bits.

Bitstrings are stored in a contiguous array of 64-bit integers, as shown in Figure \ref{tab:hl-addresses-detail}. Hence, basic pointer arithmetic provides us with performance improvements in their access, as processors realize fetches of contiguous chunks of memory  \citep{pai2004linux}.

\begin{figure}
\centering
\begin{tikzpicture}[
mycell/.style={draw, minimum size=7mm},
matrixA/.style={matrix of nodes,
    nodes={mycell, anchor=center},
    column sep=-\pgflinewidth,
    row sep=-\pgflinewidth,
    },
matrixB/.style={matrix of nodes,
    nodes={mycell, anchor=center},
    column sep=-\pgflinewidth,
    row sep=-\pgflinewidth,
}]

\matrix[matrixA] (A) { addr$_1$ & addr$_2$ & addr$_3$ & $\cdots$ & addr$_H$ \\ };

\matrix[matrixB, below=of A] (B) {
addr$_{k, 1}$ & addr$_{k, 2}$ & addr$_{k, 3}$ & $\cdots$ & addr$_{k, 8 \cdot \lceil N/64 \rceil}$ \\
};

\draw[dashed] (A-1-1.south west)--(B-1-1.north west);
\draw[dashed] (A-1-1.south east)--(B-1-5.north east);
\draw [
	thick,
    decoration={
        brace,
        mirror,
		amplitude=0.2cm,
        raise=0.2cm
    },
    decorate
] (B-1-1.south west) -- (B-1-5.south east)
node [pos=0.5,anchor=north,yshift=-0.5cm] {N bits};

\end{tikzpicture}

\caption{Address space's bitstrings are stored in a contiguous array. In a 64-bit computer, each bitstring is stored in a sub-array of 64-bit integers, with length $8 \cdot \lceil N/64 \rceil$.\label{tab:hl-addresses-detail}}
\end{figure}

The scan for activated hard locations is performed in an address space. It returns the indexes of the bitstrings which were inside the circle (and their distances). Then, each operation uses these pieces of information in a different way.

\subsection{Scanning for activated hard locations}

Scanning for the activated hard locations is a problem similar to well-known problems in computational geometry called ``range reporting in higher dimensions''. In this case, none of the known algorithms is able to solve our problem faster than $O(H)$. The algorithm which seems to best fit in our problem consumes $O(H)$ space and runs in $O(\log^n(H))$ \citep{chazelle1988functional}, which is really slower than $O(H)$ when, for instance, $H=1,000,000$ and $n=1,000$. For a review of the range reporting algorithms, see \citet{chan2011orthogonal}.

In 2014, there was published a solution to fast search in hamming space which seems applicable to our problem \citet{norouzi2014fast}. It provides a fast search when $r/n < 0.11$ or $r/n < 0.06$, where $r$ is the radius and $n$ is the number of bits. But, in our case, for a 1,000 bits SDM, $r/n = 0.451$, which changes the runtime to $O(H^{0.993})$. This is really close to $O(H)$, but with a larger constant. Unfortunately, $O(H)$ is still faster.

It is intriguing that none of those algorithms is able to solve our scanning problem. The idea behind those computational geometry algorithms is roughly to split the search space in half each step, which would take $O(\log(H))$ to go through the whole space. But this approach does not work because of the high number of dimensions (i.e., 1,000) and because the hard locations' addresses are randomly sampled from the $\{0, 1\}^n$ space. Although each addresses' bit itself splits the hard locations in half, it does not split the search space in half since both halves still must be covered by the algorithm. For instance, let's say we have $n=1,000$ dimensions with $H=1,000,000$ hard locations, and we are scanning within a circle with radius $r=451$, then after checking the first bit we have two cases: (i) for the half with the same first bit, we must keep scanning with radius 451; and (ii) for the half with a different first bit, we must keep scanning with radius 450. Hence, the search space has not been split in half because both halves have been covered (and one of them should have been skipped).

Finally, as our best approach is to scan through all hard locations, we may distribute the scan into many tasks which will be executed independently. The tasks may be executed in different processes, threads, or even computers. They may also run in the CPU or in the GPU. In this case, we may take into account both the time required to distribute the tasks and the time to receive their results.

The framework implements three main scanner algorithms: linear scanner, thread scanner, and OpenCL scanner. The linear scanner runs in a single core, is the slowest one, and was developed only for testing purposes; the thread scanner runs at the CPU in multiple threads sharing memory (and our recommendation is to use the number of threads equals to twice the number of CPU cores); and the OpenCL scanner runs in multiple GPU cores and support multiple devices. The speed of a scan depends on the CPU and GPU devices, thus the best approach to choose which scanner is best for one's setup is to run a benchmark.

The OpenCL must be initialized, which just copies the address space's bitstrings to the GPU's memory. Then, many scans may be executed with no necessity to upload the bitstrings again. The OpenCL scanner supports running into multiple devices.

\subsection{OpenCL kernel}

There are 8 OpenCL kernels which explore differently the GPU architecture to improve performance. It is necessary because there are several GPU microarchitecture and a single kernel will never be optimal for all of them. In simplified form, OpenCL splits the tasks into workgroups which, in turn, split their part of the task into workers. The works are like threads in a computer. OpenCL specifies four levels of memory hierarchy for the GPUs: global memory, read-only memory, local memory, and private memory. The global memory and read-only memory are accessible by all workgroups, while each workgroup has its own local memory, accessible by its workers. Finally, each worker has its own private memory. The number of workers per workgroup is defined by user and must be multiple of the number of tasks.

All 8 kernels do the same thing: calculate the exclusive OR (XOR) between two 64-bit integers and count the number of bits one in the result. They just do it with different approaches. For instance, \lstinline{single_scan0} calculates one distance between bitstrings per worker (Listing \ref{lst:single_scan0}); while \lstinline{single_scan2} uses a whole workgroup to calculate each distance, distributing each element of the 64-int integer array per worker (Listing \ref{lst:single_scan2}).

The OpenCL kernels \lstinline{single_scan3} (Listing \ref{lst:single_scan3}), \lstinline{single_scan4} (Listing \ref{lst:single_scan4}), \lstinline{single_scan5} (Listing \ref{lst:single_scan5}), \lstinline{single_scan5_unroll} (Listing \ref{lst:single_scan5_unroll}), \lstinline{single_scan6} (Listing \ref{lst:single_scan6}) explore the GPU architecture to improve the sum of the partial distances. Each workgroup calculates the distance of several bitstrings. During the distance calculation, each worker calculates the exclusive OR (XOR) between two 64-bit integers and use the built-in popcount function to count the number of ones. Then, they update an array of partial distances with their results. This array is stored in the local memory and is shared between all workers of the same workgroup. This whole step happens simultaneously in the GPU. Then, a reduction algorithm is used to sum the partial distances array in order to calculate the total distance. This reduction algorithm is also distributed between the workers and runs in $O(\log_2(\text{bs\_step}))$. Finally, the first worker of each workgroup checks whether the distance is less than or equal to the radius to include the bitstring index into the resulting array.

Some of the optimizations may not work in some GPUs, because not all their premises are valid. Before choosing a kernel, one should check whether it works property for one's specific GPU device.


\begin{figure}[!p]
\begin{lstlisting}[
language={[OpenCL]C},
label={lst:single_scan0},
caption={OpenCL kernel {\lstinline[columns=fixed]{single_scan0}}. It calculates one distance per worker and let the GPU decide how to distribute this task between workgroups and workers. It is the most simple kernel and does not explore any details of the GPU architecture.},
]
__kernel
void single_scan0(
		__constant const uchar *bitcount_table,
		__global const ulong *bitstrings,
		const uint bs_len,
		const uint sample,
		__constant const ulong *bs,
		const uint radius,
		__global uint *counter,
		__global uint *selected,
		__local uint *partial_dist)
{
	uint id = get_global_id(0);

	if (id < sample) {
		ulong a;
		uint dist;

		const __global ulong *row = bitstrings + id*bs_len;

		dist = 0;
		for(uint j=0; j<bs_len; j++) {
			a = row[j] ^ bs[j];
			dist += popcount(a);
		}
		if (dist <= radius) {
			selected[atomic_inc(counter)] = id;
		}
	}
}
\end{lstlisting}
\end{figure}


\begin{figure}[!p]
\begin{lstlisting}[
language={[OpenCL]C},
label={lst:single_scan1},
caption={OpenCL kernel {\lstinline[columns=fixed]{single_scan1}}. It is just like \lstinline{single_scan0}, but it distribute several distances per workgroup, which, in turn, distribute the distances among their workers.},
]
__kernel
void single_scan1(
        __constant const uchar *bitcount_table,
        __global const ulong *bitstrings,
        const uint bs_len,
        const uint sample,
        __constant const ulong *bs,
        const uint radius,
        __global uint *counter,
        __global uint *selected,
        __local uint *partial_dist)
{
    uint id;
    ulong a;
    uint dist;
    const __global ulong *row;

    for (id=get_global_id(0); id < sample; id += get_global_size(0)) {

        row = bitstrings + id*bs_len;

        dist = 0;
        for(uint j=0; j<bs_len; j++) {
            a = row[j] ^ bs[j];
            dist += popcount(a);
        }
        if (dist <= radius) {
            selected[atomic_inc(counter)] = id;
        }

    }
}
\end{lstlisting}
\end{figure}


\begin{figure}[!p]
\begin{lstlisting}[
language={[OpenCL]C},
label={lst:single_scan2},
caption={OpenCL kernel {\lstinline[columns=fixed]{single_scan2}}. It calculates one distance per workgroup, distributing each 64-bit integer operation per worker, and then summing the results obtained by the workers. The sum algorith is done by only the first worker of each workgroup.},
]
__kernel
void single_scan2(
        __constant const uchar *bitcount_table,
        __global const ulong *bitstrings,
        const uint bs_len,
        const uint sample,
        __constant const ulong *bs,
        const uint radius,
        __global uint *counter,
        __global uint *selected,
        __local uint *partial_dist)
{
    uint dist;
    ulong a;
    uint j;

    for (uint id = get_group_id(0); id < sample; id += get_num_groups(0)) {

        const __global ulong *row = bitstrings + id*bs_len;

        dist = 0;
        j = get_local_id(0);
        if (j < bs_len) {
            a = row[j] ^ bs[j];
            dist += popcount(a);
        }
        partial_dist[get_local_id(0)] = dist;

        barrier(CLK_LOCAL_MEM_FENCE);

        if (get_local_id(0) == 0) {
            dist = 0;
            for(uint t = 0; t < bs_len; t++) {
                dist += partial_dist[t];
            }
            if (dist <= radius) {
                selected[atomic_inc(counter)] = id;
            }
        }

        barrier(CLK_LOCAL_MEM_FENCE);
    }
}
\end{lstlisting}
\end{figure}

\begin{figure}[!p]
\begin{lstlisting}[
language={[OpenCL]C},
label={lst:single_scan3},
caption={OpenCL kernel {\lstinline[columns=fixed]{single_scan3}}. It calculates one distance per workgroup, distributing each 64-bit integer operation per worker, and then summing the results obtained by the workers. The sum algorith is a parallel reduction, in which the workers split the array in two parts and sum the second part in the first part every loop. So, the sum is calculated in {$O(\log_2(\text{number of workers per workgroup}))$}. This kernel only works when the number of workers per workgroup is a power-of-2.},
]
__kernel
void single_scan3(
        __constant const uchar *bitcount_table,
        __global const ulong *bitstrings,
        const uint bs_len,
        const uint sample,
        __constant const ulong *bs,
        const uint radius,
        __global uint *counter,
        __global uint *selected,
        __local uint *partial_dist)
{
    uint dist;
    ulong a;
    uint j;

    for (uint id = get_group_id(0); id < sample; id += get_num_groups(0)) {

        const __global ulong *row = bitstrings + id*bs_len;

        dist = 0;
        j = get_local_id(0);
        if (j < bs_len) {
            a = row[j] ^ bs[j];
            dist = popcount(a);
        }
        partial_dist[get_local_id(0)] = dist;

        // Parallel reduction to sum all partial_dist array.
        for(uint stride = get_local_size(0)/2; stride > 0; stride /= 2) {
            barrier(CLK_LOCAL_MEM_FENCE);
            if (get_local_id(0) < stride) {
                partial_dist[get_local_id(0)] +=
                    partial_dist[get_local_id(0) + stride];
            }
        }

        if (get_local_id(0) == 0) {
            if (partial_dist[0] <= radius) {
                selected[atomic_inc(counter)] = id;
            }
        }

        barrier(CLK_LOCAL_MEM_FENCE);
    }
}
\end{lstlisting}
\end{figure}

\begin{figure}[!p]
\begin{lstlisting}[
language={[OpenCL]C},
label={lst:single_scan4},
caption={OpenCL kernel {\lstinline[columns=fixed]{single_scan4}}. This kernel is just like \lstinline{single_scan3}, but it works with any number of workers per workgroup. The tradeoff is that it includes an aditional step in the parallel reduction algorithm.},
]
__kernel
void single_scan4(
        __constant const uchar *bitcount_table,
        __global const ulong *bitstrings,
        const uint bs_len,
        const uint sample,
        __constant const ulong *bs,
        const uint radius,
        __global uint *counter,
        __global uint *selected,
        __local uint *partial_dist)
{
    uint dist;
    ulong a;
    uint j;

    for (uint id = get_group_id(0); id < sample; id += get_num_groups(0)) {

        const __global ulong *row = bitstrings + id*bs_len;

        dist = 0;
        j = get_local_id(0);
        if (j < bs_len) {
            a = row[j] ^ bs[j];
            dist = popcount(a);
        }
        partial_dist[get_local_id(0)] = dist;

        uint old_stride = get_local_size(0);
        __local uint extra;
        extra = 0;
        for(uint stride = get_local_size(0)/2; stride > 0; stride /= 2) {
            barrier(CLK_LOCAL_MEM_FENCE);
            if ((old_stride&1) == 1 && get_local_id(0) == old_stride-1) {
                extra += partial_dist[get_local_id(0)];
            }
            if (get_local_id(0) < stride) {
                partial_dist[get_local_id(0)] +=
                    partial_dist[get_local_id(0) + stride];
            }
            old_stride = stride;
        }

        if (get_local_id(0) == 0) {
            if (partial_dist[0] + extra <= radius) {
                selected[atomic_inc(counter)] = id;
            }
        }

        barrier(CLK_LOCAL_MEM_FENCE);
    }
}
\end{lstlisting}
\end{figure}

\begin{figure}[!p]
\begin{lstlisting}[
language={[OpenCL]C},
label={lst:single_scan5},
caption={OpenCL kernel {\lstinline[columns=fixed]{single_scan5}}. This kernel is just like \lstinline{single_scan3}, but it explores one more detail of many GPU microarchitecture: the size of the warp. As the workers in the same warp are always synchronized, there is no need to synchronize them using a barrier. This specific kernel only works when the number of workers per workgroup is a power-of-2.},
]
__kernel
void single_scan5(
        __constant const uchar *bitcount_table,
        __global const ulong *bitstrings,
        const uint bs_len,
        const uint sample,
        __constant const ulong *bs,
        const uint radius,
        __global uint *counter,
        __global uint *selected,
        __local uint *partial_dist)
{
    uint dist;
    ulong a;
    uint j;

    for (uint id = get_group_id(0); id < sample; id += get_num_groups(0)) {
        const __global ulong *row = bitstrings + id*bs_len;

        dist = 0;
        j = get_local_id(0);
        if (j < bs_len) {
            a = row[j] ^ bs[j];
            dist = popcount(a);
        }
        partial_dist[get_local_id(0)] = dist;

        uint stride;
        for(stride = get_local_size(0)/2; stride > 32; stride /= 2) {
            barrier(CLK_LOCAL_MEM_FENCE);
            if (get_local_id(0) < stride) {
                partial_dist[get_local_id(0)] +=
                    partial_dist[get_local_id(0) + stride];
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        for(/**/; stride > 0; stride /= 2) {
            if (get_local_id(0) < stride) {
                partial_dist[get_local_id(0)] +=
                    partial_dist[get_local_id(0) + stride];
            }
        }

        if (get_local_id(0) == 0) {
            if (partial_dist[0] <= radius) {
                selected[atomic_inc(counter)] = id;
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }
}
\end{lstlisting}
\end{figure}

\begin{figure}[!p]
\begin{lstlisting}[
language={[OpenCL]C},
basicstyle=\scriptsize\ttfamily,
label={lst:single_scan5_unroll},
caption={OpenCL kernel {\lstinline[columns=fixed]{single_scan5_unroll}}. This kernel is exactly like \lstinline{single_scan5}, but it unrolls the last for since it has at most 5 loops.},
]
__kernel
void single_scan5_unroll(
        __constant const uchar *bitcount_table,
        __global const ulong *bitstrings,
        const uint bs_len,
        const uint sample,
        __constant const ulong *bs,
        const uint radius,
        __global uint *counter,
        __global uint *selected,
        __local uint *partial_dist)
{
    uint dist;
    ulong a;
    uint j;

    for (uint id = get_group_id(0); id < sample; id += get_num_groups(0)) {
        const __global ulong *row = bitstrings + id*bs_len;

        dist = 0;
        j = get_local_id(0);
        if (j < bs_len) {
            a = row[j] ^ bs[j];
            dist = popcount(a);
        }
        partial_dist[get_local_id(0)] = dist;

        for(uint stride = get_local_size(0)/2; stride > 32; stride /= 2) {
            barrier(CLK_LOCAL_MEM_FENCE);
            if (get_local_id(0) < stride) {
                partial_dist[get_local_id(0)] += partial_dist[get_local_id(0) + stride];
            }
        }

        // We do not need to sync because they all run in the same warp.
        if (get_local_id(0) < 32 && get_local_size(0) >= 64) {
            partial_dist[get_local_id(0)] += partial_dist[get_local_id(0) + 32];
        }
        if (get_local_id(0) < 16 && get_local_size(0) >= 32) {
            partial_dist[get_local_id(0)] += partial_dist[get_local_id(0) + 16];
        }
        if (get_local_id(0) < 8 && get_local_size(0) >= 16) {
            partial_dist[get_local_id(0)] += partial_dist[get_local_id(0) + 8];
        }
        if (get_local_id(0) < 4 && get_local_size(0) >= 8) {
            partial_dist[get_local_id(0)] += partial_dist[get_local_id(0) + 4];
        }
        if (get_local_id(0) < 2 && get_local_size(0) >= 4) {
            partial_dist[get_local_id(0)] += partial_dist[get_local_id(0) + 2];
        }

        if (get_local_id(0) == 0) {
            partial_dist[0] += partial_dist[1];
            if (partial_dist[0] <= radius) {
                selected[atomic_inc(counter)] = id;
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }
}
\end{lstlisting}
\end{figure}

\begin{figure}[!p]
\begin{lstlisting}[
language={[OpenCL]C},
basicstyle=\scriptsize\ttfamily,
label={lst:single_scan6},
caption={OpenCL kernel {\lstinline[columns=fixed]{single_scan6}}. This kernel is just like \lstinline{single_scan5}, but it works with any number of workers per work. The tradeoff is an aditional step in the parallel reduction algorithm.},
]
__kernel
void single_scan6(
        __constant const uchar *bitcount_table,
        __global const ulong *bitstrings,
        const uint bs_len,
        const uint sample,
        __constant const ulong *bs,
        const uint radius,
        __global uint *counter,
        __global uint *selected,
        __local uint *partial_dist)
{
    uint dist;
    ulong a;
    uint j;

    for (uint id = get_group_id(0); id < sample; id += get_num_groups(0)) {
        const __global ulong *row = bitstrings + id*bs_len;

        dist = 0;
        j = get_local_id(0);
        if (j < bs_len) {
            a = row[j] ^ bs[j];
            dist = popcount(a);
        }
        partial_dist[get_local_id(0)] = dist;

        uint old_stride = get_local_size(0);
        uint stride;
        __local uint extra;
        extra = 0;
        for(stride = get_local_size(0)/2; stride > 32; stride /= 2) {
            barrier(CLK_LOCAL_MEM_FENCE);
            if ((old_stride&1) == 1 && get_local_id(0) == old_stride-1) {
                extra += partial_dist[get_local_id(0)];
            }
            if (get_local_id(0) < stride) {
                partial_dist[get_local_id(0)] +=
                    partial_dist[get_local_id(0) + stride];
            }
            old_stride = stride;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        for(/**/; stride > 0; stride /= 2) {
            if ((old_stride&1) == 1 && get_local_id(0) == old_stride-1) {
                extra += partial_dist[get_local_id(0)];
            }
            if (get_local_id(0) < stride) {
                partial_dist[get_local_id(0)] +=
                    partial_dist[get_local_id(0) + stride];
            }
            old_stride = stride;
        }

        if (get_local_id(0) == 0) {
            if (partial_dist[0] + extra <= radius) {
                selected[atomic_inc(counter)] = id;
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }
}
\end{lstlisting}
\end{figure}

\section{Counters}

Each hard-location has one integer of data per bit. For instance, each hard-location of a 1,000 bits SDM has 1,000 bits. Those integers are stored in a counter.

A counter is an array of integers which stores the data of all hard locations. So, the counter's array has $n \cdot H$ integers.

When two counters are added in a third counter, there may occur an overflow. It is not supposed to be a problem because, by default, each counter is a signed 32-bit integer that can store any number between -2,147,483,648 and 2,147,483,647, which means they will not overflow with less writes than $2^{31}-1$ divided by the average number of activated hard locations. For instance, when $n=1,000$, $H=1,000,000$, and $r=451$, the average number of activated hard locations is 1,000 and it would require at least one million writes before being possible to a counter to overflow.  Note also that it would be more likely to saturate the memory before any overflow.

Anyway, counters may have overflow protection depending on compiling options. By default, there is no overflow check for performance reasons (and because it does not seem necessary).

\section{Read and write operations}

The reading and writing operations are executed in two steps: first, the address space is swept looking for the activated addresses; then, the operation is performed in the counters. Reading operation assemblies the bitstring according to the counters of the activated addresses, while the writing operation changes the counters.

The iterated reading keeps reading until it gets exactly the same bitstring (or the number of maximum interations has been reached), i.e., it performs $\eta_{i+1} = \text{read}(\eta_i)$ and stops when $\eta_{k+1} = \eta_{k}$. If the initial bitstring is inside the critical distance of $\eta$, it will converge to $\eta$, but, if it is not, it will diverge and reach the maximum number of iterations.

The framework has both Kanerva's read and Murilo's generalized read. The generalization brings a parameter $z$, which is the exponent. In this case, the results are floating point instead of integer, which considerably reduces performance. When $z=1$, it is exactly as the Kanerva's read. When $z=0$, it is the Chada's read. We also explored how SDM would behave for different values of $z$.

There is another special read operation: the weighted reading. In the weighted reading, the value of the counters are multiplied by a weight which depends only on the distance between the reading address and the hard-location address. The weight is retrieved from a lookup table of integers indexed by the distance. The rest of the read operation is exactly the same.

There is also a weighted writing operation. In this case, the weight is applied when the counters are updated, i.e., if the weight is 2, the counters are increased twice when bits are 1, and decreased twice when bits are 0. Just as in the weighted reading, the weights depend only on the distance between the writing address and the hard-location address. The weights are retrieved from a lookup table of integers indexed by the distance.
