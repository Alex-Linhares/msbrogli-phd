% !TEX root = ../partial-sdm.tex

\chapter{Introduction}

\bigskip

\begin{flushright}{\slshape
    {How is memory gradually built up during one's conscious, or\\
    even unconscious, life and thought?  My guess is that everything \\
    we experience is classified and registered on very many parallel \\
    channels in different locations} \\ \medskip
    --- Stanislaw Ulam}

    \bigskip
    \bigskip
    {\slshape Pentti Kanerva's memory model was a revelation for me: \\
it was the very first piece of research I had ever come across\\
that made me feel I could glimpse the distant goal of understanding \\
how the brain works as a whole. It gave me a concrete sense for\\
how familiar mental phenomena could be thought of as distributed\\
patterns of micro-events, thanks to beautiful mathematics. \\ \medskip
--- Douglas Hofstadter}


\end{flushright}
\bigskip
\bigskip







Suddenly, you feel the danger.  You tighten your grip on the child's hand and step up your walking.  In these large cities, walking on a dark alley was never recommended, but somehow that's what you have done --- and now everything you can think of is to get back to safety.  You are afraid of something, but you cannot explain --- in rigorous detail, at least --- what exactly is causing your fear. One just feels it.

We may interpret this situation as clues of the present --- a dark alley; a giant metropolitan area; people going on about their lives mostly indifferent from each other; constrained routes ahead and behind; a general lack of activity; that very feeling that something isn't right... without knowing precisely what isn't right, or even what `right' means --- recalling past experiences from memory and thus generating the feeling. Our memory is able to make a parallel between previous experiences and the clues. Although one has never been in the exactly same situation, one's brain involuntarily, through analogy with previous experiences, recognizes the possibility of danger. The whole process happens so fast it feels like it only took a single indivisible unit of psychological time.  A single cycle, to use the computing word.

As much as we can rationalize our ability to understand the why of a feeling like this, it seems unlikely that we will be able to nail all reasons down into rules.  Let us compare this to what a child might do.  While an adult may respond by discreetly stepping up the walking speed, a child might open her eyes and be surprised and ask, indiscretely, questions about the walking speed... Because children do not have the accumulated experience of adults in dealing with large metropolian areas, their behavior differs markedly from ours.

Sparse Distributed Memory (SDM) \citep{Kanerva1988} (see also \citep{denning_sparse_1989,  flynn_sparse_1989, kanerva_sparse_1993, kanerva_parallel_1985, goos_binary_1996, kanerva_hyperdimensional_2009,  keeler_comparison_1988, rwcp_fully_1997, marinaro_spatter_1994, sahlgren_permutations_nodate, uesaka_foundations_2001}) is a mathematical model of long-term memory that has a number of neuroscientific and psychologically plausible dynamics. This model is used in all sort of applications due to its incredible ability to closely reflect the human capacity to remember past experiences from the subtlest of clues. Applications range from call admission control \citep{kwon_atm_1998, hee-yong_kwon_atm_1997}, to behavior-based robotics \citep{Rajesh1998, mendes2008robot, jockel_sparse_2009}, to noise filtering \citep{meng_modified_2009}, etc.

This flexibility into mapping one situation in another is an essential human feature which is hard to replicate in computers.

\section{Desiderata for a theory of memory}

% TODO Venn Diagram Psychology, Neuroscience, Cognitive modelling
% TODO Graph between these disciplines and how they might interact.

Sparse Distributed Memory reads like a \emph{desiderata} of a theory for human long-term memory.  To understand the breadth of topics that SDM encompasses, consider the following questions:
\begin{enumerate}
    \item Why are most concepts orthogonal, unrelated to each other?
    \item Why is there Miller's magic number, i.e., we can't hold too many things in mind at once?
    \item Why do we at times instantly recall an experience; other times we can't recall anything at all; and still other times we get into this strange tip-of-the-tongue situation... it is clearly `there'... but where?  (and `what' is `there'?\footnote{Psychologists have documented interesting properties about this state:  it happens, for instance, around every 1-week cycle; and it happens mostly over proper names, subjects have access to the first letter and to the number of syllables better than chance, etc. Also interesting is their clever way of triggerring the state: through definitions of rarely-used words.  For some psychological studies about the tip-of-tongue, see \citet{meyer_tip---tongue_1992, brown_review_1991, brown_tip_1966}.}.)
    \item How does this recall process work?  What is remembering?
    \item Why do neurons die and we still remember most everything?
    \item What do neurons actually do?  What is their primary function?
\end{enumerate}

These are some of the questions touched by SDM.  It is a theory of long-term memory that constructs the process of recall from a very microscopic level of neuron-firings. While there are numerous groups working on related research, it seems that most are simply bypassing each other; starting from scratch instead of building on top of what already exists.


\section{The wasted effort of duplicated, ad hoc, work}

As expected for such an important theory, SDM has been applied in many different fields, like pattern recognition \citep{norman2003modeling, rao1995natural}, noise reduction \citep{Meng2009}, handwriting recognition \citep{fan1997genetic}, robot automation \citep{Rajesh1998, mendes2008robot}, and so forth. \citet{Linhares2011} has argued that SDM respects the limits of short-term memory \citep{Miller1955, Cowan2001}. Kanerva's book has over 1,000 citations in Google Scholar.  Despite this, there is not a reference implementation which would allow one to replicate the results published in a paper, to check the source code for details, and to improve it. Thus, even though intriguing results have been achieved using SDM, it requires counter-productive, duplicate effort from researchers to build on top of previous work.

Our motivation is our own effort to run our models. As there is no reference implementation, we had to develop our own and run several simulations to ensure that our implementation was correct and bug-free. Thus, we had to deviate from our primary goal --- which was to test our hypothesis and explore the `ideas space' --- and to focus on the implementation itself. Furthermore, new members of our research group had to go through different source code developed by former members, in different languages.  Consider the implementations of SDM present --- as of this writing --- on its wikipedia entry\citep{noauthor_sparse_2018}\footnote{There has recently been a surge of hardware implementations to minimize energy expenditure and parallelize processes \citep{kang_energy-efficient_2015, kang_-memory_2016, li_hyperdimensional_2016, montagna_pulp-hd:_2018, salahuddin_energy_nodate}.  While these implementations are extremely interesting, they do not afford the flexibility to experiment that software does.}:

\begin{enumerate}
    \item The original 1989 hardware implementation developed in NASA by \citet{flynn_sparse_1989};

    \item a 1995 LISP implementation for the Connection Machine by \citet{turk_kanervas_1995};

    \item a 1992 APL implementation by \citet{surkan_wsdm:_1992};

    \item a 2004 FPGA implementation by \citet{silva_reconfigurable_2004};

    \item a 2005 C++ implementation by \citet{berchtold_processing_2005} from Lancaster University, in the `CommonSense ToolKit' (CSTK) \citep{noauthor_cstk:_nodate} for realtime sensor data includes SDM as one of its classification algorithms;

    \item  a 2015 `C Binary Vector Symbols (CBVS)':  includes SDM implementation as a part of  vector symbolic architecture developed by \citet{emruli_vector_2015} from EISLAB at Luleå University of Technology \footnote{The code is available at http://pendicular.net/cbvs.php};

    \item a 2013 Java implementation `Learning Intelligent Distribution Agent' (LIDA) developed by \citep{franklin_lida:_2014, snaider_integer_2013, snaider_modular_2014} Stan Franklin's group from the University of Memphis includes implementation. \footnote{http://ccrg.cs.memphis.edu/framework.html; see also http://ccrg.cs.memphis.edu/projects.html where they link to a github repository.};

\end{enumerate}

Let us analyze these.  The Connection Machine is obsolete.  The NASA implementation is hardware-based and obsolete.  APL, while reasonably influential, is not a mainstream language in science.

The FPGA implementation by \citet{silva_reconfigurable_2004} has yielded a fast scan of hard locations at low energy costs, provided one has access to the proper hardware. Their article claims a four-fold speedup over assembly language; but it does not deal with parallel processing details.  For example, it is unclear whether there was more than a single thread running on the software implementation.  Note that the framework presented here is also able to reconfigure field-programmable gate arrays, through the OpenCL heterogeneous computing platform ability to interface with Hardware Description Language and hence, reconfigure FPGAs \citep{waidyasooriya2018design, czajkowski_opencl_2012} for our tasks.

Then there is LIDA --- a whole cognitive architecture based on Hofstadter's Fluid Concepts, Kanerva's SDM, and other ideas \citep{Anwar2003, snaider_integer_2013, snaider_modular_2014, franklin_lida:_2014}.  It is developed in Java; which makes it difficult to connect to the lowest levels of hardware; to connect to GPUs or FPGAs, and to other languages --- at least in comparison to the combination Python and OpenCL proposed here\footnote{Python is sometimes called a `glue language'. That is, in my opinion, not the best metaphor.  A glue connects two things leaving an inflexible structure.  Python is perhaps best described as the interstate highway system of Programming; if something is out there, there is a way to reach it with Python.  In the comparison with Java, for instance, take the $popcnt(xor(b_i,b_j))$ operation, executed billions of times in SDM. How easy is it to program that for a particular GPU or FPGA with Java?}.  It has a non-standard license, strange to the open-source community, \emph{the LIDA Framework Software NonExclusive, Non-Commercial Use License}.  We have not found any parallelism in their code \citep{ccrg_ccrg_nodate}, which may make simulations slow or unfeasible. Moreover, potential contributors must sign an ``Agreement Regarding Contributory Code for the LIDA Framework Software''... \emph{`before Memphis can accept it'} \footnote{What they are attempting to do with this bureaucracy remains unclear, the history of computing has not been kind to those who favored centralization \citep{ferguson_computer_2002}.  We certainly refrain from contributing given the legal uncertainties of non-standard licenses and dubious processes --- even as we would like to link these libraries}.

The closest implementations to ours, in philosophy at least, is the one in `the common sense toolkit'. It is executed in C++, with a normal open-source license, and hosted on an open-source code repository.  It is, however, strikingly dissimilar to ours on the following aspects:

\begin{enumerate}
    \item SDM is but a part of the system; the description of the system reads that cstk is `A toolkit for processing and visualising sensor data in real time with support for use with embedded platforms.'

    \item The whole SDM code is composed of 143 \emph{lines} of C++ in the $cstk/cstk-devonly/sdm$ folder.

    \item There is no work on making the system parallel.

    \item There is strong coupling between location address and location data, which makes experimentation hard.

    \item There are no tests or examples to be found instantly.

    \item Finally, the last commit to this repository seems to have been made in 2005?

    \item There are no publishable or published scientific applications or experiments available to be reproduced at installation time.

    \item there is no tutorial, installation instructions, performance benchmarks, framework validation or SDM Documentation.

\end{enumerate}

Note that all these criticisms apply to the implementations in both the `common sense toolkit' and the `C Binary Vector Symbols' \citep{berchtold_processing_2005, noauthor_cstk:_nodate, emruli_vector_2015}.  While these implementations have around 150 lines of C++; at last count, the \emph{documentation of our implementation} had \emph{over 100 pages} \citep{linhares_sparse_2018}: they have aimed at running code, and we aim at improving a community and industry standard.

There is obviously a demand for use of SDM; but each group has been tied to their own \emph{ad-hoc} needs, and there has not been the emergence of a community centered on a tool. It is our belief that a tool such as standard open-source framework could bring orders of magnitude more researchers and attention if they were able to use the model, at zero cost, with an easy to use high-level language such as Python, in an intuitive platform such as Juypyter notebooks. Neuroscientists interested in long-term memory storage should not have to worry about high-bandwidth vector parallel computation.  This new tool would provide a ready to use system in which experiments could be executed almost as soon as designed --- and provide the needed replication of studies \citep{shen2014interactive}.

The main contribution of this work is a reference implementation which yields (i) orders of magnitude gains in performance, (ii) has several backends\footnote{CPUs, GPUs, FPGAs} and operations, (iii) is fully validated against the mathematical model, (iv) is cross-platform\footnote{Unix, Linux, MacOs, Windows, Amazon Web Services, etc.}, and (v) is easily extensible to test new research ideas --- and to let others replicate the studies.

Another issue is \emph{extensibility}: Extensions of SDM have been used in many applications. For example, \citet{Snaider2011} extended SDM to store sequences of vectors and trees efficiently.  \citet{Rajesh1998} used a modified SDM in an autonomous robot. \citet{Meng2009} modified SDM to clean patterns from noisy inputs. \citet{fan1997genetic} extended SDM with genetic algorithms. \citet{chada2016you} extended SDM creating the Rotational Sparse Distributed Memory (RSDM), which models network motifs, dynamic flexibility, and hierarchical organization --- reflecting results from the neuroscience literature.

Our reference implementation may, hopefully, accelerate research into the model's dynamics and make it easier for readers to replicate any previous results and easily understand the source-code of the model.  Moreover, it is compatible with Jupyter notebook and researchers may share their notebooks possibly accelerating the advances in their fields \citep{shen2014interactive}.

Other contributions have also been introduced, which include (i) a noise filtering approach, (ii) a supervised classification algorithm, (iii) and a reinforcement learning algorithm, all of them using only the original SDM proposed by Kanerva, i.e., with no additional mechanisms, algorithms, data structures, etc. Although some of these applications have already been explored in previous work \citep{Meng2009, fan1997genetic, rao1995natural}, all of them have adapted SDM to fit their problems, and none of them have used just the ideas introduced by Kanerva. We have presented different approaches with no adaptations whatsoever.

Finally, I have striven to provide a visual tour of the theory and application of SDM: whenever possible, detailed figures should tell the story --- or at least do the heavy lifting. In this study, we will see an anomaly in one of Kanerva's predictions, which I believe is related to SDM capacity. We will see tests of a generalized reading operation proposed by Physics Professor Paulo Murilo (personal communication).  We will see what happens when neurons --- and all their information --- is simply and suddenly lost.  We will see whether information-theory can improve some of Kanerva's ideas.  From (basic) noise filtering to learning to play tic-tac-toe, we will review the entirety of Dr. Pentti Kanerva's proposal.

This time, however, it will be running on a computer:   anomalies between expected theory and numerical results will appear; implicit, hidden, assumptions will emerge; and reproducible experiments will be conducted.





\chapter{Notation}

\begin{tabular}{cp{\textwidth}}
  $n$ & Number of dimensions, i.e., $n=1,000$. \\
  $N$ & Size of the binary space, $|\{0, 1\}^n| = 2^n$. \\
  $N'$ & Number of hard locations samples from $\{0, 1\}^n$. Its typical value is 1,000,000, as suggested by \citet{Kanerva1988}. \\
  $H$ & Same as $N'$. \\
  $r$ & Access radius, i.e., when $n=1,000$ and $N'=1,000,000$, its typical value is $451$. This value is calculated to activate, on average, one-thousandth of $N'$. \\
  $\eta$ & A bitstring, usually a datum. \\
  $\eta_x$ & A clue $x$ bits away from $\eta$, i.e., $\text{dist}(\eta, \eta_x) = x$. \\
  $\xi$ & A bitstring, usually an address. \\
  $\text{dist}(x, y)$ & Hamming distance between $x$ and $y$. \\
  $\text{d}(x, y)$ & Same as $\text{dist}(x, y)$.
\end{tabular}\\

\chapter{Sparse Distributed Memory}
\input{./Chapters/sdm2.tex}


\chapter{Framework Architecture}
\input{./Chapters/sdm-architecture.tex}

\chapter{Results (i): Performance}
\input{./Chapters/sdm-performance.tex}

\chapter{Results (ii): Framework validation}
\input{./Chapters/sdm-validation.tex}


\chapter{Results (iii): Loss of neurons}
In SDM, the data is written distributed among millions of hard locations, which theoretically gives SDM robustness against loss of neurons. In other words, SDM should keep converging correctly even when some neurons are dead. The question is: how robust it really is? How many neurons may die before it starts to forget things? These questions have never been addressed before.

Looking for answers to these questions, we run simulations in which we kept killing some neurons and checking whether SDM remained converging to a given bitstring or not. In these simulations, 10,000 random bitstrings were written to a 1,000-bit SDM with 1,000,000 hard locations, and we choose one of them as our target. As the bitstrings were all written exactly once, we may generalize the results. The code is available in the ``Resetting hard locations'' notebook \citep{sdmframework}.

As neurons are hard locations in SDM, when we say that a neuron has been killed, we mean that its counters have been zeroed and a new random bitstring address has been assigned. During our simulations, no other bitstring has been written after the 10,000. Consequently, as their counters will remain zero, it is exactly like ignoring the dead hard locations in the subsequent reading operations.

\begin{figure}[!p]
\centering\includegraphics[width=\textwidth]{./images02/new-images/sdm-neuron-death-200k.png}
\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=1,000$ and $H=1,000,000$. It shows that a loss of 200,000 neurons, 20\% of the total, does not seem to affect SDM whatsoever.
\label{fig:sdm-neuron-death-200k}}
\end{figure}

In Figure \ref{fig:sdm-neuron-death-200k}, we can notice that SDM is robust up to 200,000 neuron deaths which are 20\% of all hard locations. Its robustness is astonishing.  In fact, SDM begins to be significantly affected by the loss of neurons after 600,000 neuron deaths (Figure \ref{fig:sdm-neuron-death-1m}) and obviously forgets everything when all neurons are dead.

\begin{figure}[!p]
\centering\includegraphics[width=\textwidth]{./images02/new-images/sdm-neuron-death-1m.png}
\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=1,000$ and $H=1,000,000$. The more neurons are lost, the smaller the critical distance, i.e., the worse the SDM recall.
\label{fig:sdm-neuron-death-1m}}
\end{figure}

It is interesting that 500,000 neuron deaths have a minor effect on SDM's recall capability (see Figure \ref{fig:sdm-neuron-death-500k}). It is analogous to do a hemispherectomy in a person and, after the procedure, the person being able to recall and learn almost just like before. In fact, there are clinical reports of children submitted to hemispherectomy who live an almost normal life with minor function problems.

\begin{figure}[!p]
\centering\includegraphics[width=\textwidth]{./images02/new-images/sdm-neuron-death-500k.png}
\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=1,000$ and $H=1,000,000$. Even when 50\% of neurons are dead, SDM recall is barely affected, which is an impressive result and matches with some clinical results of children submitted to hemispherectomy.
\label{fig:sdm-neuron-death-500k}}
\end{figure}

An important observation is that around 800,000 neuron deaths (80\% of all neurons) the critical distance becomes small, i.e., SDM recall capacity is hugely diminished. After 900,000 neuron deaths, the critical distance is zero, and everything has been lost.

Although there is some decrease in SDM recall after 600,000 neuron deaths, it is curious that there is a sudden change between 900,000 (90\%) and 1,000,000 (100\%). In Figure \ref{fig:sdm-neuron-death-details} we can see the details of this non-linear change. Notice that after 950,000 even the exact clue $\eta_0$ does not converge to $\eta$.

\begin{figure}[!p]
\centering\includegraphics[width=\textwidth]{images02/new-images/sdm-neuron-death.png}
\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=1,000$ and $H=1,000,000$.
\label{fig:sdm-neuron-death-details}}
\end{figure}

We run the same simulation for a 256-bit SDM with 1,000,000 hard locations. The results were even more surprising, as the 256-bit SDM seems to be more robust to loss of neurons than the 1,000-bit SDM (see Figure \ref{fig:sdm-neuron-death-256bits}). Notice that the loss of 50\% of neurons barely affected the 256-bit SDM... and it even remained functional when facing an enormous loss of 90\% of neurons!  What these results seem to imply is not only that the memory is extremely robust, but that a very small expected number of neurons activated in each access radius would preserve the most valueable information.  This may have implications as to the number of standard deviations to define the access radius, as we will discuss in Section \ref{Magic numbers}.  Is there a thing such as excessive robustness, and, if so, when does robustness become waste?\footnote{Consider, for instance, error-correcting codes in Information Theory or the analogous computer ECC memory in Electrical Engineering: tradeoffs between robustness and waste must be considered at each design decision; Why should it be different in the modeling of human memory?}

\begin{figure}[!p]
\centering
\subfloat[Up to 500,000 neuron deaths ]{\includegraphics[width=\textwidth]{images02/new-images/sdm-neuron-death-256-500k.png}}

\subfloat[From 600,000 to 1,000,000 neuron deaths ]{\includegraphics[width=\textwidth]{images02/new-images/sdm-neuron-death-256-1m.png}}

\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=256$ and $H=1,000,000$.
\label{fig:sdm-neuron-death-256bits}}
\end{figure}


\chapter{Results (iv): Critical distance}
\input{./Chapters/sdm-critical-distance.tex}



\chapter{Results (v): Generalized read operation}

Dr. Murilo observed that the models of Kanerva's read ($z=1$) and Chada's read ($z=0$) were simple variations of a generalized read with an exponent $z$, which suggests experimenting with different values. Mathematically, let $A$ be the set of the counters of the activated hard location, and $c_i$ be the counter of the $i$th bit. Then,

$$
s_i = \sum_{c \in A} \frac{c_i}{|c_i|} |c_i|^z
$$

The sum of $|c_i|^z$ turns the intermediate values from integers to floating point numbers. Thus, we have developed a specific read operation which stored the intermediate values in double variables.

The results, however, have not yielded performance improvements. Though for $z \leq 1$ results are comparable to $z=1$, for $z>1$, the system shows an evident deterioration, with a smaller critical distance and faster divergence at large-distance reads. This is shown in Figures \ref{fig:murillo-generalization-experiments} and \ref{fig:murillo-generalization-experiments-6reads}.

We understand that the critical distance is an important parameter of SDM. The bigger the critical distance, the better, because SDM is able to converge even with farther clues. For $z>1$, the bigger the $z$, the smaller the critical distance. For $z = 6$, the critical distance almost reaches zero.

It is interesting that Kanerva has proposed $z=1$ without realizing the generalized reading. Even so, he proposed the $z$ with the highest critical distance.

\begin{figure}[h!]
  \centering
  \subfloat[SDM behavior when $z \in \{0.1, 0.2, 0.3, 0.4, 0.5, 1\}$ ]{\includegraphics[width=\textwidth]{./images02/new-images/iter_z_01-05.png}}

  \subfloat[SDM behavior when $z \in \{1.5, 3, 4.5, 6\}$ ]{\includegraphics[width=\textwidth]{./images02/new-images/z_15_3_45_6.png}}

  \caption{(a) and (b) show the behavior of a single read. As stated previously, we can see a deterioration of convergence, with lower critical distance as $z>1$.  Another observation can be made here, concerning the discrepancy of Kanerva's Fig 7.3 and our data.  It seems that Kanerva may not have considered that a single read would only `clean' a small number of dimensions \emph{after the critical distance}. What we observe clearly is that with a single read, as the distance grows, the system only `cleans' towards the orthogonal distance 500 after a number of iterative readings.}
  \label{fig:murillo-generalization-experiments}
\end{figure}

\begin{figure}[h!]
  \centering
  \subfloat[$z \in \{0, 1\}$]{\includegraphics[width=\textwidth]{./images02/new-images/z_0_1.png}}

  \subfloat[$z \in \{0, 0.5, 1, 1.5, 3, 4.5, 6\}$]{\includegraphics[width=\textwidth]{./images02/new-images/iter_z_all_2.png}}

  \caption{(a) and (b) show the behavior of Figure \ref{fig:murillo-generalization-experiments}, now executed with 6-iterative reads. What we observe clearly is that with a single read, as the distance grows, the system only `cleans' towards the orthogonal distance 500 after a number of iterative readings.}
  \label{fig:murillo-generalization-experiments-6reads}
\end{figure}


%\chapter{Results (v): Performance}
%\input{./Chapters/sdm-performance.tex}

% Application
\input{./Chapters/sdm-applications.tex}





\chapter{Results (ix): Information-theoretical write operation}

\begin{figure}[h!]
  \centering
  \subfloat[$w(d), d \in \{ 1, 2, ..., n\}.$]{\includegraphics[width=3.2in]{./images02/info-theory/weights.png}}

  \subfloat[$w(d)$ for the desired range.]{\includegraphics[width=3.2in]{./images02/info-theory/weights-region.png}}

  \subfloat[Stepwise $\left \lfloor{w(d)}\right \rfloor$ for fast integer computation.]{\includegraphics[width=3.2in]{./images02/info-theory/weights-region-stepwise.png}}

  \caption{Shannon write operation:  Computing the amount of information of a signal to each hard location in its access radius. (a) entirety of the space; (b) region of interest; (c) Fast integer computation is possible through a stepwise function.}
  \label{fig:info-theory-hypothesis}
\end{figure}

\begin{figure}[h!]
  \centering
  \subfloat[Write process weighted by the amount of information contained in the distance between the written bitstring and each hard location \label{fig:info-theory-figure73} ]{\includegraphics[width=0.7\textwidth]{./images02/info-theory/shannon-figure73.png}}

  \subfloat[Zoom in Figure \ref{fig:info-theory-figure73} ]{\includegraphics[width=0.7\textwidth]{./images02/info-theory/shannon-figure73-zoom.png}}

  \subfloat[Behavior of weighted write operation according to the distance from the center and the number of items previously stored in the memory]{\includegraphics[width=0.7\textwidth]{./images02/info-theory/shannon-heatmap.png}}

  \caption{Behavior of the critical distance under the information-theoretic weighted write operation when $n=1,000$, $H=1,000,000$ and $r=451$.}
  \label{fig:info-theory-experiments}
\end{figure}


My advisor, Alexandre Linhares, has proposed another write operation: an information-theoretical weighted write. In it, the sum of the counter's value is weighted based on the distance between each hard location's address and the reading address. The logic behind it is to vary the importance of each hard location inside the circle.  It is only natural that one encodes an item in closer hard locations with a stronger signal, and a natural candidate for this signal function is the amount of information contained in the distance between the item and each hard location.  Closer hard locations have lower probabilities and therefore should encode more information.

Note that this is not the first time in which a weighted function has been applied to writing in SDM --- \citet{hely1997new} suggest a rather complex spreading model based on floating point signals in the interval [0.05, 1.0] --- they were, however, only able to test their model with 1,000 hard locations.


Consider the following. Information Theory \citep{cover2012elements} let us compute the precise amount of information in an event when given its probability $p$, through the measure of \emph{self-information}:

$$
I(p)= -log_2(p)
$$

Now, given any two $n$-sized bitstrings, the probability of their Hamming distance being exactly $d$ is given by $P(X=d)= 2^{-n} \binom{n}{d}$, and the probability of it being at most $d$ is:

$$
P(X\leq d)= 2^{-n} {\displaystyle\sum_{i=0}^{d}{\binom{n}{i}}}
$$

But we must consider that not all hard locations are activated in each write operation, which changes our probability function. Thus, let $r$ be the access radius then:

\begin{align*}
P(X = d | X \leq r) &= \frac{P((X = d) \cap (X \leq r))}{P(X \leq r)} \\
    &= \frac{P(X = d)}{P(X \leq r)}, \quad \text{as $d \leq r$} \\
    &= \frac{ 2^{-n} \binom{n}{d} }{ 2^{-n} \sum_{i=0}^{r} \binom{n}{i} } \\
    &= \frac{\binom{n}{d}}{\sum_{i=0}^{r} \binom{n}{i}}, \quad d \leq r
\end{align*}

And the probability of it being at most $d$ is:

$$
P(X \leq d | X \leq r) = \frac{\sum_{i=0}^{d} \binom{n}{i}}{\sum_{i=0}^{r} \binom{n}{i}}, \quad d \leq r
$$

As expected, $P(X \leq d | X \leq r) = 1$ when $d = r$.

Hence the weighted write would, on each hard location, sum (or subtract) using the following weights, as seen in Figure \ref{fig:info-theory-hypothesis}:

$$
w(d) = -\log_2 \left( P(X = d | X \leq r) \right) = - \log_2 \binom{n}{d} + \log_2 \sum_{i=0}^{r} \binom{n}{i}, \quad d \leq r
$$



The initial results of this \emph{Shannon write} operation can be seen in Figure \ref{fig:info-theory-experiments} and seem promising. It seems that, when $n=1,000$, $H=1,000,000$, $r=451$, and 10,000 written random bitstrings, the critical distance increased from around 221 to around 250. This increase may be interpreted as an improvement in SDM, because it would converge to the correct bitstring even for farther bitstrings. Note that 29 additional bits imply an attractor area $2^{29}$ times larger than the original. This Shannon write may affect memory capacity \citep{chou_capacity_1989, chou_capacity_1988, sjodin_improving_1995} --- possibly increasing it. Another point to keep in mind is that, since the modulus of the vectors are not uniform in this approach, the shape of the attractor may have asymmetries. Whereas these are just some initial tests, the idea seems meritorious so far.  As for future research, we will execute all tests in the thesis and compare this Shannon write with the original Kanerva model.







%It is easy to interpret this weight through a binary tree approach.  How many binary questions would be needed to precisely define a bitstring inside the access radius and exactly $d$ bits away?

%Another possibility would be to use the sum of all distances closer (and less likely) locations within the weighting function $w(d)$,

%$w(d) = -\log_2 \left( 2^{-n} \displaystyle\sum_{i=0}^{d}{\binom{n}{i}} \right) = n - \log_2 \displaystyle\sum_{i=0}^{d}{\binom{n}{i}}$. \\

%This can be seen in \ref{fig:info-theory-sum-hypothesis}.

%and, consequently,

%$p(H\geq n-d)=2^{-n}{\displaystyle\sum_{i=n-d}^{n}{\binom{n}{i}}  }$, \\

%$p(d+1 \leq H \leq n-d-1)=2^n - 2^{1-n}{\displaystyle\sum_{i=0}^{d}{\binom{n}{i}}, \forall d<n/2}$. \\






%\begin{figure}[h!]
%  \centering
%  \subfloat[$w_1(d), d \in \{ 1, 2, ..., n\}.$]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-a-global.jpeg}}
%
%  \subfloat[$w_1(d)$ for the desired range. ]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-b-to-500.jpeg}}
%
%  \subfloat[stepwise $\left \lfloor{w_1(d)}\right \rfloor$ for fast integer computation.]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-c-stepwise.jpeg}}

%  \caption{Shannon write operation:  Computing the amount of information of a signal to each hard location in its access radius. (a) entirety of the space; (b) region of interest; (c) Fast integer computation is possible through a stepwise function.}
%  \label{fig:info-theory-hypothesis}
%\end{figure}



%\begin{figure}[h!]
%  \centering
%  \subfloat[$w_2(d), d \in \{ 1, 2, ..., n\}.$]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-a-sum.jpeg}}
%
%  \subfloat[$w_2(d)$ for the desired range. ]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-b-sum-range.jpeg}}
%
%  \subfloat[stepwise $\left \lfloor{w_2(d)}\right \rfloor$ for fast integer computation.]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-c-sum-stepwise.jpeg}}
%
%  \caption{SOON TO BE DEPRECATED.  Shannon write operation:  Computing the sum of low-likelihood signals. (a) entirety of the space; (b) region of interest; (c) Fast integer computation through a stepwise function. }
%  \label{fig:info-theory-sum-hypothesis}
%\end{figure}
%










\chapter{Conclusion}

\begin{flushright}{\slshape

    \begin{quote}
    The most exciting phrase to hear in science, the one that heralds new discoveries, is not `Eureka!' (I found it!) but ``That's funny...'' \\
    — Isaac Asimov
    \end{quote}

\bigskip
\bigskip

}
\end{flushright}


Sparse Distributed Memory is a viable model of human memory, yet it does require researchers to (re-)implement a number of parallel algorithms in different architectures.

We provide a new, open-source, cross-platform, highly parallel framework in which researchers may be able to create hypotheses and test them computationally with minimal effort. The framework is well-documented for public release at this time (http://sdm-framework.readthedocs.io), it has already served as the backbone of Chada's Ph.D. thesis \citep{chada2016you}. The single-line command ``pip install sdm'' will install the framework on posix-like systems, and single-line commands will let users test the framework, generate some of the figures from Kanerva's theoretical predictions in their own machines, and --- if interested enough ---, test their own theories and improve the framework, and the benchmarks used to evaluate the framework, in open-source fashion. It is our belief that such work is a necessary component towards accelerating research in this promising field.

Here are interesting questions that have been considered during this work, but have had to be left for future research.


\section{Another `funny thing'...}

Over the course of this work we looked into a `funny thing' that had appeared in my master's thesis:  the graphs of convergence did not reflect the original expectations put forth by Kanerva, and neither my own previous calculations.  These numerical results pointed us back to the math and to some interesting random variables not studied before.

Another funny thing that has appeared in my master's thesis was that the critical distance collapsed when the network was exclusively writing `random-at-x'.  This is something I would like to look into but have not had the time to conduct a proper study yet.

\section{Magic numbers}

Kanerva suggests, in his book, the use of 1,000 dimensions and 1,000,000 hard locations.  More recently, he suggested the use of 10,000 dimensions, and on personal discussions suggested that this should be a minimum; as he has been concerned in latent semantic analysis and seems to be the proper scale in that application.

Each parameter set choice like this will lead to particular numbers --- many of them emergent ---, such as the access radius size, critical distance, memory capacity, and so forth.  For example, the number of dimensions $n$ leads to the number of standard deviations available on the space: $2 \sqrt{n}$.  One then must choose a proper multiple $m\times\sigma$: Kanerva (and most in the literature) have been using $m=3$, as $\mu-3\sigma$ selects approximately $1/1000$ of the space. These constants will determine the size of the access radius.  Then one must choose the number of hard locations, which will determine how many hard-locations are activated on average; and so on and so forth...

One intriguing question here is:  is there a `better' number of dimensions and hard locations?  If so, can such numbers be better studied algebraically or numerically? How should these parameters be compared?  What are the tradeoffs that should be considered?  What are the `best' benchmarks possible?



% \subsection{Classification with context using sequences --- for words instead of only letters}



\section{Symmetrical, rapidly accessible, hard locations}

A hypercube with n dimensions can be divided by two hypercubes with $n-1$ dimensions. Is there an algorithm that separates the area of each hard location in such a form that there exists a function mapping each bitstring in $\{0,1\}^n$ to the set of hard locations it `belongs to'?  Though this would break Kanerva's assumption of a randomly yet uniformly distributed set of hard locations --- for a perfectly symmetrical set of hard locations ---, there could be large performance gains if such a mapping function from a bitstring to its corresponding set of nearest hard locations exists. Note that others have attempted heuristic approaches to this\footnote{\citet{Kanerva2009} has recently mentioned that `The computationally most efficient implementation of it, by \citet{Karlsson95afast}, is equivalent to the RAM-based WISARD of  \citet{aleksander1982computer}.' Note, however, that Karlsson's model may introduce non-uniform asymmetries in the space, and seems to require a staggering $O(2^n)$ hard-locations.}.

Consider the hypercube with $n$ dimensions.  We want to select a subset of its vertices with cardinality $2^{20}$ that is symmetrically distributed over the space. Afterward, $\forall b \in \{ 0,1\} ^n$, we want an algorithm $A$ that yields the particular list of hard locations for $b$ and all hard locations respect the desired properties of the memory.

A reduction from measuring the distance to $2^{20}$ hard locations to a computation of $2^{10}$ hard locations might yield astonishing performance gains, depending, of course, on our optimistic assumptions concerning existence and complexity of such algorithm.  At large scales of computing, the very ability to perform some experiments is a function of sheer performance. The horizon of experiments --- and possibly of knowledge --- expands \emph{as a function of computational demands}. A little more on this will follow below.




\section{``i'' versus ``l''}

The classification algorithm had some problems classifying the patterns ``i'' and ``l'', due to the low distance between them. In this case, SDM could not discriminate the differences --- it has only considered the big picture. Although this behavior is close to how humans see things, we also have the ability to zoom and focus on the details, clearly discerning letter ``i'' from letter ``l''.

I have run the classification algorithm under the MNIST database of handwritten digits \citep{deng2012mnist}. First, SDM has been trained with the 60,000 training images, and then it classified the 10,000 testing images. In these initial tests, the memory has given the correct classification for 79.22\% of the images, which is inferior to the specialized algorithms. For instance, in 1998, \citet{lecun1998gradient} have developed algorithms which achieve from 88\% through a linear classifier, to 99.7\% through a convolutional net. For a review of algorithms' performance in the MNIST database, see \cite{deng2012mnist}.

Looking into the reason behind images incorrectly classified, I have found that the issue is very related to the ``i'' versus ``l'' issue. Some handwritten digits are very close to others, and a ``2'' or a ``7'' may look like a ``1'', for instance. So, how can we solve this issue without using anything specific to images?  Machine learning algorithms use specific techniques to improve performance. I would like to unveil a solution psychologically closer to how we behave --- even if that eventually leads to lower performance.

An unexplored idea is to use multiple SDMs which communicate. A first SDM would write the whole picture, just like we have done. Another SDM would write specific regions of the image, just like our eye focusing on specific regions. When reading, they may compose the counters and give a more precise classification.


\section{Deep learning, multiple SDMs --- and the incredible animal behavior of Dr. Linhares}

There is the intention of studying what, if any, capabilities do multiple SDMs have...  yet it is probably advisable to leave the following comments as originally made by my advisor, Dr. Linhares. To quote:

\begin{table}[ht]
\begin{tabular}{|p{4cm}|p{7cm}|}  %{|c|c|}
\hline
System 1	& System 2\tabularnewline
\hline
\hline
Unconscious Reasoning	& Conscious Reasoning\tabularnewline
\hline
Implicit	& Explicit\tabularnewline
\hline
Automatic	& Controlled\tabularnewline
\hline
Low Effort	& High Effort\tabularnewline
\hline
Large Capacity	& Small Capacity\tabularnewline
\hline
Rapid	& Slow\tabularnewline
\hline
Default Process	& Inhibitory\tabularnewline
\hline
Associative	& Rule-Based\tabularnewline
\hline
Contextualized	& Abstract\tabularnewline
\hline
Domain Specific	& Domain General\tabularnewline
\hline
Evolutionarily Old	& Evolutionarily Recent\tabularnewline
\hline
Nonverbal	& Linked to language\tabularnewline
\hline
Includes recognition, perception, orientation	& Includes rule following, comparisons, weighing of options\tabularnewline
\hline
Modular Cognition	& Fluid Intelligence\tabularnewline
\hline
Independent of working memory	& Limited by working memory capacity\tabularnewline
\hline
Non-Logical	& Logical\tabularnewline
\hline
Parallel	& Serial\tabularnewline
\hline
\end{tabular}

\caption{Dual process theories.}

\end{table}\label{table:twosys}

\begin{figure}[h!]
  \centering

  \subfloat[Bongard problem 71]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp071.png}}

  \subfloat[Bongard problem 72]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp072.png}}

  \subfloat[Bongard problem 73]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp073.png}}

  \caption{Bongard problems 71 - 73, courtesy of noted Bongardologist, Dr. Harry Foundalis.  What distinguishes the boxes on the left hand side to those on the right hand side?}
  \label{fig:bongard-problems-71}
\end{figure}

\begin{figure}[h!]
  \centering

  \subfloat[Bongard problem 74]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp074.png}}

  \subfloat[Bongard problem 75]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp075.png}}

  \subfloat[Bongard problem 76]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp076.png}}

  \label{fig:bongard-problems-74}
\end{figure}

\begin{figure}[h!]
  \centering

  \subfloat[Bongard problem 77]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp077.png}}

  \subfloat[Bongard problem 78]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp078.png}}

  \subfloat[Bongard problem 79]{\includegraphics[width=0.7\textwidth]{./images02/Bongard-problemp079.png}}

  \label{fig:bongard-problems-77}
\end{figure}




\begin{quote}

    Those deep neural networks... alphaGo... alphazero... very impressive, and to me very right and proper in a sense and also very wrong and misguided in another. I have not seriously studied the topic. Eric Nichols explained it to me in a napkin, and I watched some youtube videos and browsed the books. So what follows, like all great science, is completely based on gossip and hearsay, prejudice and anecdote.  Or at least that's how I believe all great \emph{hunches} over what is promising and what is a dead-end come from; that aesthetic sense that forms over time and seems to guide us towards what feels beautiful and correct and away from what seems foolish and wasteful...

    So here is my hunch:  \emph{SDM is System 1 and System 1 is SDM} (e.g., as in Table \ref{table:twosys}). I haven't seen anyone say this out loud, but Kanerva has pretty much nailed system 1 --- I feel he is at least 90\% right.

    You know those animals that create those burrows underground, creatures like the prairie dogs?  There, underground, they build all sorts of passageways from here to where they want to go; and in the process they discover all that \emph{can't be passageway}.  What can't be passageway --- what can't be corridor --- will never be used as passageway, by definition.  And most of the world can't be passageway.  Think about the animal behavior: endless dead-end nooks and cranies yet the beast won't give up! To me it's all very similar to what these systems seem to be doing... it seems they are crafting and molding a model of their problem; discovering what can't be passageway and finding some other route, very tortuous if need be...  You have these monstrous networks creating layers upon layers in which they differentiate and distribute the errors throughout all the nooks and cranies of, say, a domain like the game of Go.  Then, after some biblical number-crunching, it seems that there may be a very subtle way... however tortuous, however tight, however improbable, however unconfortable.

    And these machines are exhibiting intelligence.  The victories in the game of Go seem to have shown that alphaGo did have vague, non-explicit, non-formal, but very strong intuitions about what the nooks and cranies of the Go universe looks like.  So here is what I like and I find beautiful:  These gigantic clusters of inter-related micro impressions that the system has acquired and can relate to.  I imagine multiple SDMs exchanging information; gossiping to each other about all the dead-ends out there and trying to find some tiny tortuous pathway, and it feels to me that that's a good and proper way to move forward in research.

    But it feels like a mistake to say that that is thinking.  It's not.  It is to me a great model of someone getting so attuned into an area that they are able to simply flow through tasks; in the positive psychology sense of the word flow.  It's very powerful and very intelligent:  but it is a form of intelligence without thinking.  Intelligence that can't make leaps from one arena to another; Intelligence that can't work on Bongard problems.  So what I would fear as a dead-end in research would be to have another AI winter... decades of massive, massive number-crunching and machine learning of this type... in disregard to big questions such as the interaction of short-term memory with SDM and, again, tasks like Bongard problems (Figure \ref{fig:bongard-problems-71}).  Bongard problems demand high-gossip between system 1 and system 2; they demand a high dose of imagination and mental imagery manipulation; they demand discrimination and demand clusterization, among many other things that have been spelled out before by \citet{bongard_recognition_1968, douglas1980godel, Linhares2000} and the landmark, deep, study being \citet{foundalis_phaeaco:_2006}.

    Cognitive Science has to study the \emph{plausible} interaction between system 1 and system 2.  We can't continue to study, say, a silly, implausible, model of system 1 and a great model of system 2 (or vice-versa). We need both system 1 and system 2 to be plausible, in the psychological sense and in the neuroscientific sense.  AlphaGo, for instance, has this wonderful model of the game, reflecting many of the properties of system 1... but then falls back into silliness as soon as it turns to the other side of the problem:  it uses montecarlo methods in very efficient fashion to probe some 70,000 thousands of positions per second and go on to to the next probing of this wonderful system 1...

    It works; it is impressive; it is intelligent; but it is not thinking; and it hasn't thought us much about our cognition.

    Hofstadter has nailed the interactions and the content... analogies, fluid concepts, things seen through a different perspective, abstract roles, variations on a theme, etc.  Kanerva has nailed system 1, long-term-memory, and its  theoretical neuroscience; micro neuro-firings bringing a memory back to life.  Isn't it time to work on System 2 models that are counterparts to SDM --- in fact, isn't it obvious that that's our glaring omission?  Attaching something as beautiful as SDM to, say, some centralized hyper-efficient montecarlo algorithm is nothing short of heresy against all the gods of science for which the villain shall be put to death without clemency or mercy. Seems like the only reasonable path going forward...

\end{quote}



\section{Not a real Conclusion}



% Preview source code for paragraph 8

\begin{table}[ht]
\begin{tabular}{|p{4cm}|p{7cm}|}  %{|c|c|}
\hline
Project & Observation\tabularnewline
\hline
\hline
Source-code  & C, OpenCL, Python (NVIDIA CUDA, FPGAs desirable in the future)\tabularnewline
\hline
Step-by-step notebook studies & replication and dissemination of studies\tabularnewline
\hline
Documentation of framework & 100+ pages currently\tabularnewline
\hline
Open-Source Creative Commons Book & Under consideration (to be created from this thesis)\tabularnewline
\hline
Collection of SDM links, results, resources & modeled after `Awesome Machine Learning'\citep{misiti_awesome-machine-learning:_2018} and `Awesome Artificial Intelligence'\citep{lewis_awesome-artificial-intelligence:_2018} lists, community
managed\tabularnewline
\hline
Slides from the Jupyter Notebooks & online course, currently in planning phase\tabularnewline
\hline
Videos following the Course Slides  & online course, currently in planning phase\tabularnewline
\hline
\end{tabular}

\caption{Desiderata of open-source, creative-common deliverables, longer-term.  Some of these items are mature for wide dissemination, whereas others are in planning phase.  For an online SDM course, for instance, we might partner with a Ph.D. program in the Teaching of Mathematics and Physics, such as \citet{im/ufrj_programa_nodate}, who are intent on developing and measuring the effects of new educational tools.}

\end{table}






Let us revisit, in these concluding thoughts, the emphasis employed over speed of computation.  At first sight, that might seem like a typical objective of efficiency in computer science. But we are not only interested in the computer science effects here --- the ambition is different. More important than this `computer-sciency' goal, i.e., a beautiful, clean, efficient algorithm with the primary effect of enhanced speed, however, is the secondary effect on the sociology of science:  communities form around open-source code, specialists in a particular arena magnify the capabilities of the code, debates and meetings are regularly held, and deep divisive disagreements lead to community splits in multiple potential directions of exploration.  In a very real sense, given a shared code to work and experiment on, we can see farther.

Beyond speed, I have also strived for \emph{ease of use} and \emph{replicable studies}.  All the simulations and graphics generated in this thesis are promptly available to be re-executed and explored by those interested. Most importantly, they also serve as a tutorial for learning about the framework's design, and perhaps as a teaching tool for SDM itself. I have generated a Docker image, which makes it even easier to explore the framework by delegating heavy computational work to cloud services. After running the container, a Jupyter Notebook is available with sdm-framework and other tools already installed. We invite the reader to take a look and explore a little bit.

The overarching intention here is to not only provide a starting point, but to provide a documented Framework in which SDM research can be conducted.  Consider having the ability to compare the results of a new (‘forked’) model to the previous `best' (under a particular benchmark set).  For example, some of the benchmarks that we plan to develop in future research are: how fast is convergence through iterative reading?  How large is the attractor of the critical distance?  How well does the system filter noise?  How well does the system work under the supervised learning task?  And other authors may be able to improve this benchmark set themselves, as is usual in open source development.  It is perhaps this facility of ease to build on top of previous work that seems most exciting at this stage.

Consider the misunderstanding concerning the SDM read operation:  Dr. Stan Franklin describes Kanerva's read operation in a way that each hard location, at each dimension, provides only a single bit of information to the read operation (instead of Kanerva's full counter).  We have referred to this modified read operation as Chada read\footnote{Legend has it that my friend \text{\&} colleague, Dr. Daniel de Magalhães Chada, along with Linhares, did not consult and re-check with Kanerva's book and only discovered the discrepancy in code and ideas a couple of years afterward.}.  Having an open, testable, codebase reduces the possibilities of such misunderstandings in the long run.  Indeed, a high-quality codebase seems to have become a scientific community's form of unequivocally standing behind a consensus. For example, the journal \emph{Nature} analyzed the top-100 cited papers in history, to find:

\begin{quote}
... some surprises, not least that it takes a staggering 12,119 citations to rank in the top 100 — and that many of the world’s most famous papers do not make the cut. A few that do, such as the first observation of carbon nanotubes (number 36) are indeed classic discoveries. But the vast majority describe experimental methods or \emph{software that have become essential in their fields. [...] The list reveals just how powerfully research has been affected by computation and the analysis of large data sets.} \\
\hfill --- \citet{van2014top}, emphasis mine.
\end{quote}

It is no coincidence that scientific journals such as \emph{BMC Neuroscience}, or the \emph{Journal of Machine Learning Research} have specific sections on open-source software. The journal \emph{Neurocomputing} states, bluntly: “software is scientific method by machine”.

Of course, for the skeptical reader who may consider software a less worthy pursuit, there is also new work here.  The mathematics of the model has been shown to be correct numerically (with a single, small, anomaly); we have shown how to execute unsupervised learning with nothing besides operations original to the SDM; we have studied the generalized Murilo read; we have seen noise filtering; the death of neurons; how information-theory may be of use; and finally, we have reproduced numerous of the original propositions put forth by Kanerva.  The emphasis might have been on the \emph{breadth of topics}, in detriment of depth here or there.  The work on, say, reinforcement learning, is most definitely not the last word we will see on the subject, but a challenge left for readers to contemplate. But this is due to our research group's enthusiasm for the topic; we do indeed believe that SDM is --- if not correct --- extremely close to a full scientific understanding of human long-term memory.  If so, it is such a monumental achievement that we want readers to be able to see all of what we see and imagine the vastness of possibilities.

Ralph Waldo Emerson once said: \emph{do not go where the path may lead. Go, instead, where there is no path, and leave a trail.}  Professor Pentti Kanerva has left the trail.  It is my job to illuminate it and to pave it and to clear it; to try to deliver an easier pathway for the next generation.  Some essays completely shut the door close at the end; this one intends to leave it wide open.  As the reader might have noticed, this final section does not read as an analysis of the work done; it reads, instead, as a \emph{desideratum}, a prologue, a yearning for others to join me in imagining the shape of things to come.











\chapter{Appendix}
%\section{Generating Kanerva's table 7.3}

%\includepdf[
%  pages=-,
%  pagecommand={\pagestyle{headings}},
%  %addtotoc={1,section,1,Quilling Shapes,sec:shapes}
%]{./Chapter02-SDM/Kanerva-Table-7.3/Kanerva-Table-7_3.pdf}

\chapter*{List of Jupyter notebooks}


\begin{enumerate}
  \item Calculate critical distance.ipynb
  \item Calculate radius for any SDM.ipynb
  \item Classification Test 1.ipynb
  \item Classification Test 2.ipynb
  \item Classification Test 3-Copy1.ipynb
  \item Classification Test 3.ipynb
  \item Critical Distance - New.ipynb
  \item Critical Distance.ipynb
  \item Distance between bitstrings.ipynb
  \item Distances of activated hard-locations.ipynb
  \item Generates a new SDM.ipynb
  \item Intersection of activated hard-locations.ipynb
  \item Kanerva-Table-7 - Experiment.ipynb
  \item Kanerva-Table-7.3 - Generic Read.ipynb
  \item Kanerva-Table-7.3 - Multiple dimensions.ipynb
  \item Kanerva-Table-7.3 - Weighted table write.ipynb
  \item Kanerva-Table-7.3.ipynb
  \item Kanerva's Figure 1.2.ipynb
  \item Noise filter 2.ipynb
  \item Noise filter A-Z 0-1 Generator.ipynb
  \item Noise filter with labels.ipynb
  \item Noise filter.ipynb
  \item Number of activated hard-locations.ipynb
  \item Performance test.ipynb
  \item Reseting hard-locations.ipynb
  \item Sequences (Kanerva Ch 8).ipynb
  \item Sequences in one SDM (Kanerva Ch 8).ipynb
  \item TicTacToe.ipynb
  \item Weighted operations using information.ipynb
\end{enumerate}
