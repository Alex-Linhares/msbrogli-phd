% !TEX root = ../partial-sdm.tex

\chapter{Introduction}

\bigskip

\begin{flushright}{\slshape
    {Pentti Kanerva's memory model was a revelation for me: \\
    it was the very first piece of research I had ever come across\\
    that made me feel I could glimpse the distant goal of understanding \\
    how the brain works as a whole. It gave me a concrete sense for\\
    how familiar mental phenomena could be thought of as distributed\\
    patterns of micro-events, thanks to beautiful mathematics.} \\ \medskip
    --- Douglas Hofstadter
    \bigskip
    \bigskip

    {How is memory gradually built up during one's conscious, or\\
    even unconscious, life and thought?  My guess is that everything \\
    we experience is classified and registered on very many parallel \\
    channels in different locations} \\ \medskip
    --- Stanislaw Ulam}
\end{flushright}
\bigskip
\bigskip




Sparse Distributed Memory (SDM) \citep{Kanerva1988} is a mathematical model of long-term memory that has a number of neuroscientific and psychologically plausible dynamics. This model may be applied in all sort of applications because of its incredible ability to closely reflect the human capacity to remember past experiences from clues of the present. For instance, when one is walking on a dark alley and is afraid of something, one cannot explain where one's fear comes from. One just feels it. We may interpret this situation as clues of the present --- a dark alley; a giant metropolitan area; people going on about their lives mostly indifferent from each other; constrained routes ahead and behind you; a general lack of activity; that very feeling that something isn't right... without knowing precisely what isn't right, or even what `right' means... etc. --- recalling past experiences from memory and thus generating the feeling. Our memory is able to make a parallel between previous experiences and the clues. Although one has never been in the exactly same situation, one's brain involuntarily makes an analogy and recognizes the possibility of danger. This flexibility into mapping one situation in another is an important human feature which is hard to replicate in computers.

SDM has been applied in many different fields, like pattern recognition \citep{norman2003modeling, rao1995natural}, noise reduction \citep{Meng2009}, handwriting recognition \citep{fan1997genetic}, robot automation \citep{Rajesh1998, mendes2008robot}, and so forth. \citet{Linhares2011} has shown that SDM respects the limits of short-term memory discussed by \citet{Miller1955} and \citet{Cowan2011}. Despite all those applications, there is not a reference implementation which would allow one to replicate the results published in a paper, to check the source code for details, and to improve it. Thus, even though intriguing results have been achieved using SDM, it requires counter-productive, duplicate effort from researchers to build on top of previous work.

It is our belief that a tool such as a framework could bring orders of magnitude more researchers and attention if they were able to use the model, at zero cost, with an easy to use high-level language such as Python, in an intuitive platform such as Juypyter notebooks. Neuroscientists interested in long-term memory storage should not have to worry about high-bandwidth vector parallel computation.  This new tool would provide a ready to use system in which experiments could be executed almost as soon as designed --- and it might accelerate research \citep{shen2014interactive}.

Our motivation was our own effort to run our models. As there is no reference implementation, we had to implement our own and run several simulations to ensure that our implementation was correct and bug-free. Thus, we had to deviate from our main goal --- which was to test our hypothesis and explore the `ideas space' --- and to focus on the implementation itself. Furthermore, new members of our research group had to go through different source codes developed by former members.

Extensions of SDM have been used in many applications. For example, \citet{Snaider2011} extended SDM to efficiently store sequences of vectors and trees.  \citet{Rajesh1998} used a modified SDM in an autonomous robot. \citet{Meng2009} modified SDM to clean patterns from noisy inputs. \citet{fan1997genetic} extended SDM with genetic algorithms. \citet{chada2016you} extended SDM creating the Rotational Sparse Distributed Memory (RSDM), which is used to modeling network motifs, dynamic flexibility, and hierarchical organization, all results from neuroscience literature.

The main contribution of this work is a reference implementation which yields (i) orders of magnitude gains in performance, (ii) has several backends and operations, (iii) has been validated against the mathematical model, (iv) is cross-platform, and (v) is easily extended to test new research ideas. Our reference implementation may, hopefully, accelerate research into the model's dynamics and make it easier for readers to replicate any previous results and easily understand the source-code of the model.  Moreover, it is compatible with Jupyter notebook and researchers may share their notebooks possibly accelerating the advances in their fields \citep{shen2014interactive}.

Other contributions have also been introduced, which include (i) a noise filtering approach, (ii) a supervised classification algorithm, (iii) and a reinforcement learning algorithm, all of them using only the original SDM proposed by Kanerva, i.e., with no additional mechanisms, algorithms, data structures, etc. Although some of these applications have already been explored in previous work \citep{Meng2009, fan1997genetic, rao1995natural}, all of them have done some adapting of SDM to their problems, and none of them have used just the ideas introduced by Kanerva. We have presented different approaches with no adaptations whatsoever.

Finally, I have striven to provide a visual tour of the theory and application of SDM: whenever possible, detailed figures should tell the story --- or at least do the heavy lifting. In this study, we will see an anomaly in one of Kanerva's predictions, which I believe is related to SDM capacity. We will see tests of a generalized reading operation proposed by Physics Professor Paulo Murilo (personal communication).  We will see what happens when neurons --- and
all their information --- is simply and suddenly lost.  We will see whether information-theory can improve some of Kanerva's ideas.  From (basic) noise filtering to learning to play tic-tac-toe, we will review the entirety of Dr. Pentti Kanerva's proposal.

This time, however, it will be running on a computer.

\chapter{Notation}

\begin{tabular}{cp{\textwidth}}
  $n$ & Number of dimensions, i.e., $n=1,000$. \\
  $N$ & Size of the binary space, $|\{0, 1\}^n| = 2^n$. \\
  $N'$ & Number of hard locations samples from $\{0, 1\}^n$. Its typical value is 1,000,000, as suggested by \citet{Kanerva1988}. \\
  $H$ & Same as $N'$. \\
  $r$ & Access radius, i.e., when $n=1,000$ and $N'=1,000,000$, its typical value is $451$. This value is calculated to activate, on average, one-thousandth of $N'$. \\
  $\eta$ & A bitstring, usually a datum. \\
  $\eta_x$ & A clue $x$ bits away from $\eta$, i.e., $\text{dist}(\eta, \eta_x) = x$. \\
  $\xi$ & A bitstring, usually an address. \\
  $\text{dist}(x, y)$ & Hamming distance between $x$ and $y$. \\
  $\text{d}(x, y)$ & Same as $\text{dist}(x, y)$.
\end{tabular}\\

\chapter{Sparse Distributed Memory}
\input{./Chapters/sdm2.tex}


\chapter{Framework Architecture}
\input{./Chapters/sdm-architecture.tex}


\chapter{Results (i): Framework validation}
\input{./Chapters/sdm-validation.tex}


\chapter{Results (ii): Critical distance}
\input{./Chapters/sdm-critical-distance.tex}


\chapter{Results (iii): Loss of neurons}
In SDM, the data is written distributed among millions of hard locations, which theoretically gives SDM robustness against loss of neurons. In other words, SDM should keep converging correctly even when some neurons are dead. The question is: how robust it really is? How many neurons may die before it starts to forget things? These questions have never been addressed before.

Looking for answers to these questions, we run simulations in which we kept killing some neurons and checking whether SDM remained converging to a given bitstring or not. In these simulations, 10,000 random bitstrings were written to a 1,000-bit SDM with 1,000,000 hard locations, and we choose one of them as our target. As the bitstrings were all written exactly once, we may generalize the results. The code is available in the ``Resetting hard locations'' notebook \citep{sdmframework}.

As neurons are hard locations in SDM, when we say that a neuron has been killed, we mean that its counters have been zeroed and a new random bitstring address has been assigned. During our simulations, no other bitstring has been written after the 10,000. Consequently, as their counters will remain zero, it is exactly like ignoring the dead hard locations in the subsequent reading operations.

\begin{figure}[!p]
\centering\includegraphics[width=\textwidth]{./images02/new-images/sdm-neuron-death-200k.png}
\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=1,000$ and $H=1,000,000$. It shows that a loss of 200,000 neurons, 20\% of the total, does not seem to affect SDM whatsoever.
\label{fig:sdm-neuron-death-200k}}
\end{figure}

In Figure \ref{fig:sdm-neuron-death-200k}, we can notice that SDM is robust up to 200,000 neuron deaths which are 20\% of all hard locations. Its robustness is astonishing.  In fact, SDM begins to be significantly affected by the loss of neurons after 600,000 neuron deaths (Figure \ref{fig:sdm-neuron-death-1m}) and obviously forgets everything when all neurons are dead.

\begin{figure}[!p]
\centering\includegraphics[width=\textwidth]{./images02/new-images/sdm-neuron-death-1m.png}
\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=1,000$ and $H=1,000,000$. The more neurons are lost, the smaller the critical distance, i.e., the worse the SDM recall.
\label{fig:sdm-neuron-death-1m}}
\end{figure}

It is interesting that 500,000 neuron deaths have a minor effect on SDM's recall capability (see Figure \ref{fig:sdm-neuron-death-500k}). It is analogous to do a hemispherectomy in a person and, after the procedure, the person being able to recall and learn almost just like before. In fact, there are clinical reports of children submitted to hemispherectomy who live an almost normal life with minor function problems.

\begin{figure}[!p]
\centering\includegraphics[width=\textwidth]{./images02/new-images/sdm-neuron-death-500k.png}
\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=1,000$ and $H=1,000,000$. Even when 50\% of neurons are dead, SDM recall is barely affected, which is an impressive result and matches with some clinical results of children submitted to hemispherectomy.
\label{fig:sdm-neuron-death-500k}}
\end{figure}

An important observation is that around 800,000 neuron deaths (80\% of all neurons) the critical distance becomes small, i.e., SDM recall capacity is very diminished. After 900,000 neuron deaths, the critical distance is zero. In this case, everything has been lost.

Although there is some decrease in SDM recall after 600,000 neuron deaths, it is curious that there is a sudden change between 900,000 (90\%) and 1,000,000 (100\%). In Figure \ref{fig:sdm-neuron-death-details} we can see the details of this non-linear change. Notice that after 950,000 even the exact clue $\eta_0$ does not converge to $\eta$.

\begin{figure}[!p]
\centering\includegraphics[width=\textwidth]{images02/new-images/sdm-neuron-death.png}
\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=1,000$ and $H=1,000,000$.
\label{fig:sdm-neuron-death-details}}
\end{figure}

We run exactly the same simulation for a 256-bit SDM with 1,000,000 hard locations. The results were even more surprising, as the 256-bit SDM seems to be more robust to loss of neurons than the 1,000-bit SDM (see Figure \ref{fig:sdm-neuron-death-256bits}). Notice that the loss of 50\% of neurons barely affected the 256-bit SDM which remained functional even facing an enormous loss of 90\% of neurons.

\begin{figure}[!p]
\centering
\subfloat[Up to 500,000 neuron deaths ]{\includegraphics[width=\textwidth]{images02/new-images/sdm-neuron-death-256-500k.png}}

\subfloat[From 600,000 to 1,000,000 neuron deaths ]{\includegraphics[width=\textwidth]{images02/new-images/sdm-neuron-death-256-1m.png}}

\caption{This graph shows the SDM's robustness against loss of neurons in a SDM with $n=256$ and $H=1,000,000$.
\label{fig:sdm-neuron-death-256bits}}
\end{figure}


\chapter{Results (iv): Generalized read operation}

Murilo observed that the models of Kanerva's read ($z=1$) and Chada's read ($z=0$) were simple variations of a generalized read with an exponent $z$, which suggests experimenting with different values. Mathematically, let $A$ be the set of the counters of the activated hard location, and $c_i$ be the counter of the $i$th bit. Then,

$$
s_i = \sum_{c \in A} \frac{c_i}{|c_i|} |c_i|^z
$$

The sum of $|c_i|^z$ turns the intermediate values from integers to floating point numbers. Thus, we have developed a specific read operation which stored the intermediate values in double variables.

The results, however, have not yielded performance improvements. Though for $z \leq 1$ results are comparable to $z=1$, for $z>1$, the system shows a clear deterioration, with a smaller critical distance and faster divergence at large-distance reads. This is shown in Figures \ref{fig:murillo-generalization-experiments} and \ref{fig:murillo-generalization-experiments-6reads}.

We understand that the critical distance is an important parameter of SDM. The bigger the critical distance, the best, because SDM is able to converge even with farther clues. For $z>1$, the bigger the $z$, the smaller the critical distance. For $z = 6$, the critical distance almost reaches zero.

It is interesting that Kanerva has proposed $z=1$ without realizing the generalized reading. Even so, he proposed the $z$ with the highest critical distance.

\begin{figure}[h!]
  \centering
  \subfloat[SDM behavior when $z \in \{0.1, 0.2, 0.3, 0.4, 0.5, 1\}$ ]{\includegraphics[width=\textwidth]{./images02/new-images/iter_z_01-05.png}}

  \subfloat[SDM behavior when $z \in \{1.5, 3, 4.5, 6\}$ ]{\includegraphics[width=\textwidth]{./images02/new-images/z_15_3_45_6.png}}

  \caption{(a) and (b) show the behavior of a single read. As stated previously, we can see a deterioration of convergence, with lower critical distance as $z>1$.  Another observation can be made here, concerning the discrepancy of Kanerva's Fig 7.3 and our data.  It seems that Kanerva may not have considered that a single read would only `clean' a small number of dimensions \emph{after the critical distance}. What we observe clearly is that with a single read, as the distance grows, the system only `cleans' towards the orthogonal distance 500 after a number of iterative readings.}
  \label{fig:murillo-generalization-experiments}
\end{figure}

\begin{figure}[h!]
  \centering
  \subfloat[$z \in \{0, 1\}$]{\includegraphics[width=\textwidth]{./images02/new-images/z_0_1.png}}

  \subfloat[$z \in \{0, 0.5, 1, 1.5, 3, 4.5, 6\}$]{\includegraphics[width=\textwidth]{./images02/new-images/iter_z_all_2.png}}

  \caption{(a) and (b) show the behavior of Figure \ref{fig:murillo-generalization-experiments}, now executed with 6-iterative reads. What we observe clearly is that with a single read, as the distance grows, the system only `cleans' towards the orthogonal distance 500 after a number of iterative readings.}
  \label{fig:murillo-generalization-experiments-6reads}
\end{figure}


\chapter{Results (v): Performance}
\input{./Chapters/sdm-performance.tex}

% Application
\input{./Chapters/sdm-applications.tex}





\chapter{Results (ix): Information-theoretical write operation}


My advisor, Alexandre Linhares, has proposed another write operation: an information-theoretical weighted write. In it, the sum of the counter's value is weighted based on the distance between each hard location's address and the reading address. The logic behind it is to vary the importance of each hard location inside the circle.  It is only natural that one encodes an item in closer hard locations with a stronger signal, and a natural candidate for this signal function is the amount of information contained in the distance between the item and each hard location.  Closer hard locations have lower probabilities and therefore should encode more information.

Note that this is not the first time in which a weighted function has been applied to writing in SDM --- \citet{hely1997new} suggest a rather complex spreading model based on floating point signals in the interval [0.05, 1.0] --- they were, however, only able to test their model with 1,000 hard locations.


Consider the following. Information Theory \citep{cover2012elements} let us compute the precise amount of information in an event when given its probability $p$, through the measure of \emph{self-information}:

$$
I(p)= -log_2(p)
$$

Now, given any two $n$-sized bitstrings, the probability of their Hamming distance being exactly $d$ is given by $P(X=d)= 2^{-n} \binom{n}{d}$, and the probability of it being at most $d$ is:

$$
P(X\leq d)= 2^{-n} {\displaystyle\sum_{i=0}^{d}{\binom{n}{i}}}
$$

But we must consider that not all hard locations are activated in each write operation, which changes our probability function. Thus, let $r$ be the access radius then:

\begin{align*}
P(X = d | X \leq r) &= \frac{P((X = d) \cap (X \leq r))}{P(X \leq r)} \\
    &= \frac{P(X = d)}{P(X \leq r)}, \quad \text{as $d \leq r$} \\
    &= \frac{ 2^{-n} \binom{n}{d} }{ 2^{-n} \sum_{i=0}^{r} \binom{n}{i} } \\
    &= \frac{\binom{n}{d}}{\sum_{i=0}^{r} \binom{n}{i}}, \quad d \leq r
\end{align*}

And the probability of it being at most $d$ is:

$$
P(X \leq d | X \leq r) = \frac{\sum_{i=0}^{d} \binom{n}{i}}{\sum_{i=0}^{r} \binom{n}{i}}, \quad d \leq r
$$

As expected, $P(X \leq d | X \leq r) = 1$ when $d = r$.

Hence the weighted write would, on each hard location, sum (or subtract) using the following weights, as seen in Figure \ref{fig:info-theory-hypothesis}:

$$
w(d) = -\log_2 \left( P(X = d | X \leq r) \right) = - \log_2 \binom{n}{d} + \log_2 \sum_{i=0}^{r} \binom{n}{i}, \quad d \leq r
$$

\begin{figure}[h!]
  \centering
  \subfloat[$w(d), d \in \{ 1, 2, ..., n\}.$]{\includegraphics[width=3.2in]{./images02/info-theory/weights.png}}

  \subfloat[$w(d)$ for the desired range.]{\includegraphics[width=3.2in]{./images02/info-theory/weights-region.png}}

  \subfloat[Stepwise $\left \lfloor{w(d)}\right \rfloor$ for fast integer computation.]{\includegraphics[width=3.2in]{./images02/info-theory/weights-region-stepwise.png}}

  \caption{Shannon write operation:  Computing the amount of information of a signal to each hard location in its access radius. (a) entirety of the space; (b) region of interest; (c) Fast integer computation is possible through a stepwise function.}
  \label{fig:info-theory-hypothesis}
\end{figure}


The initial results of this \emph{Shannon write} operation can be seen in Figure \ref{fig:info-theory-experiments} and seem promising. It seems that, when $n=1,000$, $H=1,000,000$, $r=451$, and 10,000 written random bitstrings, the critical distance increased from around 221 to around 250. This increase may be interpreted as an improvement in SDM, because it would converge to the correct bitstring even for farther bitstrings. Note that 29 additional bits imply an attractor area $2^{29}$ times larger than the original. This Shannon write may affect memory capacity --- possibly increasing it. Another point to keep in mind is that, since the modulus of the vectors are not uniform in this approach, the shape of the attractor may have asymmetries. Whereas these are just some initial tests, the idea seems meritorious so far.  As for future research, we will execute all tests in the thesis and compare this Shannon write with the original Kanerva model.




\begin{figure}[h!]
  \centering
  \subfloat[Write process weighted by the amount of information contained in the distance between the written bitstring and each hard location \label{fig:info-theory-figure73} ]{\includegraphics[width=0.7\textwidth]{./images02/info-theory/shannon-figure73.png}}

  \subfloat[Zoom in Figure \ref{fig:info-theory-figure73} ]{\includegraphics[width=0.7\textwidth]{./images02/info-theory/shannon-figure73-zoom.png}}

  \subfloat[Behavior of weighted write operation according to the distance from the center and the number of items previously stored in the memory]{\includegraphics[width=0.7\textwidth]{./images02/info-theory/shannon-heatmap.png}}

  \caption{Behavior of the critical distance under the information-theoretic weighted write operation when $n=1,000$, $H=1,000,000$ and $r=451$.}
  \label{fig:info-theory-experiments}
\end{figure}



%It is easy to interpret this weight through a binary tree approach.  How many binary questions would be needed to precisely define a bitstring inside the access radius and exactly $d$ bits away?

%Another possibility would be to use the sum of all distances closer (and less likely) locations within the weighting function $w(d)$,

%$w(d) = -\log_2 \left( 2^{-n} \displaystyle\sum_{i=0}^{d}{\binom{n}{i}} \right) = n - \log_2 \displaystyle\sum_{i=0}^{d}{\binom{n}{i}}$. \\

%This can be seen in \ref{fig:info-theory-sum-hypothesis}.

%and, consequently,

%$p(H\geq n-d)=2^{-n}{\displaystyle\sum_{i=n-d}^{n}{\binom{n}{i}}  }$, \\

%$p(d+1 \leq H \leq n-d-1)=2^n - 2^{1-n}{\displaystyle\sum_{i=0}^{d}{\binom{n}{i}}, \forall d<n/2}$. \\






%\begin{figure}[h!]
%  \centering
%  \subfloat[$w_1(d), d \in \{ 1, 2, ..., n\}.$]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-a-global.jpeg}}
%
%  \subfloat[$w_1(d)$ for the desired range. ]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-b-to-500.jpeg}}
%
%  \subfloat[stepwise $\left \lfloor{w_1(d)}\right \rfloor$ for fast integer computation.]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-c-stepwise.jpeg}}

%  \caption{Shannon write operation:  Computing the amount of information of a signal to each hard location in its access radius. (a) entirety of the space; (b) region of interest; (c) Fast integer computation is possible through a stepwise function.}
%  \label{fig:info-theory-hypothesis}
%\end{figure}



%\begin{figure}[h!]
%  \centering
%  \subfloat[$w_2(d), d \in \{ 1, 2, ..., n\}.$]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-a-sum.jpeg}}
%
%  \subfloat[$w_2(d)$ for the desired range. ]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-b-sum-range.jpeg}}
%
%  \subfloat[stepwise $\left \lfloor{w_2(d)}\right \rfloor$ for fast integer computation.]{\includegraphics[width=3.2in]{./images02/new-images/Info-theory-c-sum-stepwise.jpeg}}
%
%  \caption{SOON TO BE DEPRECATED.  Shannon write operation:  Computing the sum of low-likelihood signals. (a) entirety of the space; (b) region of interest; (c) Fast integer computation through a stepwise function. }
%  \label{fig:info-theory-sum-hypothesis}
%\end{figure}
%










\chapter{Conclusion}

Sparse Distributed Memory is a viable model of human memory, yet it does require researchers to (re-)implement a number of parallel algorithms in different architectures.

We provide a new, open-source, cross-platform, highly parallel framework in which researchers may be able to create hypotheses and test them computationally with minimal effort. The framework is well-documented for public release at this time (http://sdm-framework.readthedocs.io), it has already served as the backbone of Chada's Ph.D. thesis \citep{chada2016you}. The single-line command ``pip install sdm'' will install the framework on posix-like systems, and single-line commands will let users test the framework, generate some of the figures from Kanerva's theoretical predictions in their own machines, and --- if interested enough ---, test their own theories and improve the framework, and the benchmarks used to evaluate the framework, in open-source fashion. It is our belief that such work is a necessary component towards accelerating research in this promising field.

Here are interesting questions that have been considered during this work, but have had to be left for future research.

\section{``i'' versus ``l''}

The classification algorithm had some problems classifying the letters ``i'' and ``l''. It may have happened because of the low distance between them. In this case, SDM could not differentiate the details --- it has only considered the big picture. Although this behavior is close to how humans see things, we also have the ability to zoom and focus on the details, clearly discerning letter ``i'' from letter ``l''.

I have run the classification algorithm under the MNIST database of handwritten digits \citep{deng2012mnist}. First, SDM has been trained with the 60,000 training images, and then it classified the 10,000 testing images. In these initial tests, the memory has given the correct classification for 79.22\% of the images, which is inferior to the specialized algorithms. For instance, in 1998, \citet{lecun1998gradient} have developed algorithms which achieve from 88\% through a linear classifier, to 99.7\% through a convolutional net. For a review of algorithms' performance in the MNIST database, see \cite{mnist}.

Looking into the reason behind images incorrectly classified, I have found that the issue is very related to the ``i'' versus ``l'' issue. Some handwritten digits are very close to others, and a ``2'' or a ``7'' may look like a ``1'', for instance. So, how can we solve this issue without using anything specific to images?  Machine learning algorithms use specific techniques to improve performance. I would like to unveil a solution psychologically closer to how we behave --- even if that eventually leads to lower performance.

An unexplored idea is to use multiple SDMs which communicate. A first SDM would write the whole picture, just like we have done. Another SDM would write specific parts of the image, just like our eye focusing on specific parts. When reading, they may compose the counters and give a more precise classification.

% \subsection{Multiple levels}

% Deep neural networks, alphaGo, alphazero



\section{Magic numbers}

Kanerva suggests, in his book, the use of 1,000 dimensions and 1,000,000 hard locations.  More recently, he suggested the use of 10,000 dimensions, and on personal discussions suggested that this should be a minimum; as he has been concerned in latent semantic analysis and seems to be the proper scale in that application.

Each parameter set choice like this will lead to particular numbers --- many of them emergent---, such as the access radius size, critical distance, memory capacity, and so forth.

One intriguing question here is:  is there a `better' number of dimensions and of hard locations?  If so, can such numbers be better studied algebraically or numerically?

How should these parameters be compared?  What are the tradeoffs that should be considered?  What are the `best' benchmarks possible?

% \subsection{Classification with context using sequences --- for words instead of only letters}



\section{Symmetrical, rapidly accessible, hard locations}

A hypercube with n dimensions can be divided by two hypercubes with $n-1$ dimensions. Is there an algorithm that separates the area of each hard location in such a form that there exists a function mapping each bitstring in $\{0,1\}^n$ to the set of hard locations it `belongs to'?  Though this would break Kanerva's assumption of a randomly yet uniformly distributed set of hard locations --- for a perfectly symmetrical set of hard locations ---, there could be large performance gains if such a mapping function from a bitstring to its corresponding set of nearest hard locations exists.

Consider the hypercube with $n$ dimensions.  We want to select a subset of its vertices with cardinality $2^{20}$ that is symmetrically distributed over the space. Afterward, $\forall b \in \{ 0,1\} ^n$, we want an algorithm $A$ that yields the particular list of hard locations for $b$ and all hard locations respect the desired properties of the memory.

A reduction from measuring the distance to $2^{20}$ hard locations to a computation of $2^{10}$ hard locations might yield astonishing performance gains, depending, of course, on our optimistic assumptions concerning existence and complexity of such algorithm.  At large scales of computing, the very ability to perform some experiments is a function of sheer performance. The horizon of experiments --- and possibly of knowledge --- expands \emph{as a function of computational demands}. A little more on this in my closing words.



\section{Illuminating and paving a pathway}

Let us revisit, in these concluding thoughts, the emphasis employed over speed of computation.  At first sight, that might seem like a typical objective of efficiency in computer science. But we are not only interested in the computer science effects here --- the ambition is different. More important than this `computer-sciency' goal, i.e., a beautiful, clean, efficient algorithm with the primary effect of enhanced speed, however, is the secondary effect on the sociology of science:  \emph{We can see farther}.


Beyond speed, I have also strived for \emph{ease of use}.  All the simulations and graphics generated in this thesis are promptly available to be re-executed and explored by those interested. I have generated a Docker image, which makes it even easier to explore the framework. After running the container, a Jupyter Notebook is available with sdm-framework and other tools already installed. We invite the reader to take a look and explore a little bit.

The overarching intention here is to not only provide a starting point, but to provide a documented Framework in which SDM research can be conducted.  Consider having the ability to compare the results of a new (‘forked’) model to the previous `best' (under a particular benchmark set).  For example, some of the benchmarks that we plan to develop in future research are: how fast is convergence through iterative reading?  How large is the attractor of the critical distance?  How well does the system filter noise?  How well does the system work under the supervised learning task?  And other authors may be able to improve this benchmark set themselves, as is usual in open source development.  It is perhaps this facility of ease to build on top of previous work that seems most exciting at this stage.

Consider the misunderstanding concerning the SDM read operation:  Dr. Stan Franklin describes Kanerva's read operation in a way that each hard location, at each dimension, provides only a single bit of information to the read operation (instead of Kanerva's full counter).  We have referred to this modified read operation as Chada read\footnote{Legend has it that my friend \text{\&} colleague, Dr. Daniel de Magalhães Chada, along with Linhares, did not consult and re-check with Kanerva's book and only discovered the discrepancy in code and ideas a couple of years afterward.}.  Having an open, testable, codebase reduces the possibilities of such misunderstandings in the long run.  Indeed, a high-quality codebase seems to have become a scientific community's form of unequivocally standing behind a consensus. For example, the journal Nature analyzed the top-100 cited papers in history, to find:

\begin{quote}
... some surprises, not least that it takes a staggering 12,119 citations to rank in the top 100 — and that many of the world’s most famous papers do not make the cut. A few that do, such as the first observation of carbon nanotubes (number 36) are indeed classic discoveries. But the vast majority describe experimental methods or \emph{software that have become essential in their fields. [...] The list reveals just how powerfully research has been affected by computation and the analysis of large data sets.} \\
\hfill --- \citet{van2014top}, emphasis mine.
\end{quote}

It is no coincidence that scientific journals such as BMC Neuroscience, or the Journal of Machine Learning Research have specific sections on open-source software. The journal Neurocomputing states, bluntly: “software is scientific method by machine”.

Of course, for the skeptical reader who may consider software a less worthy pursuit, there is also new work here.  The mathematics of the model has been shown to be correct numerically (with a single, small, anomaly); we have shown how to execute unsupervised learning with nothing besides operations original to the SDM; we have studied the generalized Murilo read; we have seen noise filtering; the death of neurons; how information-theory may be of use; and finally, we have reproduced numerous of the original propositions put forth by Kanerva.  The emphasis might have been on the \emph{breadth of topics}, in detriment of depth here or there.  But this is due to our research group's enthusiasm for the topic; we do indeed believe that SDM is --- if not correct --- extremely close to a full scientific understanding of human long-term memory.  If so, it is such a monumental achievement that we want readers to be able to see all of what we see and imagine the vastness of possibilities.  The work on, say, reinforcement learning, is most definitely not the definitive work we will see on the subject, but a challenge left for readers to contemplate. Ralph Waldo Emerson once said \emph{do not go where the path may lead. Go, instead, where there is no path, and leave a trail.}  Professor Pentti Kanerva has left the trail.  It is my job to illuminate it and to pave it and to clear it; to try to deliver an easier pathway for the next generation.  Some essays completely shut the door close at the end; this one intends to leave it wide open. As the reader might have noticed, this final section does not read as an analysis of the work done; it reads, instead, as a \emph{desideratum}, a prologue, a yearning for others to join me in imagining the shape of things to come.










% TODO Include in the to do list simulations based on the suggestion of Murilinho.

\chapter{Appendix}
%\section{Generating Kanerva's table 7.3}

%\includepdf[
%  pages=-,
%  pagecommand={\pagestyle{headings}},
%  %addtotoc={1,section,1,Quilling Shapes,sec:shapes}
%]{./Chapter02-SDM/Kanerva-Table-7.3/Kanerva-Table-7_3.pdf}

\chapter*{List of Jupyter notebooks}
