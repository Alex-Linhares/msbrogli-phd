\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subfig}



\begin{document}

Let $x$ and $y$ be random bitstrings and $n$ be the SDM's number of bits; let $x_i$ and $y_i$ be the $i$th bit of $x$ and $y$, respectivelly; and $d(x, y)$ be the Hamming distance.

Applying the law of total probability:

\begin{align}
P(x_i = y_i | d(x, y) \le r) = \sum_{k=0}^{r} P(x_i = y_i | d(x, y) = k \le r) \cdot P(d(x, y) = k | d(x, y) \le r)
\end{align}

We also know that

\begin{align}
P(x_i = y_i | d(x, y) = k) &= \frac{n-k}{n} \\
P(d(x, y) = k | d(x, y) \le r) &= \frac{\binom{n}{k}}{\sum_{j=0}^{r} \binom{n}{j}}
\end{align}

Hence,

\begin{align}
P(x_i = y_i | d(x, y) \le r) = \frac{\sum_{k=0}^{r} \frac{n-k}{n} \binom{n}{k}}{\sum_{j=0}^{r} \binom{n}{j}}
\end{align}

Finally, as $\frac{n-k}{n} \binom{n}{k} = \binom{n-1}{k}$,

\begin{align}
P(x_i = y_i | d(x, y) \le r) = \frac{\sum_{k=0}^{r} \binom{n-1}{k}}{\sum_{k=0}^{r} \binom{n}{k}}
\end{align}

This equation is valid for both ``x at x'' (autoassociative memory) and ``random at x'' (heteroassociative memory). When $n=1,000$ and $r=451$, $P(x_i = y_i | d(x, y) \le r) = p = 0.552905498137$.

Hence, let $Z$ be the number of activated hard location with the same bit as the reading address, then: $Z = \sum_{i=1}^{h} X_i$, where $\mathbf{E}[h] = h$, $\mathbf{V}[h] = H p_1 (1-p_1)$, $p_1 = 2^{-n} \sum_{k=0}^{r} \binom{n}{k}$, and $X_i \sim \text{Bernoulli}(p)$. By the central limit theorem, $Z$ is normally distributed with $\mathbf{E}[Z] = \mathbf{E}[\mathbf{E}[W_i | h]] = \mathbf{E}[ph] = p \mathbf{E}[h] = ph$, and $\mathbf{V}[Z] = \mathbf{E}[\mathbf{V}[X_i|h]] + \mathbf{V}[\mathbf{E}[X_i|h]] = \mathbf{E}[hp(1-p)] + \mathbf{V}[ph] = p(1-p) \mathbf{E}[h] + p^2 \mathbf{V}[h] = hp(1-p) + p^2 H p_1 (1-p_1)$. See Figure \ref{fig:sdm-same-bit-histogram} for a comparison between the theoretical model and a simulation.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.75\textwidth]{./images02/autocorrelation/same-bit-histogram.png}

  \caption{The histogram was obtained through simulation. The red curve is the theoretical normal distribution.}
  \label{fig:sdm-same-bit-histogram}
\end{figure}

\section{Counter bias}

The bias begins in the counters. Let's analyze the $i$th counter of a hard location.

Let $s$ be the number of bitstrings written into memory (in our case, $s=10,000$), $h$ be the average number of activated hard locations ($h=1,071.85$), $H$ be the number of hard locations ($H=1,000,000$), and $\text{addr}_i$ be the $i$th bit of the hard location's address.

Let $\theta = \frac{sh}{H}$ be the average number of bitstrings written in each hard location, and $X_k \sim \text{Bernoulli}(p)$ (where $p = P(x_i = y_i | d(x, y) \le r)$). Thus, $Y_i = \sum_{k=1}^{\theta} X_k \sim \mathcal{N}(\mu_1, \sigma_1^2)$ with $\mu_1 = p \theta$, and $\sigma_1^2 = p(1-p) \theta + p^2 s^2 p_1 (1 - p_1) / H$.

During a write operation, the counters are incremented for every bit 1 and decremented for every bit 0. So, after $s$ writes, there will be $\theta$ bitstrings written in each hard location, $Y_i$ bits 1, and $\theta - Y_i$ bits 0. Thus, $[\text{cnt}_i | \text{addr}_i = 1] = (Y_i) - (\theta - Y_i) = 2Y_i - \theta$; and $[\text{cnt}_i | \text{addr}_i = 0] = \theta - 2Y_i$.

Hence, as $\text{cnt}_i = 2Y_i - \theta$, $\mathbf{E}[2Y_i - \theta] = 2 \mathbf{E}[Y_i] - \theta$, and $\mathbf{V}[2Y_i - \theta] = 4 \mathbf{V}[Y_i]$, then,

\begin{align}
\left[ \text{cnt}_i | \text{addr}_i=1 \right] &\sim \mathcal{N}(\mu_2 = (2p-1) \theta, \sigma_2^2 = 4 \sigma_1^2) \\
\left[ \text{cnt}_i | \text{addr}_i=0 \right] &\sim \mathcal{N}(\mu_2 = -(2p-1) \theta, \sigma_2^2 = 4 \sigma_1^2)
\end{align}

In our case, $p=0.5529$, $s=10,000$, $h=1,071.85$, and $H=1,000,000$, so $\theta = 10.7185$ and $\text{cnt}_i \sim \mathcal{N}(\mu=1.1341, \sigma^2 = 10.7294)$. For ``random at x'', $p=0.5$, so $\mu = 0$ and $\sigma^2 = 10.8255$. See Figure \ref{fig:sdm-corr-counters}.

\begin{figure}[h!]
  \centering
  \subfloat[$\text{addr}_i=1$]{\includegraphics[width=0.5\textwidth]{./images02/autocorrelation/x_at_x_addr1.png}}
  \subfloat[$\text{addr}_i=0$]{\includegraphics[width=0.5\textwidth]{./images02/autocorrelation/x_at_x_addr0.png}}

  \caption{The value of the counters after $s=10,000$ writes shows the autocorrelation in the counters in autoassociative memories (``x at x''). The histogram was obtained through simulation. The red curve is the theoretical normal distribution.}
  \label{fig:sdm-corr-counters}
\end{figure}


Finally,

\begin{align}
P(\text{cnt}_i > 0 | \text{addr}_i = 1) = P(\text{cnt}_i < 0 | \text{addr}_i = 0) = 1 - \mathcal{N}.\text{cdf}(0)
\end{align}

For ``random at x'', $p=0.5$ implies $\mu_2 = 0$, which implies $P(\text{cnt}_i > 0 | \text{addr}_i = 1) = P(\text{cnt}_i < 0 | \text{addr}_i = 0) = 0.5$, independently of the parameters because they will only affect the variance and the normal distribution is symmetrical around the average.

However, for ``x at x'', $p=0.5529$ and the probabilities depend on $s$. For $s=10,000$, they are equal to 0.6354. For $s=20,000$, they are equal to 0.6867. For $s=30,000$, they are equal to 0.7232. The more random bitstrings are written into the memory, the more a hard location to point to itself. See Figure \ref{fig:sdm-corr-prob} --- and notice that I still have to figure out why the mean is correct, but the standard deviation is not. As each of the $n$ counters of a hard location may be equal or not with the same probability, I assumed it would follow a Binomial distribution (and it worked for ``random at x'').

\begin{figure}[h!]
  \centering
  \subfloat[``random at x'']{\includegraphics[width=0.5\textwidth]{./images02/autocorrelation/random_at_x_counters.png}}
  \subfloat[``x at x'']{\includegraphics[width=0.5\textwidth]{./images02/autocorrelation/x_at_x_counters.png}}

  \caption{Autocorrelation in the counters in autoassociative memories (``x at x''). The histogram was obtained through simulation. The red curve is the theoretical distribution.}
  \label{fig:sdm-corr-prob}
\end{figure}



\section{Read bias}

Now that we know the distribution of $\text{cnt}_i | \text{addr}_i$, we may go to the read operation. During the read operation, on average, $h$ hard locations are activated and their counters are summed up. So, for the $i$th bit,

\begin{align}
\text{acc}_i = \sum_{k=1}^{h} \text{cnt}_k
\end{align}

Let $\eta$ be the reading address and $\eta_i$ the $i$th bit of it. Then,

\begin{align}
\left[ \text{acc}_i|\eta_i=1 \right] &= \sum_{k=1}^{ph} \left[ \text{cnt}_k | \text{addr}_k=1 \right] + \sum_{k=1}^{(1-p)h} \left[ \text{cnt}_k | \text{addr}_k=0 \right] \\
\left[ \text{acc}_i|\eta_i=0 \right] &= \sum_{k=1}^{ph} \left[ \text{cnt}_k | \text{addr}_k=0 \right] + \sum_{k=1}^{(1-p)h} \left[ \text{cnt}_k | \text{addr}_k=1 \right]
\end{align}

We may analyze only one case, because the probability of the other is exactly the same.

Each sum is a sum of normally distributed random variables, so

\begin{align}
\sum_{k=1}^{ph} \left[ \text{cnt}_k | \text{addr}_k=1 \right] &\sim \mathcal{N}(\mu_3 = \mu_2 ph, \sigma^2 = \sigma_1^2 ph) \label{eqn:sdm-eta1-addr1} \\
\sum_{k=1}^{(1-p)h} \left[ \text{cnt}_k | \text{addr}_k=0 \right] &\sim \mathcal{N}(\mu = -\mu_2 (1-p)h, \sigma^2 = \sigma_1^2 (1-p)h) \label{eqn:sdm-eta1-addr0}
\end{align}

In our case, $\sum_{k=1}^{ph} \left[ \text{cnt}_k | \text{addr}_k=1 \right] \sim \mathcal{N}(\mu=672.12, \sigma^2=6281.00)$, and $\sum_{k=1}^{ph} \left[ \text{cnt}_k | \text{addr}_k=1 \right] \sim \mathcal{N}(\mu=-543.49, \sigma^2=5078.99)$. See Figure \ref{fig:sdm-read-sums} --- we can notice there a small but significant difference between the theoretical and the simulated mean.

\begin{figure}[h!]
  \centering
  \subfloat[Equation \ref{eqn:sdm-eta1-addr1}  ($\text{addr}_k=1)$]{\includegraphics[width=0.5\textwidth]{./images02/autocorrelation/read-counters-eta1_addr1.png}}
  \subfloat[Equation \ref{eqn:sdm-eta1-addr0} ($\text{addr}_k=0$)]{\includegraphics[width=0.5\textwidth]{./images02/autocorrelation/read-counters-eta1_addr0.png}}

  \caption{The histogram was obtained through simulation. The red curve is the theoretical normal distribution.}
  \label{fig:sdm-read-sums}
\end{figure}

Hence,

\begin{align}
\left[ \text{acc}_i|\eta_i=1 \right] &\sim \mathcal{N}(\mu = (2p-1)^2 \theta h, \sigma^2 = 4 \theta p (1-p) h) \label{eqn:sdm-eta1} \\
\left[ \text{acc}_i|\eta_i=0 \right] &\sim \mathcal{N}(\mu = -(2p-1)^2 \theta h, \sigma^2 = 4 \theta p (1-p) h) \label{eqn:sdm-eta0}
\end{align}

In our case, $\left[ \text{acc}_i|\eta_i=1 \right] \sim \mathcal{N}(\mu = 128.62, \sigma^2 = 11359.99)$, and $\left[ \text{acc}_i|\eta_i=0 \right] \sim \mathcal{N}(\mu = -128.62, \sigma^2 = 11359.99)$. See Figure \ref{fig:sdm-read} --- we can notice that the small difference in the means from Figure \ref{fig:sdm-read-sums} has propagated to these images.

\begin{figure}[h!]
  \centering
  \subfloat[Equation \ref{eqn:sdm-eta1}  ($\eta_k=1)$]{\includegraphics[width=0.5\textwidth]{./images02/autocorrelation/read-counters-eta1.png}}
  \subfloat[Equation \ref{eqn:sdm-eta0} ($\eta_k=0$)]{\includegraphics[width=0.5\textwidth]{./images02/autocorrelation/read-counters-eta0.png}}

  \caption{The histogram was obtained through simulation. The red curve is the theoretical normal distribution.}
  \label{fig:sdm-read}
\end{figure}


Finally,

\begin{align}
P(wrong) &= P(\text{acc}_i < 0 | \eta_i = 1) \cdot P(\eta_i = 1) + P(\text{acc}_i > 0 | \eta_i = 0) \cdot P(\eta_i = 0) \\
    &= \frac{\mathcal{N}_{\eta_i=1}.\text{cdf}(0)}{2} + \frac{1-\mathcal{N}_{\eta_i=0}.\text{cdf}(0)}{2} \\
    &= \frac{\mathcal{N}_{\eta_i=1}.\text{cdf}(0)}{2} + \frac{\mathcal{N}_{\eta_i=1}.\text{cdf}(0)}{2} \\
    &= \mathcal{N}_{\eta_i=1}.\text{cdf}(0)
\end{align}

In our case, $P(wrong) = 0.1137518032308093$.

In order to check this probability, I have run a simulation reading from 1,000 random bitstrings (which have never been written into memory) and calculate the distance from the result of a single read. As the $P(wrong) = 0.11375$, I expected to get an average distance of 113.75 with a standard deviation of 10.04. See Figure \ref{fig:sdm-read-random-bs} --- We can notice a big difference between the theoretical model and the simulation. Using $\mu=102$ and $\sigma=121$, the curves match. I'm still looking for the mistake in the equations. I believe the problem is in Equation \ref{eqn:sdm-eta1-addr1}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.75\textwidth]{./images02/autocorrelation/read-random-bs.png}

  \caption{The histogram was obtained through simulation. The red curve is the theoretical normal distribution.}
  \label{fig:sdm-read-random-bs}
\end{figure}


\end{document}
